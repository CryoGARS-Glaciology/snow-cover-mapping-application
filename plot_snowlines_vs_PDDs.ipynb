{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 877,
     "status": "ok",
     "timestamp": 1692729489845,
     "user": {
      "displayName": "Rainey Aberle",
      "userId": "06245029574837198852"
     },
     "user_tz": 360
    },
    "id": "Aj-11NYMS_s2"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19974,
     "status": "ok",
     "timestamp": 1692729509815,
     "user": {
      "displayName": "Rainey Aberle",
      "userId": "06245029574837198852"
     },
     "user_tz": 360
    },
    "id": "OZQNy6Fet_Do",
    "outputId": "aeffd10b-82ad-41aa-8a38-84685ce74be7"
   },
   "outputs": [],
   "source": [
    "# If using Google Colab, mount Google Drive so you can access your Drive folders\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1692729509816,
     "user": {
      "displayName": "Rainey Aberle",
      "userId": "06245029574837198852"
     },
     "user_tz": 360
    },
    "id": "YwXx-lXmuF2w"
   },
   "outputs": [],
   "source": [
    "# Define path to 'snow_cover_mapping_application/study-sites/'\n",
    "study_sites_path = 'drive/MyDrive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5071,
     "status": "ok",
     "timestamp": 1692729514884,
     "user": {
      "displayName": "Rainey Aberle",
      "userId": "06245029574837198852"
     },
     "user_tz": 360
    },
    "id": "uo1BT5RbuMTs",
    "outputId": "973d5a54-a210-4e59-8c61-846167ffea65"
   },
   "outputs": [],
   "source": [
    "# Grab list of study site names in folder\n",
    "os.chdir(study_sites_path)\n",
    "site_names = sorted([x[0:-1] for x in glob.glob('*/', recursive = True)])\n",
    "site_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1692729530417,
     "user": {
      "displayName": "Rainey Aberle",
      "userId": "06245029574837198852"
     },
     "user_tz": 360
    },
    "id": "vEZJdG30-7kC"
   },
   "outputs": [],
   "source": [
    "# define functinos to solve for the PDD normalization factor using gradient descent\n",
    "# adapted from: https://towardsdatascience.com/gradient-descent-in-python-a0d07285742f\n",
    "\n",
    "def cal_cost(max_value, y1, y2):\n",
    "    \"\"\"\n",
    "    max_value = maximum value over which to normalize y1\n",
    "    y1 = PDDs\n",
    "    y2 = median snowline elevations [m]\n",
    "    \"\"\"\n",
    "\n",
    "    # remove NaNs from time series\n",
    "    ireal = np.argwhere(~np.isnan(y2))\n",
    "    y1 = y1[ireal]\n",
    "    y2 = y2[ireal]\n",
    "\n",
    "    # normalize y1 from min(y1) to max_value\n",
    "    y1_norm = ((y1 - np.nanmin(y1)) / (np.nanmax(y1) - np.nanmin(y1))\n",
    "              * (max_value - np.nanmin(y2)) + np.nanmin(y2))\n",
    "\n",
    "    # calculate mean difference between y1 and y2\n",
    "    diff = np.nanmean(np.abs(y1_norm-y2))\n",
    "\n",
    "    return diff\n",
    "\n",
    "def gradient_descent(y1, y2, learning_rate=1, iterations=1000):\n",
    "\n",
    "    cost_history = np.zeros(iterations)\n",
    "    max_value_history = np.zeros(iterations)\n",
    "    max_value = np.nanmax(y2)\n",
    "    for it in range(iterations):\n",
    "        max_value = max_value - learning_rate\n",
    "        max_value_history[it] = max_value\n",
    "        cost_history[it] = cal_cost(max_value, y1, y2)\n",
    "\n",
    "    # find optimum max_value where cost = minimum\n",
    "    ibest = np.argwhere(cost_history==np.nanmin(cost_history))[0][0]\n",
    "    max_value_best = max_value_history[ibest]\n",
    "\n",
    "    # normalize y1 from min(y2) to max_value_best\n",
    "    y1_norm = ((y1 - np.nanmin(y1)) / (np.nanmax(y1) - np.nanmin(y1))\n",
    "              * (max_value_best - np.nanmin(y2)) + np.nanmin(y2))\n",
    "\n",
    "    return y1_norm, max_value_best, max_value_history, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 47699,
     "status": "error",
     "timestamp": 1692730127426,
     "user": {
      "displayName": "Rainey Aberle",
      "userId": "06245029574837198852"
     },
     "user_tz": 360
    },
    "id": "AxZbBRcwuSaZ",
    "outputId": "baf2d82e-b9df-4a04-afb3-9450994da45e"
   },
   "outputs": [],
   "source": [
    "# Define names of study sites to plot\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "# loop through study sites\n",
    "for site_name in site_names:\n",
    "\n",
    "  print(site_name)\n",
    "\n",
    "  # load snowlines\n",
    "  sl_full = pd.DataFrame()\n",
    "  sl_fns = glob.glob(site_name + '/imagery/snowlines/*.csv')\n",
    "  for sl_fn in sl_fns:\n",
    "    sl = pd.read_csv(sl_fn)\n",
    "    sl_full = pd.concat([sl_full, sl])\n",
    "  sl_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "  if len(sl_full) < 1:\n",
    "    print('No snowlines in files, continuing...')\n",
    "    print(' ')\n",
    "    continue\n",
    "  sl_full['datetime'] = sl_full['datetime'].astype('datetime64[ns]')\n",
    "  sl_full = sl_full.sort_values(by='datetime')\n",
    "\n",
    "  # load ERA data\n",
    "  if len(glob.glob(site_name + '/ERA/*.csv')) < 1:\n",
    "    print('No ERA data in files, continuing...')\n",
    "    print(' ')\n",
    "    continue\n",
    "  ERA_fn = glob.glob(site_name + '/ERA/*.csv')[0]\n",
    "  ERA = pd.read_csv(ERA_fn)\n",
    "  ERA['Date'] = ERA['Date'].astype('datetime64[ns]')\n",
    "\n",
    "  # add days since first ERA date column\n",
    "  ERA['days'] = (ERA['Date'] - ERA['Date'][0]) / np.timedelta64(1, 'D')\n",
    "  sl_full['days'] = (sl_full['datetime'] - ERA['Date'][0]) / np.timedelta64(1, 'D')\n",
    "  # interpolate PDD_norm at snowline estimate dates\n",
    "  PDD_interp = np.interp(sl_full['days'].values, ERA['days'].values, ERA['Cumulative_Positive_Degree_Days'].values)\n",
    "\n",
    "  # solve for best maximum value for normalizing PDDs\n",
    "  _, max_value_best, _, _ = gradient_descent(PDD_interp, sl_full['snowline_elevs_median_m'].values)\n",
    "  print('Optimum max value = ' + str(max_value_best))\n",
    "  PDD_norm = ((ERA['Cumulative_Positive_Degree_Days'].values - np.nanmin(ERA['Cumulative_Positive_Degree_Days'].values))\n",
    "              / (np.nanmax(ERA['Cumulative_Positive_Degree_Days'].values) - np.nanmin(ERA['Cumulative_Positive_Degree_Days'].values))\n",
    "              * (max_value_best - np.nanmin(sl_full['snowline_elevs_median_m'].values)) + np.nanmin(sl_full['snowline_elevs_median_m'].values))\n",
    "  # interpolate normalized PDDs to snowline observation dates\n",
    "  PDD_norm_interp = np.interp(sl_full['days'].values, ERA['days'].values, PDD_norm)\n",
    "\n",
    "  # calculate difference between PDD_norm_interp and median snowline elevations\n",
    "  diff = sl_full['snowline_elevs_median_m'].values - PDD_norm_interp\n",
    "\n",
    "  # define threshold for filtering snowline points\n",
    "  threshold = 0.5 * (np.nanmax(sl_full['snowline_elevs_median_m'].values) - np.nanmin(sl_full['snowline_elevs_median_m'].values))\n",
    "  ifilt = np.ravel(np.argwhere(np.abs(diff) >= threshold))\n",
    "  ikeep = np.ravel(np.argwhere(np.abs(diff) < threshold))\n",
    "\n",
    "  # plot normalized PDDs and snowline elevations\n",
    "  fig, ax = plt.subplots(1,2, figsize=(16, 8))\n",
    "  ax[0].plot(ERA['Date'], PDD_norm, '.b', markersize=1.5, label='normalized PDDs')\n",
    "  ax[0].plot(sl_full['datetime'].iloc[ifilt], sl_full['snowline_elevs_median_m'].iloc[ifilt], '.r', label='removed points')\n",
    "  ax[0].plot(sl_full['datetime'].iloc[ikeep], sl_full['snowline_elevs_median_m'].iloc[ikeep], '.k', label='filtered time series')\n",
    "  ax[0].set_xlim([np.datetime64('2013-01-01'), np.datetime64('2022-12-01')])\n",
    "  ax[0].set_ylabel('Elevation [m]')\n",
    "  ax[0].legend(loc='best')\n",
    "  ax[0].grid()\n",
    "  # plot differences\n",
    "  ax[1].plot(sl_full['days'].iloc[ifilt], diff[ifilt], '.r', label='removed points')\n",
    "  ax[1].plot(sl_full['days'].iloc[ikeep], diff[ikeep], '.k', label='remaining points')\n",
    "  ax[1].plot([np.nanmin(sl_full['days'].values), np.nanmax(sl_full['days'].values)], [threshold, threshold], '--k', label='threshold')\n",
    "  ax[1].plot([np.nanmin(sl_full['days'].values), np.nanmax(sl_full['days'].values)], [-threshold, -threshold], '--k')\n",
    "  ax[1].set_xlim([np.nanmin(sl_full['days'].values), np.nanmax(sl_full['days'].values)])\n",
    "  ax[1].legend(loc='best')\n",
    "  ax[1].grid()\n",
    "  ax[1].set_xlabel('Days')\n",
    "  ax[1].set_ylabel('Differences')\n",
    "\n",
    "  fig.suptitle(site_name)\n",
    "  fig.tight_layout()\n",
    "  plt.show()\n",
    "\n",
    "  print(' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1692730005537,
     "user": {
      "displayName": "Rainey Aberle",
      "userId": "06245029574837198852"
     },
     "user_tz": 360
    },
    "id": "D4OtB1rPkgv8",
    "outputId": "028084b6-19a9-40ad-eab1-09f8dbff7450"
   },
   "outputs": [],
   "source": [
    "ERA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1692729997334,
     "user": {
      "displayName": "Rainey Aberle",
      "userId": "06245029574837198852"
     },
     "user_tz": 360
    },
    "id": "57isJ-OKY0kK",
    "outputId": "3328ab5d-a52c-42c3-d729-86ec69ba2276"
   },
   "outputs": [],
   "source": [
    "ERA.plot(x='Date', y='Cumulative_Positive_Degree_Days')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8_YFNQ7j1z_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOkG0ad0ui0qno5HBCnxYps",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
