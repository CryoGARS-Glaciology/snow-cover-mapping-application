{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a268cb2-8765-4785-a416-f29c9289f371",
   "metadata": {},
   "source": [
    "# Make figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c886bd1-2d50-401f-aaef-8353f32db569",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "from shapely import wkt\n",
    "import seaborn as sns\n",
    "import string\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239e6df1-03f5-472a-8c1c-222f8f92ebee",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "aois_path = os.path.join(scm_path, 'all_AOIs')\n",
    "aois_fn = 'all_aois.shp'\n",
    "eras_path = os.path.join(scm_path, 'all_ERA_data')\n",
    "eras_fn = 'all_era_data.csv'\n",
    "snowlines_path = os.path.join(scm_path, 'all_snowlines')\n",
    "snowlines_fn = 'all_snowlines.csv'\n",
    "snowlines_medians_fn = 'all_snowlines_weekly_median_trends.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e202626-5344-42af-8409-b6926e4c8118",
   "metadata": {},
   "source": [
    "## Plot minimum AAR with different characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c18a2fa-da10-4d01-837d-259aa1602fb1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load minimum snow cover characteristics\n",
    "min_snow_cover_stats_fn = 'min_snow_cover_stats.csv'\n",
    "min_snow_cover_stats = pd.read_csv(os.path.join(snowlines_path, min_snow_cover_stats_fn))\n",
    "min_snow_cover_stats['geometry'] = min_snow_cover_stats['geometry'].apply(wkt.loads)\n",
    "# sort by region numbers\n",
    "min_snow_cover_stats.sort_values(by=['O1Region', 'O2Region'], inplace=True)\n",
    "\n",
    "# -----Add glacier boundary x and y centroid coordinates for plotting\n",
    "min_snow_cover_stats['centroid_x'] = [x.centroid.coords.xy[0][0] for x in min_snow_cover_stats['geometry']]\n",
    "min_snow_cover_stats['centroid_y'] = [x.centroid.coords.xy[1][0] for x in min_snow_cover_stats['geometry']]\n",
    "\n",
    "# -----Load country outlines for plotting\n",
    "countries_fn = os.path.join(scm_path, '..', 'GIS_data', 'countries_shp', 'countries.shp')\n",
    "countries = gpd.read_file(countries_fn)\n",
    "usca = countries.loc[(countries['NAME']=='United States') | (countries['NAME']=='Canada')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8731d23-5255-445b-a22e-7987439647d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Plot maps of minimum AARs by various columns\n",
    "columns = ['Subregion', 'Aspect', 'Zmed', 'Area', 'Slope']\n",
    "columns_display = ['Subregion', 'Aspect [degrees]', 'Median elevation [m]', 'Area [km$^2$]', 'Slope [degrees]']\n",
    "palettes = [dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values),\n",
    "            'hls', \n",
    "            'mako', \n",
    "            'viridis', \n",
    "            'crest']\n",
    "bins_list = ['N/A', \n",
    "             np.linspace(0,360, num=7), \n",
    "             np.linspace(500, 3000, num=6),\n",
    "             np.linspace(0, 750, num=7),\n",
    "             np.linspace(0, 35, num=8)]\n",
    "plt.rcParams.update({'font.sans-serif':'Arial', 'font.size':12})\n",
    "xmin, xmax = -168, -112\n",
    "ymin, ymax = 45, 65\n",
    "for column, column_display, palette, bins in zip(columns, columns_display, palettes, bins_list):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10,10), gridspec_kw={'height_ratios':[2.5, 1]})\n",
    "    # plot country outlines on the map\n",
    "    usca.plot(ax=ax[0], facecolor='None', edgecolor='grey')\n",
    "    # plot points on the map\n",
    "    # if type(bins) != str:\n",
    "    #     sns.scatterplot(data=min_snow_cover_stats, x='centroid_x', y='centroid_y', \n",
    "    #                     vmin=np.min(bins), vmax=np.max(bins),\n",
    "    #                     hue=column, palette=palette, size='AAR_P50_min', \n",
    "    #                     sizes=(5, 100), legend=True, ax=ax[0])\n",
    "    # else:\n",
    "    sns.scatterplot(data=min_snow_cover_stats, x='centroid_x', y='centroid_y', \n",
    "                    hue=column, palette=palette, size='AAR_P50_min', \n",
    "                    sizes=(5, 100), legend=True, ax=ax[0])\n",
    "    sns.move_legend(ax[0], \"center right\", bbox_to_anchor=[1.1, 0.4, 0.2, 0.2])\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].set_ylabel('')\n",
    "    ax[0].grid()\n",
    "    ax[0].set_xlim(xmin, xmax)\n",
    "    ax[0].set_ylim(ymin, ymax)\n",
    "    # plot regional barplots\n",
    "    if type(bins) != str:\n",
    "        # add bin column\n",
    "        min_snow_cover_stats[column + '_bin'] = pd.cut(min_snow_cover_stats[column], bins)\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=min_snow_cover_stats, x=column + '_bin', y='AAR_P50_min', showfliers=False, palette=palette, ax=ax[1])\n",
    "    else:\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=min_snow_cover_stats, x=column, y='AAR_P50_min', showfliers=False, palette=palette, ax=ax[1])\n",
    "    if type(bins) == str:\n",
    "        ax[1].set_xticks(ax[1].get_xticks())\n",
    "        ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "    ax[1].set_xlabel(column_display)\n",
    "    ax[1].set_ylabel('AAR')\n",
    "\n",
    "    # Edit legend labels for map\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    labels = [x.replace('AAR_P50_min', 'Median AAR').replace(column, column_display) for x in labels]\n",
    "    ax[0].legend(handles, labels, loc='center right')\n",
    "    sns.move_legend(ax[0], \"center right\", bbox_to_anchor=[1.1, 0.4, 0.2, 0.2])\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "    # Edit legend\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    \n",
    "    # Save figure\n",
    "    fig_fn = 'AARs_map_' + column + '.png'\n",
    "    fig.savefig(os.path.join(figures_out_path, fig_fn), dpi=250, bbox_inches='tight')\n",
    "    print('figure saved to file: ', os.path.join(figures_out_path, fig_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c7ec6c-dc79-4c0c-b676-5c11b42e31a7",
   "metadata": {},
   "source": [
    "## Plot linear changes in AARs for 2016-2023 by terrain characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9b33bf-506e-4112-b171-9c69e5100211",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load AARs linear fit\n",
    "aars_linear_fit_fn = 'minimum_AARs_linear_fit.csv'\n",
    "aars_linear_fit = pd.read_csv(os.path.join(snowlines_path, aars_linear_fit_fn))\n",
    "\n",
    "# -----Add glacier terrain and geometry columns\n",
    "columns = ['geometry', 'centroid_x', 'centroid_y', 'O1Region', 'O2Region', 'Subregion', 'color', 'Aspect', 'Slope', 'Zmed', 'Area']\n",
    "for column in columns:\n",
    "    aars_linear_fit[column] = np.nan\n",
    "# iterate over site names\n",
    "for site_name in aars_linear_fit['site_name'].drop_duplicates().values:\n",
    "    min_snow_cover_stats_site = min_snow_cover_stats.loc[min_snow_cover_stats['RGIId']==site_name]\n",
    "    try:\n",
    "        for column in columns:\n",
    "            aars_linear_fit.loc[aars_linear_fit['site_name']==site_name, column] = [min_snow_cover_stats_site[column].values[0]]\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "aars_linear_fit.dropna(inplace=True)\n",
    "\n",
    "# -----Add column for negative value of change category to show more negative values as larger markers\n",
    "aars_linear_fit['linear_fit_coef_neg'] = - aars_linear_fit['linear_fit_coef']\n",
    "# Sort by region numbers\n",
    "aars_linear_fit.sort_values(by=['O1Region', 'O2Region'], inplace=True)\n",
    "aars_linear_fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703f590-0e6d-47a1-87ff-64543942cf04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Plot map of AARs linear fit with various columns\n",
    "columns = ['Subregion', 'Aspect', 'Zmed', 'Area', 'Slope']\n",
    "columns_display = ['Subregion', 'Aspect [degrees]', 'Median elevation [m]', 'Area [km$^2$]', 'Slope [degrees]']\n",
    "palettes = [dict(aars_linear_fit[['Subregion', 'color']].drop_duplicates().values),\n",
    "            'hls', \n",
    "            'mako', \n",
    "            'viridis', \n",
    "            'crest']\n",
    "bins_list = ['N/A', \n",
    "             np.linspace(0,360, num=7), \n",
    "             np.linspace(500, 3000, num=6),\n",
    "             np.linspace(0, 750, num=7),\n",
    "             np.linspace(0, 35, num=8)]\n",
    "plt.rcParams.update({'font.sans-serif':'Arial', 'font.size':12})\n",
    "xmin, xmax = -168, -112\n",
    "ymin, ymax = 45, 65\n",
    "def map_size(value):\n",
    "    return np.abs(value) * 100\n",
    "for column, column_display, palette, bins in zip(columns, columns_display, palettes, bins_list):\n",
    "    fig, ax = plt.subplots(2, 1, figsize=(10,10), gridspec_kw={'height_ratios':[2.5, 1]})\n",
    "    # plot country outlines on the map\n",
    "    usca.plot(ax=ax[0], facecolor='None', edgecolor='grey')\n",
    "    # plot points on the map\n",
    "    if type(bins) != str:\n",
    "        sns.scatterplot(data=aars_linear_fit, x='centroid_x', y='centroid_y', \n",
    "                        hue=column, palette=palette, size='linear_fit_coef_neg', sizes=(5, 150), legend=True, ax=ax[0])\n",
    "    else:\n",
    "        sns.scatterplot(data=aars_linear_fit, x='centroid_x', y='centroid_y', \n",
    "                        hue=column, palette=palette, size='linear_fit_coef_neg', sizes=(5, 150), legend=True, ax=ax[0])\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].set_ylabel('')\n",
    "    ax[0].grid()\n",
    "    ax[0].set_xlim(xmin, xmax)\n",
    "    ax[0].set_ylim(ymin, ymax)\n",
    "    # plot regional barplots\n",
    "    if type(bins) != str:\n",
    "        # add bin column\n",
    "        aars_linear_fit[column + '_bin'] = pd.cut(aars_linear_fit[column], bins)\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=aars_linear_fit, x=column + '_bin', y='linear_fit_coef', showfliers=False, palette=palette, ax=ax[1])\n",
    "    else:\n",
    "        # plot boxplots\n",
    "        sns.boxplot(data=aars_linear_fit, x=column, y='linear_fit_coef', showfliers=False, palette=palette, ax=ax[1])\n",
    "    if type(bins) == str:\n",
    "        ax[1].set_xticks(ax[1].get_xticks())\n",
    "        ax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\n",
    "    ax[1].set_xlabel(column_display)\n",
    "    ax[1].set_ylabel('AAR linear trend [per year]')\n",
    "    \n",
    "    # Edit legend labels for map\n",
    "    handles, labels = ax[0].get_legend_handles_labels()\n",
    "    labels = [x.replace('linear_fit_coef_neg', 'AAR linear trend [per year]').replace(column, column_display) for x in labels]\n",
    "    labels = [x.replace('0.', '-0.').replace('−-0', '0') for x in labels]\n",
    "    ax[0].legend(handles, labels, loc='center right')\n",
    "    sns.move_legend(ax[0], \"center right\", bbox_to_anchor=[1.15, 0.4, 0.2, 0.2])\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Save figure\n",
    "    fig_fn = 'AARs_linear_fit_map_' + column + '.png'\n",
    "    fig.savefig(os.path.join(figures_out_path, fig_fn), dpi=250, bbox_inches='tight')\n",
    "    print('figure saved to file: ', os.path.join(figures_out_path, fig_fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b68ebc-9738-402a-be93-00ec03a03188",
   "metadata": {},
   "source": [
    "## Plot AAR and linear fit time series for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487d862d-aba2-4425-9f6f-fcac82f11a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load snowlines\n",
    "snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "# Load minimum AAR fits\n",
    "min_aars_fn = 'minimum_AARs_linear_fit.csv'\n",
    "min_aars = pd.read_csv(os.path.join(snowlines_path, min_aars_fn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd656bc3-3b94-4120-8fd3-4629c3414e36",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Iterate over sites\n",
    "for site_name in tqdm(min_aars['site_name'].drop_duplicates().values):\n",
    "    # Grab snowlines\n",
    "    snowlines_site = snowlines.loc[snowlines['site_name']==site_name]\n",
    "    # Grab min AARs\n",
    "    min_aars_site = min_aars.loc[min_aars['site_name']==site_name].reset_index(drop=True)\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1,1,figsize=(8,6))\n",
    "    ax.plot(snowlines_site['datetime'], snowlines_site['AAR'], '.k', markersize=1)\n",
    "    ax.plot(min_aars_site['minimum_AARs_dts'], min_aars_site['minimum_AARs'], '*m', markersize=2)\n",
    "    ax.grid()\n",
    "    ax.set_title(site_name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8362f05b-eeea-4d4a-8230-48acd61f31aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.literal_eval(min_aars['minimum_AARs'][0])#.apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c27fc6-44b4-4ef4-865d-d5d5f01882b7",
   "metadata": {},
   "source": [
    "## Plot AAR and PDD time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf2134d-7570-4fc9-9a34-30034c9bb982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load all snowlines\n",
    "snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "\n",
    "# -----Load all ERA data\n",
    "eras = pd.read_csv(os.path.join(eras_path, eras_fn))\n",
    "eras['Date'] = pd.to_datetime(eras['Date'], format='mixed')\n",
    "\n",
    "# -----Load all AOIs\n",
    "aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42f1189-9eef-4dfc-8a76-a5066d748d24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Plot median trends for each subregion\n",
    "# add year and WOY columns to snowlines and ERA\n",
    "snowlines['Year'] = snowlines['datetime'].dt.isocalendar().year\n",
    "snowlines['WOY'] = snowlines['datetime'].dt.isocalendar().week\n",
    "eras['Year'] = eras['Date'].dt.isocalendar().year\n",
    "eras['WOY'] = eras['Date'].dt.isocalendar().week\n",
    "# grab all subregions\n",
    "subregions = aois[['O1Region', 'O2Region']].drop_duplicates().values\n",
    "# Set up figure\n",
    "fig, ax = plt.subplots(len(subregions), 1, figsize=(10, 4*len(subregions)))\n",
    "text_labels = [x for x in string.ascii_lowercase[0:len(subregions)]]\n",
    "# Iterate over subregions\n",
    "for i, (o1region, o2region) in enumerate(subregions):\n",
    "    \n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    \n",
    "    # Grab all AOIs in subregion\n",
    "    aois_subregion = aois.loc[(aois['O1Region']==o1region) & (aois['O2Region']==o2region)]\n",
    "    site_names_subregion = aois_subregion['RGIId'].drop_duplicates().values\n",
    "    # Grab all snowlines in subregion\n",
    "    Isubregion = [i for i in np.arange(len(snowlines)) \n",
    "                  if snowlines.loc[i, 'site_name'] in site_names_subregion]\n",
    "    snowlines_subregion = snowlines.iloc[Isubregion]\n",
    "    # Calculate AAR median and IQR trends over time\n",
    "    aar_median = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR'].median().reset_index()\n",
    "    aar_median['deciyear'] = aar_median['Year'] + (aar_median['WOY'] / 52)\n",
    "    # aar_q1 = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR'].apply(np.quantile(q=0.25))\n",
    "    # aar_q3 = snowlines_subregion.groupby(by=['Year', 'WOY'])['AAR'].apply(np.quantile(q=0.75))\n",
    "    # Grab all ERA data in subregion\n",
    "    Isubregion = [i for i in np.arange(len(eras)) \n",
    "                  if eras.loc[i, 'site_name'] in site_names_subregion]\n",
    "    eras_subregion = eras.iloc[Isubregion]\n",
    "    # Calculate PDD median and IQR trends over time\n",
    "    pdd_median = eras_subregion.groupby(by=['Year', 'WOY'])['Cumulative_Positive_Degree_Days'].median().reset_index()\n",
    "    pdd_median['deciyear'] = pdd_median['Year'] + (pdd_median['WOY'] / 52)\n",
    "    # pdd_q1 = eras_subregion.groupby(by=['Year', 'WOY'])['Cumulative_Positive_Degree_Days'].apply(np.quantile(q=0.25))\n",
    "    # pdd_q3 = eras_subregion.groupby(by=['Year', 'WOY'])['Cumulative_Positive_Degree_Days'].apply(np.quantile(q=0.75))\n",
    "\n",
    "    # Plot\n",
    "    ax[i].plot(aar_median['deciyear'], aar_median['AAR'], '.-b', linewidth=0.5, markersize=3)\n",
    "    ax[i].grid()\n",
    "    ax[i].set_ylim(0, 1)\n",
    "    ax[i].set_ylabel('Transient AAR', color='b')\n",
    "    ax[i].tick_params(axis='y', colors='b')\n",
    "    ax[i].set_title(text_labels[i] + ') ' + subregion_name)\n",
    "    ax2 = ax[i].twinx()\n",
    "    ax2.plot(pdd_median['deciyear'], pdd_median['Cumulative_Positive_Degree_Days'], '-m')\n",
    "    ax2.set_ylabel('$\\Sigma$PDD', color='m')\n",
    "    ax2.tick_params(axis='y', colors='m')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "fig_fn = 'time_series_AAR_PDDs.png'\n",
    "fig.savefig(os.path.join(figures_out_path, fig_fn), dpi=200, bbox_inches='tight')\n",
    "print('figure saved to file:', os.path.join(figures_out_path, fig_fn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "874f51b6-4503-4d61-b50e-1c1f5c9f353d",
   "metadata": {},
   "source": [
    "## Plot correlation statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b5dc5e-7c6c-4259-8cb8-47ce49c3c363",
   "metadata": {},
   "source": [
    "### AAR-PDD correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6522aa82-b120-4f8b-8251-0ea1d5e3db1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load AAR-PDD correlations for each subregion\n",
    "corr_coeffs_fns = sorted(glob.glob(os.path.join(eras_path, 'correlation_*aar-pdd*.csv')))\n",
    "corr_coeffs = pd.DataFrame()\n",
    "for corr_coeffs_fn in corr_coeffs_fns:\n",
    "    # grab subregion name from file name\n",
    "    subregion_name = os.path.basename(corr_coeffs_fn).split('aar-pdd_')[1].split('.csv')[0]\n",
    "    # load correlation coefficients for all sites\n",
    "    corr_coeffs_subregion = pd.read_csv(corr_coeffs_fn)\n",
    "    # add subregion name to dataframe\n",
    "    corr_coeffs_subregion['Subregion'] = subregion_name    \n",
    "    # concatenate to full dataframe\n",
    "    corr_coeffs = pd.concat([corr_coeffs, corr_coeffs_subregion])\n",
    "corr_coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "544dd756-0e05-4bc6-96d9-4eaed6072494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Plot boxplots\n",
    "palette = dict(min_snow_cover_stats[['Subregion', 'color']].drop_duplicates().values)\n",
    "fig, ax = plt.subplots(figsize=(8,6))\n",
    "sns.boxplot(data=corr_coeffs, x='Subregion', y='AAR-PDD Corr. Coeff.', palette=palette, ax=ax)\n",
    "ax.set_xticks(ax.get_xticks())\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n",
    "ax.set_xlabel('')\n",
    "ax.set_ylabel('AAR - $\\Sigma$PDD Correlation Coefficients')\n",
    "# ax.grid()\n",
    "plt.show()\n",
    "\n",
    "# -----Save to file\n",
    "fig_fn = os.path.join(figures_out_path, 'aar-pdd_correlation_coefficients_boxplot.png')\n",
    "fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "print('figure saved to file: ', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03124374-6931-413b-864f-63fdeefb61f0",
   "metadata": {},
   "source": [
    "## Plot scatter plots of AAR vs. cumulative PDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3bdfa7-74d9-438e-a239-f549636f8f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load all snowlines\n",
    "snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "snowlines['Date'] = snowlines['datetime'].values.astype('datetime64[D]')\n",
    "\n",
    "# -----Load all ERA data\n",
    "eras = pd.read_csv(os.path.join(eras_path, eras_fn))\n",
    "eras['Date'] = pd.to_datetime(eras['Date'], format='mixed')\n",
    "\n",
    "# -----Load all AOIs\n",
    "aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f116bf17-992f-4e2a-8be1-1c30a2ac6df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Fit linear trendlines to AAR vs. PDDs and AAR vs. snowfall\n",
    "# intialize full dataframe\n",
    "snowlines_eras_merged = pd.DataFrame()\n",
    "# iterate over sites\n",
    "for site_name in tqdm(snowlines['site_name'].drop_duplicates().values):\n",
    "    # subset dataframes to site\n",
    "    snowlines_site = snowlines.loc[snowlines['site_name']==site_name]\n",
    "    eras_site = eras.loc[eras['site_name']==site_name]\n",
    "    # merge snowlines and ERA dataframes\n",
    "    snowlines_eras_merged_site = snowlines_site.merge(eras_site, how='left', on='Date')\n",
    "    snowlines_eras_merged_site.rename(columns={'site_name_x': 'site_name'}, inplace=True)\n",
    "    # save in dataframe\n",
    "    snowlines_eras_merged_site = snowlines_eras_merged_site[['site_name', \n",
    "                                                             'AAR', \n",
    "                                                             'Cumulative_Positive_Degree_Days',\n",
    "                                                             'Cumulative_Precipitation_mwe',\n",
    "                                                             'Cumulative_Snowfall_mwe']]\n",
    "    snowlines_eras_merged_site.reset_index(drop=True, inplace=True)\n",
    "    snowlines_eras_merged_site.dropna()\n",
    "    # plot\n",
    "    # ax.plot(snowlines_eras_merged_site['Cumulative_Positive_Degree_Days'], \n",
    "    #         snowlines_eras_merged_site['AAR'], \n",
    "    #         '.', markersize=1)\n",
    "    # concatenate to full dataframe\n",
    "    snowlines_eras_merged = pd.concat([snowlines_eras_merged, snowlines_eras_merged_site])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fe7e02-267d-43a9-9b30-2711cb80c050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----Fit separate linear regression models\n",
    "# model = LinearRegression()\n",
    "# snowlines_eras_merged.dropna(inplace=True)\n",
    "# X_columns = ['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']\n",
    "# for X_column in X_columns:\n",
    "#     # fit model\n",
    "#     X = snowlines_eras_merged[X_column].values.reshape(-1, 1)\n",
    "#     y = snowlines_eras_merged['AAR'].values\n",
    "#     model_fit = model.fit(X, y)\n",
    "#     # calculate R^2\n",
    "#     r2 = fit.score(X, y)\n",
    "#     # plot\n",
    "#     fig, ax = plt.subplots(figsize=(6,6))\n",
    "#     ax.plot(X, y, '.', color='grey', markersize=0.5)\n",
    "#     y_pred = model_fit.predict(X)\n",
    "#     ax.plot(X, y_pred, '-k')\n",
    "#     ax.set_xlabel(X_column)\n",
    "#     ax.set_ylabel('AAR')\n",
    "#     ax.set_title('R$^2$ = ' + str(r2))\n",
    "#     plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeb28dd-7be1-493e-be1e-718e3cc13be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Fit multi-linear regression model\n",
    "model = LinearRegression()\n",
    "X_columns = ['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']\n",
    "# fit model\n",
    "X = snowlines_eras_merged[X_columns]\n",
    "y = snowlines_eras_merged['AAR'].values\n",
    "model_fit = model.fit(X, y)\n",
    "# calculate R^2\n",
    "# r2 = fit.score(X, y)\n",
    "# plot\n",
    "ax = plt.figure(figsize=(8,8)).add_subplot(projection='3d')\n",
    "ax.plot(X[X_columns[0]], X[X_columns[1]], y, '.', color='grey', markersize=0.5)\n",
    "y_pred = model_fit.predict(X)\n",
    "ax.plot(X[X_columns[0]], X[X_columns[1]], y_pred, '.k')\n",
    "ax.set_xlabel(X_columns[0])\n",
    "ax.set_ylabel(X_columns[1])\n",
    "ax.set_zlabel('AAR')\n",
    "ax.set_title('AAR = ' + str(np.round(model_fit.coef_[0], 5)) \n",
    "             + '*$\\Sigma$PDDs ' + str(np.round(model_fit.coef_[1], 5)) + '*$\\Sigma$Snowfall')\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5c7de8-930e-4150-8e0b-c01af459dbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
