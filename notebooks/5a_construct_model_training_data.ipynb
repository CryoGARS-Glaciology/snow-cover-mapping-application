{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4611ae9a-ec63-497a-a099-8703aed0bc4f",
   "metadata": {},
   "source": [
    "# Construct training data for model development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63a3246b-95e1-4962-a8ef-8de5612d0654",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot\n",
    "import sys\n",
    "import rioxarray as rxr\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686f026f-6977-49ca-bcff-8ed06aaa72f9",
   "metadata": {},
   "source": [
    "## Define paths in directory, import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476c2a0c-4669-4dad-a5ce-9ff1402fdf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be47bba-54aa-44e0-8055-b452ed08ee3f",
   "metadata": {},
   "source": [
    "## Load RGI glacier boundaries, ERA time series, and snowline time series for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e8004b-ed3c-4f07-a7b5-688d250eda92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load RGI glacier boundaries (AOIs)\n",
    "aois_fn = 'all_aois.shp'\n",
    "aois = gpd.read_file(os.path.join(scm_path, 'all_AOIs', aois_fn))\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All AOIs loaded from file')\n",
    "\n",
    "# -----Load ERA data\n",
    "eras_fn = 'all_era_data.csv'\n",
    "eras = pd.read_csv(os.path.join(scm_path, 'all_ERA_data', eras_fn))\n",
    "eras['Date'] = pd.to_datetime(eras['Date'], format='mixed')\n",
    "print('All ERA data loaded from file')\n",
    "    \n",
    "# -----Load all snowlines\n",
    "snowlines_fn = 'all_snowlines.csv'\n",
    "snowlines = pd.read_csv(os.path.join(scm_path, 'all_snowlines', snowlines_fn))\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "print('All snowlines loaded from file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ae67c0-4adf-42a5-8176-e01462489eb3",
   "metadata": {},
   "source": [
    "## Add Hypsometric Index and Subregion columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e237cd-0625-4256-9d5a-d2f1261a580d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Define some functions\n",
    "# Adjust DEM data variables\n",
    "def adjust_data_vars(dem_xr):\n",
    "    if 'band_data' in dem_xr.data_vars:\n",
    "        dem_xr = dem_xr.rename({'band_data': 'elevation'})\n",
    "    if 'band' in dem_xr.dims:\n",
    "        elev_data = dem_xr.elevation.data[0]\n",
    "        dem_xr = dem_xr.drop_dims('band')\n",
    "        dem_xr['elevation'] = (('y', 'x'), elev_data)\n",
    "    return dem_xr\n",
    "\n",
    "# Calculate Hypsometric Index (HI)\n",
    "# Jiskoot et al. (2009): https://doi.org/10.3189/172756410790595796\n",
    "def calculate_hypsometric_index(dem_fn, aoi):\n",
    "    # load DEM as DataArray\n",
    "    dem = rxr.open_rasterio(dem_fn)\n",
    "    # reproject DEM to AOI CRS\n",
    "    dem = dem.rio.reproject('EPSG:'+str(aoi.crs.to_epsg()))\n",
    "    # clip DEM to AOI\n",
    "    try:\n",
    "        dem_aoi = dem.rio.clip(aoi.geometry, aoi.crs)\n",
    "    except:\n",
    "        return 'N/A', 'N/A'\n",
    "    # convert to dataset\n",
    "    dem_aoi_ds = dem_aoi.to_dataset(name='elevation')\n",
    "    # adjust DEM data variables\n",
    "    dem_aoi_ds = adjust_data_vars(dem_aoi_ds)\n",
    "    # set no data values to NaN\n",
    "    dem_aoi_ds = xr.where((dem_aoi_ds > 1e38) | (dem_aoi_ds <= -9999), np.nan, dem_aoi_ds)\n",
    "    # check that there is data after removing no data values\n",
    "    if np.isnan(dem_aoi_ds.elevation.data).all():\n",
    "        return 'N/A', 'N/A'\n",
    "    # calculate elevation statistics\n",
    "    h_max = np.nanmax(np.ravel(dem_aoi_ds.elevation.data))\n",
    "    h_min = np.nanmin(np.ravel(dem_aoi_ds.elevation.data))\n",
    "    h_med = np.nanmedian(np.ravel(dem_aoi_ds.elevation.data))\n",
    "    # calculate HI, where HI = (H_max - H_med) / (H_med - H_min). If 0 < HI < 1, HI = -1/HI.\n",
    "    hi = (h_max - h_med) / (h_med - h_min)\n",
    "    if (0 < hi) and (hi < 1):\n",
    "        hi = -1 / hi\n",
    "    # determine HI category\n",
    "    if hi <= -1.5:\n",
    "        hi_category = 'Very top heavy'\n",
    "    elif (hi > -1.5) and (hi <= -1.2):\n",
    "        hi_category = 'Top heavy'\n",
    "    elif (hi > -1.2) and (hi <= 1.2):\n",
    "        hi_category = 'Equidimensional'\n",
    "    elif (hi > 1.2) and (hi <= 1.5):\n",
    "        hi_category = 'Bottom heavy'\n",
    "    elif hi > 1.5:\n",
    "        hi_category = 'Very bottom heavy'\n",
    "\n",
    "    return hi, hi_category\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7479d9-7f43-4bae-90dd-2a764ae50aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o1, o2 in aois[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    # Determine subregion name and color\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    print(subregion_name, o1, o2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d455bb-187b-433e-bb14-a3e519bd3420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Define columns to save in training data for each dataset\n",
    "aoi_columns = ['O1Region', 'O2Region', 'Subregion', 'Area', 'Zmed', 'Slope', 'Aspect', 'Lmax', 'TermType', 'Surging']\n",
    "era_columns = ['Date', 'Cumulative_Precipitation_mwe', 'Cumulative_Snowfall_mwe', \n",
    "               'Cumulative_Snowmelt_mwe', 'Positive_Degree_Days', 'Cumulative_Positive_Degree_Days']\n",
    "snowlines_columns = ['Date', 'site_name', 'snowline_elevs_median_m', 'SCA_m2', 'AAR', 'ELA_from_AAR_m']\n",
    "\n",
    "# -----Iterate over subregions\n",
    "for o1, o2 in [[2, 5]]: #aois[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    # Determine subregion name and color\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    print('\\n' + subregion_name)\n",
    "    \n",
    "    # Check if subregion training data exists in file\n",
    "    training_data_fn = os.path.join(scm_path, 'machine_learning', 'training_data_' + subregion_name + '.csv')\n",
    "    if os.path.exists(training_data_fn):\n",
    "        print('Subregion training data already exists in file, skipping...')\n",
    "        continue\n",
    "        \n",
    "    # Subset AOIs to subregion\n",
    "    aois_subregion = aois.loc[(aois['O1Region']==o1) & (aois['O2Region']==o2)]\n",
    "    # Add subregion name column\n",
    "    aois_subregion['Subregion'] = subregion_name\n",
    "\n",
    "    # Initialize subregion training data frame\n",
    "    training_data = pd.DataFrame()\n",
    "\n",
    "    # Iterate over sites in subregion\n",
    "    for site_name in tqdm(aois_subregion['RGIId'].drop_duplicates().values):\n",
    "    \n",
    "        # Subset AOIs\n",
    "        aoi = aois_subregion.loc[aois_subregion['RGIId']==site_name]\n",
    "\n",
    "        # Subset snowlines\n",
    "        snowlines_site = snowlines.loc[snowlines['site_name']==site_name]\n",
    "        # Add date column\n",
    "        snowlines_site.loc[:, 'Date'] = snowlines_site['datetime'].values.astype('datetime64[D]')\n",
    "        # Subset columns\n",
    "        snowlines_site = snowlines_site[snowlines_columns]\n",
    "    \n",
    "        # Subset ERA data\n",
    "        eras_site = eras.loc[eras['site_name']==site_name]\n",
    "    \n",
    "        # Merge snowlines and ERA time series\n",
    "        training_data_site = snowlines_site.merge(eras_site, how='left', on='Date')\n",
    "        # Identify subregion name\n",
    "        o1, o2 = aoi[['O1Region', 'O2Region']].values[0].astype(int)\n",
    "    \n",
    "        # Add AOI columns to merged snowlines and ERA dataframe\n",
    "        subregion, color = f.determine_subregion_name_color(o1, o2)\n",
    "        aoi.loc[:, 'Subregion'] = subregion\n",
    "        for aoi_column in aoi_columns:\n",
    "            training_data_site[aoi_column] = aoi[aoi_column].values[0]\n",
    "    \n",
    "        # Determine DEM file name\n",
    "        dem_fns = glob.glob(os.path.join(scm_path, 'study-sites', site_name, 'DEMs', site_name + '*.tif'))\n",
    "        if len(dem_fns) < 1:\n",
    "            continue\n",
    "        if ('ArcticDEM' in dem_fns[0]) | ('USGS' in dem_fns[0]):\n",
    "            dem_fn = [x for x in dem_fns if '_geoid.tif' in x][0]\n",
    "        else:\n",
    "            dem_fn = dem_fns[0]\n",
    "\n",
    "        # Calculate hyspometric index using DEM and AOI\n",
    "        hi, hi_category = calculate_hypsometric_index(dem_fn, aoi)\n",
    "        if type(hi) != str:\n",
    "            # Add to training data table\n",
    "            training_data_site['Hypsometric_Index'] = hi\n",
    "            training_data_site['Hypsometric_Index_Category'] = hi_category\n",
    "            # Concatenate site training data to full training data dataframe\n",
    "            training_data = pd.concat([training_data, training_data_site])\n",
    "\n",
    "    # Save subregion training data to file\n",
    "    training_data.rename(columns={'site_name_x': 'site_name'}, inplace=True)\n",
    "    training_data.to_csv(training_data_fn, index=False)\n",
    "    print('Subregion training data saved to file:', training_data_fn)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57840823-1f71-42ff-a887-53c6deeb4c35",
   "metadata": {},
   "source": [
    "## Scale each feature and label column in training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee96af1f-8dcf-44f7-ab4f-ed60509229b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import joblib\n",
    "\n",
    "# Grab training data file names\n",
    "training_data_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'training_data*.csv')))\n",
    "training_data_fns = [x for x in training_data_fns if '_scaled' not in x]\n",
    "\n",
    "# Define columns to scale\n",
    "scale_columns = ['snowline_elevs_median_m', 'SCA_m2', 'AAR', 'ELA_from_AAR_m', \n",
    "                 'Temperature_Celsius', 'Temperature_Celsius_Adjusted', \n",
    "                 'Precipitation_Meters', 'Cumulative_Precipitation_mwe', \n",
    "                 'Snowfall_mwe', 'Cumulative_Snowfall_mwe', \n",
    "                 'Snowmelt_mwe', 'Cumulative_Snowmelt_mwe',\n",
    "                 'Positive_Degree_Days', 'Cumulative_Positive_Degree_Days',\n",
    "                 'Area', 'Zmed', 'Slope', 'Aspect', 'Lmax', 'TermType', 'Surging', 'Hypsometric_Index']\n",
    "\n",
    "# Define the scaler to apply to training data\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Iterate over training data file names\n",
    "for training_data_fn in training_data_fns:\n",
    "\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name = training_data_fn.split('training_data_')[1].split('.csv')[0]\n",
    "    \n",
    "    # Check if scaled training data already exists in file\n",
    "    training_data_scaled_fn = training_data_fn[:-4] + '_scaled.csv'\n",
    "    if os.path.exists(training_data_scaled_fn):\n",
    "        print('Scaled subregion training data already exists in file, skipping...')\n",
    "        continue\n",
    "    \n",
    "    # Load training data\n",
    "    training_data = pd.read_csv(training_data_fn)\n",
    "\n",
    "    # Fit the scaler to training data\n",
    "    scaler_fit = scaler.fit(training_data[scale_columns])\n",
    "    \n",
    "    # Apply fit scaler to training data\n",
    "    training_data_std = training_data.copy(deep=True)\n",
    "    training_data_std[scale_columns] = scaler_fit.transform(training_data[scale_columns])\n",
    "\n",
    "    # Save fit scaler to file\n",
    "    scaler_fit_fn = os.path.join(scm_path, 'machine_learning', \n",
    "                                 'scaler_fit_' + subregion_name + '.gz')\n",
    "    joblib.dump(scaler_fit, scaler_fit_fn)\n",
    "    print('Scaler saved to file:', scaler_fit_fn)\n",
    "\n",
    "    # Save scaled training data to file\n",
    "    training_data_std.to_csv(training_data_scaled_fn, index=False)\n",
    "    print('Scaled training data saved to file:', training_data_scaled_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0ca01-f60b-4d82-a70f-1b6ab1b9f01b",
   "metadata": {},
   "source": [
    "## Print value range for each column to ensure success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b63a8b-fb7f-4f91-8311-8afe8747ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab scaled training data file names\n",
    "training_data_scaled_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'training_data*_scaled.csv')))\n",
    "for i, fn in enumerate(training_data_scaled_fns):\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name = fn.split('training_data_')[1].split('.csv')[0]\n",
    "    print('\\n' + subregion_name)\n",
    "    # Load scaled training data\n",
    "    training_data_scaled = pd.read_csv(fn)\n",
    "    # Print the range for each column\n",
    "    for column in scale_columns:\n",
    "        min_val, max_val = np.min(training_data_scaled[column]), np.max(training_data_scaled[column])\n",
    "        print(f'{np.round(min_val, 3)} {np.round(max_val, 3)} {column}')\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
