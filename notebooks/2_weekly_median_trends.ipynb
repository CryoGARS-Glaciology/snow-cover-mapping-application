{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fee1d3-89a0-4332-91af-77fbee8cc9ed",
   "metadata": {},
   "source": [
    "# Calculate median weekly trends in snowlines for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ded5b9-6a41-4e78-aa7d-dbec024f6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from scipy.stats import iqr\n",
    "from shapely import wkt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89c723-245e-4769-8633-a503980f94e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "# study_sites_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/'\n",
    "# site_names = sorted(os.listdir(study_sites_path))\n",
    "# site_names = [x for x in site_names if (not x.startswith('.')) \n",
    "#               and (os.path.exists(os.path.join(study_sites_path, x, x + '_snowlines.csv')))]\n",
    "# print('Number of sites with compiled snowlines files = ', len(site_names))\n",
    "# site_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb97356-a7e0-4c83-af84-ceee07b044fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load and compile AOIs\n",
    "aois_path = os.path.join(scm_path, 'all_AOIs')\n",
    "aois_fn = 'all_aois.shp'\n",
    "# check if aois path exists\n",
    "if not os.path.exists(aois_path):\n",
    "    os.mkdir(aois_path)\n",
    "# check if all aois shapefile exists\n",
    "if not os.path.exists(os.path.join(aois_path, aois_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    aois = gpd.GeoDataFrame()\n",
    "    for site_name in tqdm(site_names):\n",
    "        aoi_path = os.path.join(study_sites_path, site_name, 'AOIs')\n",
    "        aoi_fns = glob.glob(os.path.join(aoi_path, '*RGI*.shp'))\n",
    "        if len(aoi_fns) > 0:\n",
    "            aoi_fn = aoi_fns[0]\n",
    "            aoi = gpd.read_file(aoi_fn)\n",
    "            aoi = aoi.to_crs('EPSG:4326')\n",
    "            aois = pd.concat([aois, aoi])\n",
    "    aois.reset_index(drop=True, inplace=True)\n",
    "    aois.to_file(os.path.join(aois_path, aois_fn), index=False)\n",
    "    print('All glacier boundaries saved to file: ', os.path.join(aois_path, aois_fn))\n",
    "\n",
    "else:\n",
    "    # load from file if it already exists\n",
    "    aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "    print('All glacier boundaries loaded from file.')\n",
    "\n",
    "aois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a181f582-7cd9-4715-ba62-08f12c3b1d44",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load and compile snowlines\n",
    "snowlines_path = os.path.join(scm_path, 'all_snowlines')\n",
    "snowlines_fn = 'all_snowlines.csv'\n",
    "# check if snowlines path exists\n",
    "if not os.path.exists(snowlines_path):\n",
    "    os.mkdir(snowlines_path)\n",
    "# check if all snowlines CSV exists\n",
    "if not os.path.exists(os.path.join(snowlines_path, snowlines_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    snowlines = pd.DataFrame()\n",
    "    for site_name in tqdm(site_names):\n",
    "        snowline_path = os.path.join(study_sites_path, site_name)\n",
    "        snowline_fns = glob.glob(os.path.join(snowline_path, '*_snowlines.csv'))\n",
    "        if len(snowline_fns) > 0:\n",
    "            snowline_fn = snowline_fns[0]\n",
    "            snowline = pd.read_csv(snowline_fn)\n",
    "            snowlines = pd.concat([snowlines, snowline])\n",
    "    snowlines.reset_index(drop=True, inplace=True)\n",
    "    snowlines.to_csv(os.path.join(snowlines_path, snowlines_fn), index=False)\n",
    "    print('All snowlines saved to file: ', os.path.join(snowlines_path, snowlines_fn))\n",
    "\n",
    "else:\n",
    "    # load from file if it already exists\n",
    "    snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "    print('All snowlines loaded from file.')\n",
    "\n",
    "snowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23ff0d-5260-47ad-86d0-9a053e716cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Calculate weekly median trends for each site\n",
    "snowlines_medians_fn = 'all_snowlines_weekly_median_trends.csv'\n",
    "if not os.path.exists(os.path.join(snowlines_path, snowlines_medians_fn)):\n",
    "    # add week-of-year (WOY column to snowlines\n",
    "    snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "    snowlines['WOY'] = snowlines['datetime'].dt.isocalendar().week\n",
    "    # determine columns to calculate weekly stats\n",
    "    columns = ['AAR', 'snowline_elevs_median_m', 'SCA_m2', 'ELA_from_AAR_m']\n",
    "    snowlines_medians = pd.DataFrame()\n",
    "    for site_name in tqdm(snowlines['site_name'].drop_duplicates().values):\n",
    "        # subset snowlines to site\n",
    "        snowlines_site = snowlines.loc[snowlines['site_name']==site_name]\n",
    "        # calculate weekly quartile trends\n",
    "        q1 = snowlines_site[['WOY'] + columns].groupby(by='WOY').quantile(0.25)\n",
    "        q1.columns = [x + '_P25' for x in q1.columns]\n",
    "        q2 = snowlines_site[['WOY'] + columns].groupby(by='WOY').quantile(0.5)\n",
    "        q2.columns = [x + '_P50' for x in q2.columns]\n",
    "        q3 = snowlines_site[['WOY'] + columns].groupby(by='WOY').quantile(0.75)\n",
    "        q3.columns = [x + '_P75' for x in q3.columns]\n",
    "        qs = pd.merge(q1, pd.merge(q2, q3, on='WOY'), on='WOY')\n",
    "        qs = qs.reindex(sorted(qs.columns), axis=1)\n",
    "        qs['WOY'] = qs.index\n",
    "        qs['site_name'] = site_name\n",
    "        # concatenate to medians dataframe\n",
    "        snowlines_medians = pd.concat([snowlines_medians, qs])\n",
    "    # save to file\n",
    "    snowlines_medians.to_csv(os.path.join(snowlines_path, snowlines_medians_fn), index=False)\n",
    "    print('Median weekly snow trends saved to file: ', os.path.join(snowlines_path, snowlines_medians_fn))\n",
    "        \n",
    "else:\n",
    "    snowlines_medians = pd.read_csv(os.path.join(snowlines_path, snowlines_medians_fn))\n",
    "    print('Median weekly snow trends loaded from file.')\n",
    "    \n",
    "snowlines_medians\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec25ef-248b-4fff-8a5e-b7ac70f2827e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Compile RGI characteristics and minimum snow cover median statistics\n",
    "min_snow_cover_stats_fn = 'min_snow_cover_stats.csv'\n",
    "# check if exists in directory\n",
    "if not os.path.exists(os.path.join(snowlines_path, min_snow_cover_stats_fn)):\n",
    "    # initialize dataframe for RGI stats and minimum snow cover statts\n",
    "    min_snow_cover_stats = pd.DataFrame()\n",
    "    \n",
    "    # iterate over site names in median snowlines dataframe\n",
    "    for site_name in tqdm(sorted(snowlines_medians['site_name'].drop_duplicates().values)):\n",
    "        # grab AOI for site\n",
    "        aoi_site = aois.loc[aois['RGIId']==site_name, :]\n",
    "        # grab median snowline stats for site\n",
    "        snowlines_medians_site = snowlines_medians.loc[snowlines_medians['site_name']==site_name, :]\n",
    "        # calculate min median stats\n",
    "        median_columns = [x for x in snowlines_medians.columns if 'P50' in x]\n",
    "        for column in median_columns:\n",
    "            if (column=='ELA_from_AAR_m_P50') or (column=='snowline_elevs_median_m_P50'):\n",
    "                aoi_site[column+'_max'] = snowlines_medians_site[column].max()\n",
    "            else:\n",
    "                aoi_site[column+'_min'] = snowlines_medians_site[column].min()\n",
    "        # concatenate to full dataframe\n",
    "        min_snow_cover_stats = pd.concat([min_snow_cover_stats, aoi_site])\n",
    "\n",
    "    # add subregion names and colors\n",
    "    min_snow_cover_stats[['Subregion', 'color']] = '', ''\n",
    "    min_snow_cover_stats[['O1Region', 'O2Region']] = min_snow_cover_stats[['O1Region', 'O2Region']].astype(int)\n",
    "    for o1, o2 in min_snow_cover_stats[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "        min_snow_cover_stats.loc[(min_snow_cover_stats['O1Region']==o1) \n",
    "                                 & (min_snow_cover_stats['O2Region']==o2), ['Subregion', 'color']] = f.determine_subregion_name_color(o1, o2)\n",
    "    # save to file\n",
    "    min_snow_cover_stats.to_csv(os.path.join(snowlines_path, min_snow_cover_stats_fn), index=False)\n",
    "    print('Minimum median snow cover stats saved to file: ', os.path.join(snowlines_path, min_snow_cover_stats_fn))\n",
    "        \n",
    "else:\n",
    "    # load from file\n",
    "    min_snow_cover_stats = pd.read_csv(os.path.join(snowlines_path, min_snow_cover_stats_fn))\n",
    "    print('Minimum median snow cover stats loaded from file.')\n",
    "\n",
    "# reformat as GeoDataFrame\n",
    "min_snow_cover_stats['geometry'] = min_snow_cover_stats['geometry'].apply(wkt.loads)\n",
    "min_snow_cover_stats = gpd.GeoDataFrame(min_snow_cover_stats, crs='EPSG:4326')\n",
    "min_snow_cover_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2ecd1-99bf-4f9c-90d7-b8e5138c2aae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
