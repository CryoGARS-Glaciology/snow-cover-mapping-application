{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57798819-82ed-477d-81ad-a1429bad6670",
   "metadata": {},
   "source": [
    "# Assess ELA response to cumulative PDDs from observations and modeled conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d102a4-ef90-4891-aae6-eefcfb4f0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from scipy.interpolate import RegularGridInterpolator\n",
    "import sys\n",
    "import seaborn as sns\n",
    "# Suppress future warning from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf1041-b03b-41e1-a43e-850ccfaef58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e7158-398e-4d53-aeb7-39190bdf2347",
   "metadata": {},
   "source": [
    "## Load glacier boundaries, ERA5-Land data, and compiled snow cover stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e24c99-f8a5-4e13-824f-6621f227b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load glacier boundaries with climate clusters\n",
    "aois_fn = os.path.join(scm_path, 'compiled_data', 'all_aois_climate_cluster.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All AOIs with climate clusters loaded from file.')\n",
    "\n",
    "# -----Load ERA data\n",
    "eras_fn = os.path.join(scm_path, 'compiled_data', 'all_era_data.csv')\n",
    "eras = pd.read_csv(eras_fn)\n",
    "# format dates as datetimes\n",
    "eras['Date'] = pd.to_datetime(eras['Date'])\n",
    "print('All ERA data loaded from file.')\n",
    "\n",
    "# -----Load compiled snowlines\n",
    "snowlines_fn = os.path.join(scm_path, 'compiled_data', 'all_snowlines.csv')\n",
    "snowlines = pd.read_csv(snowlines_fn)\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "print('All snowlines loaded from file.')\n",
    "# snowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39a44a-f3cf-495d-a9ec-8071606c734e",
   "metadata": {},
   "source": [
    "## Estimate and save modeled monthly transient ELAs at each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c66ac-8b4c-4c92-8a31-38a4bb1d5071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod_elas_fn = os.path.join(scm_path, 'Rounce_et_al_2023', 'modeled_elas.csv')\n",
    "if os.path.exists(mod_elas_fn):\n",
    "    mod_elas = pd.read_csv(mod_elas_fn, index_col=0)\n",
    "    print('Modeled ELAs loaded from file.')\n",
    "else:\n",
    "    \n",
    "    # load binned model data\n",
    "    bin_fns = sorted(glob.glob(os.path.join(scm_path, 'Rounce_et_al_2023', 'binned', '*.nc')))\n",
    "    \n",
    "    # remove binned file names for sites without snow cover observations\n",
    "    aoi_ids = [x[7:] for x in aois['RGIId'].drop_duplicates().values]\n",
    "    bin_fns = [x for x in bin_fns if os.path.basename(x)[0:7] in aoi_ids]\n",
    "    \n",
    "    # iterate over binned file names\n",
    "    i=0\n",
    "    for bin_fn in tqdm(bin_fns):\n",
    "        # open binned data\n",
    "        bin = xr.open_dataset(bin_fn)\n",
    "        rgi_id = bin.RGIId.data[0] # grab RGI ID\n",
    "\n",
    "        # subset data to 2013 on\n",
    "        Itime = np.ravel(np.argwhere(bin.time.dt.year.data >= 2013))\n",
    "        bin = bin.isel(time=Itime)\n",
    "        \n",
    "        # iterate over months\n",
    "        elas = np.zeros(len(bin.time.data)) # initialize transient ELAs\n",
    "        for j in range(0,len(bin.time.data)):\n",
    "            # subset binned data to datetime\n",
    "            bin_time = bin.isel(time=j)\n",
    "            dt = bin.time.data[j]\n",
    "            ba = bin_time.bin_massbalclim_monthly.data[0]\n",
    "            h = bin_time.bin_surface_h_initial.data[0]\n",
    "            # estimate transient ELA\n",
    "            if any(ba < 0) & any(ba > 0):\n",
    "                Iela = np.abs(bin_time.bin_massbalclim_monthly.data[0]).argmin()\n",
    "                elas[j] = h[Iela]\n",
    "            elif all(ba > 0):\n",
    "                elas[j] = np.min(h)\n",
    "            elif all(ba < 0):\n",
    "                elas[j] = np.max(h)\n",
    "            else:\n",
    "                elas[j] = np.nan\n",
    "                \n",
    "        # compile in dataframe\n",
    "        df = pd.DataFrame({rgi_id: elas}, index=bin.time.data)\n",
    "        if i==0:\n",
    "            mod_elas = df\n",
    "        else:\n",
    "            mod_elas[rgi_id] = elas\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    # save to file\n",
    "    mod_elas.to_csv(mod_elas_fn, index=True)\n",
    "    print('Modeled transient ELAs saved to file:', mod_elas_fn)\n",
    "\n",
    "mod_elas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e141e2f-0ec4-4474-b754-f15f1cee35aa",
   "metadata": {},
   "source": [
    "## Fit linear trendlines to modeled and observed ELAs and PDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f990c857-f9dd-4541-b11e-d4b0b9d7e51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Modeled\n",
    "# Add Year and month columns to ERA and modeled ELA data\n",
    "eras['Year'] = eras['Date'].dt.year\n",
    "eras['Month'] = eras['Date'].dt.month\n",
    "mod_elas['Year'] = pd.DatetimeIndex(mod_elas.index).year\n",
    "mod_elas['Month'] = pd.DatetimeIndex(mod_elas.index).month\n",
    "# Resample ERA data to monthly resolution to match modeled data\n",
    "eras_gb = eras.groupby(by=['site_name', 'Year', 'Month'])[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].mean().reset_index()\n",
    "# Remove dates outside 2013-2022\n",
    "eras_gb = eras_gb.loc[(eras_gb['Year'] >= 2013) & (eras_gb['Year'] < 2023)]\n",
    "# Remove dates after September\n",
    "eras_gb = eras_gb.loc[eras_gb['Month'] < 9]\n",
    "mod_elas = mod_elas.loc[mod_elas['Month'] < 9]\n",
    "mod_elas.drop(columns=['Year', 'Month'], inplace=True)\n",
    "years = eras_gb['Year'].values.astype(str)\n",
    "months = ['0' + str(x) if x < 10 else str(x) for x in eras_gb['Month'].values]\n",
    "eras_gb['Dates'] = [np.datetime64(year + '-' + month + '-01') for year, month in list(zip(years, months))]\n",
    "dates = eras_gb['Dates'].drop_duplicates().values\n",
    "# rearrange dataframe\n",
    "i = 0\n",
    "for rgi_id in tqdm(eras_gb['site_name'].drop_duplicates().values):\n",
    "    if i==0:\n",
    "        pdds_df = pd.DataFrame({rgi_id: eras_gb.loc[eras_gb['site_name']==rgi_id, \n",
    "                               'Cumulative_Positive_Degree_Days'].values}, index=dates)\n",
    "        sf_df = pd.DataFrame({rgi_id: eras_gb.loc[eras_gb['site_name']==rgi_id, \n",
    "                              'Cumulative_Snowfall_mwe'].values}, index=dates)\n",
    "    else:\n",
    "        pdds_df[rgi_id] = eras_gb.loc[eras_gb['site_name']==rgi_id, 'Cumulative_Positive_Degree_Days'].values\n",
    "        sf_df[rgi_id] = eras_gb.loc[eras_gb['site_name']==rgi_id, 'Cumulative_Snowfall_mwe'].values\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181e0f4-1acb-47b3-8aca-f50053088c5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit linear trendlines to PDDs + Snowfall = ELAs\n",
    "fits_mod_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(mod_elas.columns):\n",
    "    # subset data\n",
    "    mod_elas_site = mod_elas[rgi_id].values\n",
    "    pdds_site = pdds_df[rgi_id].values\n",
    "    sf_site = sf_df[rgi_id].values\n",
    "    # fit linear trendline\n",
    "    X = np.concatenate([pdds_site.reshape(-1,1), sf_site.reshape(-1,1)], axis=1)\n",
    "    y = mod_elas_site.reshape(-1,1)\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    # save in dataframe\n",
    "    fit_df = pd.DataFrame({'RGIId': [rgi_id], \n",
    "                           'O1Region': [aois.loc[aois['RGIId']==rgi_id, 'O1Region'].values[0]],\n",
    "                           'O2Region': [aois.loc[aois['RGIId']==rgi_id, 'O2Region'].values[0]],\n",
    "                           'Subregion': [aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values[0]],\n",
    "                           'cluster': [aois.loc[aois['RGIId']==rgi_id, 'cluster'].values[0]],\n",
    "                           'clustName': [aois.loc[aois['RGIId']==rgi_id, 'clustName'].values[0]],\n",
    "                           'coef_PDD': [model.coef_[0][0]],\n",
    "                           'coef_Snowfall': [model.coef_[0][1]],\n",
    "                           'score': [model.score(X, y)]})\n",
    "    fits_mod_df = pd.concat([fits_mod_df, fit_df])\n",
    "    # plot\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.plot(X[:,0], y, '.')\n",
    "    # ax.set_xlabel('$\\Sigma$PDDs')\n",
    "    # ax.set_ylabel('Transient ELA [m]')\n",
    "    # plt.show()\n",
    "fits_mod_df.reset_index(drop=True, inplace=True)\n",
    "fits_mod_df\n",
    "\n",
    "# Save\n",
    "fits_mod_fn = os.path.join(scm_path, 'results', 'linear_fit_ela_modeled_pdd_snowfall.csv')\n",
    "fits_mod_df.to_csv(fits_mod_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_mod_fn)\n",
    "fits_mod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699f170-70eb-467e-928d-088916dd2739",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Observed\n",
    "# Merge snowlines and ERA data\n",
    "# Add Month column to snowlines\n",
    "snowlines['Month'] = pd.DatetimeIndex(snowlines['datetime']).month.values\n",
    "eras['Month'] = pd.DatetimeIndex(eras['Date']).month.values\n",
    "# Remove observations after August\n",
    "snowlines = snowlines.loc[snowlines['Month'] < 8]\n",
    "eras = eras.loc[eras['Month'] < 8]\n",
    "# Unify date columns for merging\n",
    "snowlines['Date'] = snowlines['datetime'].values.astype('datetime64[D]')\n",
    "eras['Date'] = eras['Date'].values.astype('datetime64[D]')\n",
    "# Merge on site name and dates\n",
    "merged = pd.merge(snowlines, eras, on=['site_name', 'Date'])\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0812972-6829-4948-92a1-73cf1be60205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over RGI IDs\n",
    "fits_obs_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(merged['site_name'].drop_duplicates().values):\n",
    "    merged_site = merged.loc[merged['site_name']==rgi_id]\n",
    "    # Fit linear trendline\n",
    "    X = merged_site[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].values\n",
    "    y = merged_site['ELA_from_AAR_m'].values\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    # save in dataframe\n",
    "    fit_df = pd.DataFrame({'RGIId': [rgi_id], \n",
    "                           'O1Region': [aois.loc[aois['RGIId']==rgi_id, 'O1Region'].values[0]],\n",
    "                           'O2Region': [aois.loc[aois['RGIId']==rgi_id, 'O2Region'].values[0]],\n",
    "                           'Subregion': [aois.loc[aois['RGIId']==rgi_id, 'Subregion'].values[0]],\n",
    "                           'cluster': [aois.loc[aois['RGIId']==rgi_id, 'cluster'].values[0]],\n",
    "                           'clustName': [aois.loc[aois['RGIId']==rgi_id, 'clustName'].values[0]],\n",
    "                           'coef_PDD': [model.coef_[0]],\n",
    "                           'coef_Snowfall': [model.coef_[1]],\n",
    "                           'score': [model.score(X, y)]})\n",
    "    fits_obs_df = pd.concat([fits_obs_df, fit_df])\n",
    "\n",
    "fits_obs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save\n",
    "fits_obs_fn = os.path.join(scm_path, 'results', 'linear_fit_ela_observed_pdd_snowfall.csv')\n",
    "fits_obs_df.to_csv(fits_obs_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_obs_fn)\n",
    "fits_obs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd70302c-a9d2-4d43-a8a1-8c5938d959ff",
   "metadata": {},
   "source": [
    "## Plot scatterplot for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d876a1-5e2e-48fd-bb09-08e3f790c045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge dataframes according to RGI ID\n",
    "fits_merged_df = fits_mod_df.merge(fits_obs_df, \n",
    "                                   on=['RGIId', 'O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName'], \n",
    "                                   suffixes=['_mod', '_obs'])\n",
    "# Remove wacky values\n",
    "for col in ['coef_PDD_mod', 'coef_PDD_obs', 'coef_Snowfall_mod', 'coef_Snowfall_obs']:\n",
    "    fits_merged_df.loc[np.abs(fits_merged_df[col]) > 1e33, col] = np.nan\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(2, 1, figsize=(10,12))\n",
    "for i, col in enumerate(['PDD', 'Snowfall']):\n",
    "    sns.scatterplot(data=fits_merged_df, x=f'coef_{col}_mod', y=f'coef_{col}_obs', hue='Subregion', ax=ax[i])\n",
    "    min_coef = np.nanmin(np.ravel(fits_merged_df[[f'coef_{col}_mod', f'coef_{col}_obs']].values))\n",
    "    max_coef = np.nanmax(np.ravel(fits_merged_df[[f'coef_{col}_mod', f'coef_{col}_obs']].values))\n",
    "    ax[i].plot(np.arange(min_coef, max_coef), np.arange(min_coef, max_coef), '-k')\n",
    "    if i==0:\n",
    "        units = col\n",
    "    else:\n",
    "        units = 'snow m.w.e.'\n",
    "    ax[i].set_xlabel(f'Model coefficient [{units}/m]')\n",
    "    ax[i].set_ylabel(f'Observed coefficient [{units}/m]')\n",
    "    ax[i].grid()\n",
    "\n",
    "# Add some annotations\n",
    "ax[0].text(15, 5, 'Higher OBSERVED \\nELA sensitivity \\nto PDDs', \n",
    "           horizontalalignment='center', fontsize=12, fontweight='bold')\n",
    "ax[0].text(10, 22, 'Higher MODELED \\nELA sensitivity \\nto PDDs', \n",
    "           horizontalalignment='center', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "fig_fn = os.path.join(scm_path, 'snow-cover-mapping-application', 'figures', \n",
    "                      'ela_sensitivity_modeled_observed_comparison.png')\n",
    "fig.savefig(fig_fn, dpi=300)\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7eab94-1f1d-4abd-9a41-060de963b308",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
