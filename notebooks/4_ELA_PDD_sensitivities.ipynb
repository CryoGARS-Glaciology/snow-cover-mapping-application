{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57798819-82ed-477d-81ad-a1429bad6670",
   "metadata": {},
   "source": [
    "# Estimate ELAs and fit linear trendlines to $\\Sigma PDDs + \\Sigma Snowfall = h_{sl}$ to assess ELA sensivities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d102a4-ef90-4891-aae6-eefcfb4f0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf1041-b03b-41e1-a43e-850ccfaef58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define path to data\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "# scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e7158-398e-4d53-aeb7-39190bdf2347",
   "metadata": {},
   "source": [
    "## Load glacier boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e24c99-f8a5-4e13-824f-6621f227b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load glacier boundaries\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All AOIs loaded from file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d4393a-db98-4680-93db-8b62836c91ad",
   "metadata": {},
   "source": [
    "## Estimate and save ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebf1202-8e05-4481-86c5-00722094ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Monthly mean snowline altitudes\n",
    "# Check if results already exist in file\n",
    "obs_elas_monthly_fn = os.path.join(scm_path, 'analysis', 'observed_monthly_elas.csv')\n",
    "if os.path.exists(obs_elas_monthly_fn):\n",
    "    obs_elas_monthly = pd.read_csv(obs_elas_monthly_fn)\n",
    "    obs_elas_monthly['Date'] = pd.to_datetime(obs_elas_monthly['Date'])\n",
    "    print('Remotely-sensed monthly ELAs loaded from file.')\n",
    "else:\n",
    "    obs_elas_monthly = pd.DataFrame()\n",
    "\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load and compile snow cover stats\n",
    "        scs = pd.DataFrame()\n",
    "        sc_fns = sorted(glob.glob(os.path.join(scm_path, 'study-sites', rgi_id, 'imagery', 'snowlines', '*.csv')))\n",
    "        for fn in sc_fns:\n",
    "            sc = pd.read_csv(fn)\n",
    "            scs = pd.concat([scs, sc], axis=0)\n",
    "        scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "        # Remove wonky ELA values\n",
    "        scs.loc[scs['ELA_from_AAR_m'] > 1e10, 'ELA_from_AAR_m'] = np.nan\n",
    "        # Add Year and Month columns\n",
    "        scs['Year'] = pd.DatetimeIndex(scs['datetime']).year\n",
    "        scs['Month'] = pd.DatetimeIndex(scs['datetime']).month\n",
    "\n",
    "        # Remove pre-2016 values\n",
    "        scs = scs.loc[scs['Year'] >= 2016]\n",
    "\n",
    "        # Calculate the mean monthly snowline altitudes at each site\n",
    "        site_elas_monthly = scs.groupby(by=['Year', 'Month'])['ELA_from_AAR_m'].mean().reset_index()\n",
    "\n",
    "        # Add mean monthly PDDs and snowfall to dataframe\n",
    "        era_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'ERA', f'{rgi_id}_ERA5_daily_means.csv')\n",
    "        era = pd.read_csv(era_fn)\n",
    "        era['Date'] = pd.to_datetime(era['Date'])\n",
    "        era['Year'] = pd.DatetimeIndex(era['Date']).year\n",
    "        era['Month'] = pd.DatetimeIndex(era['Date']).month\n",
    "        era_monthly = era.groupby(by=['Year', 'Month'])[['positive_degree_days_annual_sum', \n",
    "                                                         'mean_total_precipitation_sum_annual_sum']].mean().reset_index()\n",
    "        site_elas_monthly = site_elas_monthly.merge(era_monthly, on=['Year', 'Month'])\n",
    "        # Add RGI ID and minmium ELA\n",
    "        site_elas_monthly['RGIId'] = rgi_id\n",
    "        \n",
    "        # Add to full dataframe\n",
    "        obs_elas_monthly = pd.concat([obs_elas_monthly, site_elas_monthly], axis=0)\n",
    "    \n",
    "    # Add Date column\n",
    "    obs_elas_monthly['Date'] = [np.datetime64(f'{year}-{month}-01') if month >=10 else \n",
    "                                np.datetime64(f'{year}-0{month}-01')\n",
    "                                for year, month in obs_elas_monthly[['Year', 'Month']].values]\n",
    "\n",
    "    # Reorder columns\n",
    "    obs_elas_monthly = obs_elas_monthly[['RGIId', 'Date', 'Year', 'Month', 'ELA_from_AAR_m', \n",
    "                                         'positive_degree_days_annual_sum', \n",
    "                                         'mean_total_precipitation_sum_annual_sum']]\n",
    "    # Save to file\n",
    "    obs_elas_monthly.to_csv(obs_elas_monthly_fn, index=False)\n",
    "    print('Remotely-sensed monthly ELAs saved to file:', obs_elas_monthly_fn)\n",
    "\n",
    "obs_elas_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b384c62c-06fd-48b7-b697-7ec020d190a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Annual ELAs\n",
    "# Check if ELAs already exist in file\n",
    "obs_elas_annual_fn = os.path.join(scm_path, 'analysis', 'observed_annual_elas.csv')\n",
    "if os.path.exists(obs_elas_annual_fn):\n",
    "    obs_elas_annual = pd.read_csv(obs_elas_annual_fn)\n",
    "    obs_elas_annual['Date'] = pd.to_datetime(obs_elas_annual['Date'])\n",
    "    print('Remotely-sensed annual ELAs loaded from file.')\n",
    "else:\n",
    "    # Identify indices of maximum annual ELA\n",
    "    Imax = obs_elas_monthly.groupby(by=['RGIId', 'Year'])['ELA_from_AAR_m'].idxmax().dropna().values.astype(int)\n",
    "    obs_elas_annual = obs_elas_monthly.loc[Imax, ['RGIId', 'Date', 'Year', 'ELA_from_AAR_m', \n",
    "                                                  'positive_degree_days_annual_sum', 'mean_total_precipitation_sum_annual_sum']]\n",
    "    # Save to file\n",
    "    obs_elas_annual.to_csv(obs_elas_annual_fn, index=False)\n",
    "    print('Remotely-sensed annual ELAs saved to file:', obs_elas_annual_fn)\n",
    "\n",
    "obs_elas_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e141e2f-0ec4-4474-b754-f15f1cee35aa",
   "metadata": {},
   "source": [
    "## Fit linear models to monthly snowline altitudes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb7540a-0da3-49e3-9dab-3f8a4542e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_Xy(df, X_cols, y_cols, scaler_type=StandardScaler()):\n",
    "    df_scaled = df.copy()\n",
    "    scaler = scaler_type.fit(df_scaled[X_cols + y_cols])\n",
    "    df_scaled[X_cols + y_cols] = scaler.transform(df_scaled[X_cols + y_cols])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "def subset_Xy_data(X, y, p=0.9):\n",
    "    # sort the X data by increasing PDDs\n",
    "    Iargsort = X[:, 0].argsort()\n",
    "    X_sorted, y_sorted = X[Iargsort,:], y[Iargsort]\n",
    "    # select the middle p% of the data\n",
    "    n10 = int(len(X)*(1-p)) # number of points in 20% of the data\n",
    "    X_sorted_subset = X_sorted[int(n10/2):-int(n10/2), :]\n",
    "    y_sorted_subset = y_sorted[int(n10/2):-int(n10/2)]\n",
    "\n",
    "    return X_sorted_subset, y_sorted_subset\n",
    "    \n",
    "def linear_fit(X, y):\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    score = model.score(X, y)\n",
    "    coefs = np.ravel(model.coef_)\n",
    "    return coefs, score\n",
    "    \n",
    "# Define function for K-folds cross-validation model fitting\n",
    "def kfolds_linear_fit(X, y, n_folds=5):\n",
    "    # Define K-folds\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    # Initialize parameters\n",
    "    coefs_PDD, coefs_snowfall, scores = [], [], []\n",
    "    # Iterate over fold indices\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        # Split X and y into training and testing\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Fit model to testing\n",
    "        coefs, score = linear_fit(X_train, y_train)\n",
    "        coefs_PDD.append(coefs[0])\n",
    "        coefs_snowfall.append(coefs[1])\n",
    "        scores.append(score)\n",
    "    # Calculate stats, compile in dataframe\n",
    "    df = pd.DataFrame({'coef_PDD_mean': [np.nanmean(coefs_PDD)],\n",
    "                       'coef_PDD_std': [np.nanstd(coefs_PDD)],\n",
    "                       'coef_PDD_median': [np.nanmedian(coefs_PDD)],\n",
    "                       'coef_PDD_MAD': [MAD(coefs_PDD)],\n",
    "                       'coef_snowfall_mean': [np.nanmean(coefs_snowfall)],\n",
    "                       'coef_snowfall_std': [np.nanstd(coefs_snowfall)],\n",
    "                       'coef_snowfall_median': [np.nanmedian(coefs_snowfall)],\n",
    "                       'coef_snowfall_MAD': [MAD(coefs_snowfall)],\n",
    "                       'score_mean': [np.nanmean(scores)],\n",
    "                       'score_median': [np.nanmedian(scores)]\n",
    "                      })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9bcf87-9a85-4f92-b7a0-6ea91be81afa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_cols = ['positive_degree_days_annual_sum', 'mean_total_precipitation_sum_annual_sum']\n",
    "y_cols = ['ELA_from_AAR_m']\n",
    "\n",
    "fits_obs_monthly_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(obs_elas_monthly['RGIId'].drop_duplicates().values):\n",
    "    # subset and merge data\n",
    "    site_df = obs_elas_monthly.loc[obs_elas_monthly['RGIId']==rgi_id]\n",
    "    site_df.dropna(inplace=True)\n",
    "    # only include dates before October\n",
    "    site_df = site_df.loc[site_df['Date'].dt.month < 10]\n",
    "    # remove dates where PDD==0\n",
    "    site_df = site_df.loc[site_df['positive_degree_days_annual_sum'] > 0]\n",
    "    if len(site_df) >= 2:\n",
    "        # prep the X and y data\n",
    "        X = site_df[X_cols].values\n",
    "        y = site_df[y_cols].values\n",
    "        # subset to 80% to mitigate the impact of snowfall\n",
    "        # X_sub, y_sub = subset_Xy_data(X, y, p=0.8)\n",
    "        # fit linear trendline\n",
    "        fit_df = kfolds_linear_fit(X, y)\n",
    "        fit_df['RGIId'] = rgi_id\n",
    "        # add RGI regions and climate cluster to df\n",
    "        for col in ['O1Region', 'O2Region', 'Subregion']:\n",
    "            fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "        # concatenate to full dataframe\n",
    "        fits_obs_monthly_df = pd.concat([fits_obs_monthly_df, fit_df])\n",
    "\n",
    "fits_obs_monthly_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save to file\n",
    "fits_obs_monthly_fn = os.path.join(scm_path, 'analysis', 'linear_fit_observed_monthly_ela_pdd_snowfall.csv')\n",
    "fits_obs_monthly_df.to_csv(fits_obs_monthly_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_obs_monthly_fn)\n",
    "fits_obs_monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c28fc-7002-4897-9f86-df9a4feb629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fits_obs_monthly_df['coef_PDD_median'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712d424-80e6-4664-ada0-c084203e8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print('Overall PDD coef. stats:')\n",
    "print(f'\\tMedian = {np.nanmedian(fits_obs_monthly_df[\"coef_PDD_median\"])} \\tMAD = {MAD(fits_obs_monthly_df[\"coef_PDD_median\"], nan_policy=\"omit\")}')\n",
    "print(f'\\tMean = {np.nanmean(fits_obs_monthly_df[\"coef_PDD_median\"])} \\tstd. = {np.nanstd(fits_obs_monthly_df[\"coef_PDD_median\"])}\\n')\n",
    "\n",
    "print(fits_obs_monthly_df.groupby(by='Subregion')['coef_PDD_median'].median())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gscm-application",
   "language": "python",
   "name": "gscm-application"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
