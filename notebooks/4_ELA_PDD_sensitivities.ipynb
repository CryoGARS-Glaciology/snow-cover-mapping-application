{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57798819-82ed-477d-81ad-a1429bad6670",
   "metadata": {},
   "source": [
    "# Estimate and compare ELAs from observations and modeled conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d102a4-ef90-4891-aae6-eefcfb4f0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from scipy.interpolate import CubicSpline\n",
    "import sys\n",
    "import seaborn as sns\n",
    "# Suppress future warning from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ruptures as rpt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf1041-b03b-41e1-a43e-850ccfaef58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "# scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e7158-398e-4d53-aeb7-39190bdf2347",
   "metadata": {},
   "source": [
    "## Load glacier boundaries, ERA5-Land data, and compiled snow cover stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e24c99-f8a5-4e13-824f-6621f227b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load glacier boundaries with climate clusters\n",
    "aois_fn = os.path.join(scm_path, 'compiled_data', 'all_aois_climate_cluster.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All AOIs with climate clusters loaded from file.')\n",
    "\n",
    "# -----Load ERA data\n",
    "eras_fn = os.path.join(scm_path, 'compiled_data', 'all_era_data.csv')\n",
    "eras = pd.read_csv(eras_fn)\n",
    "# format dates as datetimes\n",
    "eras['Date'] = pd.to_datetime(eras['Date'])\n",
    "# rename \"site_name\" column to \"RGIId\"\n",
    "eras.rename(columns={'site_name': 'RGIId'}, inplace=True)\n",
    "# Add Year and Month columns\n",
    "eras['Year'] = pd.DatetimeIndex(eras['Date']).year\n",
    "eras['Month'] = pd.DatetimeIndex(eras['Date']).month\n",
    "print('All ERA data loaded from file.')\n",
    "\n",
    "# -----Load compiled snow cover statts\n",
    "scs_fn = os.path.join(scm_path, 'compiled_data', 'all_snow_cover_stats.csv')\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "# Remove wonky ELA values\n",
    "scs.loc[scs['ELA_from_AAR_m'] > 1e10, 'ELA_from_AAR_m'] = np.nan\n",
    "# Add Year and Month columns\n",
    "scs['Year'] = pd.DatetimeIndex(scs['datetime']).year\n",
    "scs['Month'] = pd.DatetimeIndex(scs['datetime']).month\n",
    "print('All snow cover stats loaded from file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39a44a-f3cf-495d-a9ec-8071606c734e",
   "metadata": {},
   "source": [
    "## Estimate and save ELAs\n",
    "\n",
    "### Modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244b0cf-72ec-41d6-af5d-e74f63ca7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for linearly extrapolating the ELA when modeled SMB < 0 everywhere\n",
    "def linear_fit(x, m, b):\n",
    "    return m*x + b\n",
    "    \n",
    "def extrapolate_ela_linear(X,y, Iend=8):\n",
    "    # optimize the linear fit\n",
    "    p, e = optimize.curve_fit(linear_fit, X[0:Iend+1], y[0:Iend+1])\n",
    "    # extrapolate where y=0\n",
    "    ela = linear_fit(0, *p)\n",
    "    return ela\n",
    "\n",
    "# def extrapolate_ela_piecewise_linear(X,y):\n",
    "#     # identify breakpoints\n",
    "#     algo = rpt.Pelt(model=\"rbf\").fit(signal)\n",
    "#     result = algo.predict(pen=10)\n",
    "                                 \n",
    "# def extrapolate_ela_cubic_spline(X,y):\n",
    "#     # check that X is increasing\n",
    "#     if X[1] < X[0]:\n",
    "#         spline = CubicSpline(np.flip(X), np.flip(y), bc_type='natural')\n",
    "#     else:\n",
    "#         spline = CubicSpline(X, y, bc_type='natural')\n",
    "#     ela = spline(0)\n",
    "    \n",
    "#     return ela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c66ac-8b4c-4c92-8a31-38a4bb1d5071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Monthly ELAs\n",
    "mod_elas_monthly_fn = os.path.join(scm_path, 'results', 'modeled_monthly_elas.csv')\n",
    "if os.path.exists(mod_elas_monthly_fn):\n",
    "    mod_elas_monthly = pd.read_csv(mod_elas_monthly_fn)\n",
    "    mod_elas_monthly['Date'] = pd.DatetimeIndex(mod_elas_monthly['Date'])\n",
    "    print('Modeled monthly ELAs loaded from file.')\n",
    "else:\n",
    "    \n",
    "    # load binned model data\n",
    "    bin_fns = sorted(glob.glob(os.path.join(scm_path, 'Rounce_et_al_2023', 'binned', '*.nc')))\n",
    "    \n",
    "    # remove binned file names for sites without snow cover observations\n",
    "    aoi_ids = [x[7:] for x in aois['RGIId'].drop_duplicates().values]\n",
    "    bin_fns = [x for x in bin_fns if os.path.basename(x)[0:7] in aoi_ids]\n",
    "\n",
    "    # initialize dataframe for results\n",
    "    mod_elas_monthly = pd.DataFrame()\n",
    "\n",
    "    # iterate over binned file names\n",
    "    i=0\n",
    "    for bin_fn in tqdm(bin_fns):\n",
    "        # open binned data\n",
    "        bin = xr.open_dataset(bin_fn)\n",
    "        rgi_id = bin.RGIId.data[0] # grab RGI ID\n",
    "\n",
    "        # grab data variables\n",
    "        h = bin.bin_surface_h_initial.data[0] # surface elevation [m]\n",
    "        b_sum = np.zeros((len(bin.time.data), len(h))) # cumulative SMB\n",
    "        times = [np.datetime64(x) for x in bin.time.data] # datetimes\n",
    "        months = list(pd.DatetimeIndex(times).month) # months of each datetime\n",
    "        elas = np.zeros(len(times)) # initialize transient ELAs\n",
    "\n",
    "        # iterate over each time period\n",
    "        for j, time in enumerate(times):\n",
    "            # subset binned data to time\n",
    "            bin_time = bin.isel(time=j)\n",
    "            # grab the SMB \n",
    "            b_sum[j,:] = bin_time.bin_massbalclim_monthly.data[0]\n",
    "            # add the previous SMB (restart the count in October)\n",
    "            if months[j] != 10: \n",
    "                b_sum[j,:] += b_sum[j-1,:]\n",
    "            # If all SMB > 0, ELA = minimum elevation\n",
    "            if all(b_sum[j,:] > 0):\n",
    "                elas[j] = np.min(h)\n",
    "            # If SMB is > 0 and < 0 in some places, linearly interpolate ELA\n",
    "            elif any(b_sum[j,:] < 0) & any(b_sum[j,:] > 0):\n",
    "                elas[j] = np.interp(0, np.flip(b_sum[j,:]), np.flip(h))\n",
    "            # If SMB < 0 everywhere, fit a piecewise linear fit and extrapolate for SMB=0\n",
    "            elif all(b_sum[j,:] < 0):\n",
    "                X, y = b_sum[j,:], h\n",
    "                elas[j] = extrapolate_ela_linear(X, y, Iend=5)\n",
    "\n",
    "        # compile in dataframe\n",
    "        df = pd.DataFrame({'Date': times,\n",
    "                           'ELA_m': elas})\n",
    "        \n",
    "        # Because each SMB value represents the total SMB for each month, add 1 month to the dates\n",
    "        df['Date'] = df['Date'] + pd.DateOffset(months=1)\n",
    "        df['RGIId'] = rgi_id\n",
    "\n",
    "        # Add ERA5 data for each date\n",
    "        eras_site = eras.loc[eras['RGIId']==rgi_id]\n",
    "        eras_site = eras_site[['Date', 'Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']]\n",
    "        df = df.merge(eras_site, on='Date')\n",
    "        mod_elas_monthly = pd.concat([mod_elas_monthly, df])\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    # Rearrange columns\n",
    "    mod_elas_monthly = mod_elas_monthly[['RGIId', 'Date', 'ELA_m', \n",
    "                                         'Cumulative_Positive_Degree_Days', \n",
    "                                         'Cumulative_Snowfall_mwe']]\n",
    "    # save to file\n",
    "    mod_elas_monthly.to_csv(mod_elas_monthly_fn, index=False)\n",
    "    print('Modeled transient ELAs saved to file:', mod_elas_monthly_fn)\n",
    "\n",
    "mod_elas_monthly.reset_index(drop=True, inplace=True)\n",
    "mod_elas_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc087845-963c-49a1-b45c-69951ee1eb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Annual ELAs\n",
    "mod_elas_annual_fn = os.path.join(scm_path, 'results', 'modeled_annual_elas.csv')\n",
    "if os.path.exists(mod_elas_annual_fn):\n",
    "    mod_elas_annual = pd.read_csv(mod_elas_annual_fn)\n",
    "    mod_elas_annual['Date'] = pd.to_datetime(mod_elas_annual['Date'])\n",
    "    print('Modeled annual ELAs loaded from file.')\n",
    "else:\n",
    "    # Add Year column\n",
    "    mod_elas_monthly['Year'] = pd.DatetimeIndex(mod_elas_monthly['Date']).year\n",
    "    # Identify the row of maximum ELA for each site and each year\n",
    "    Imax = mod_elas_monthly.groupby(by=['RGIId', 'Year'])['ELA_m'].idxmax().values\n",
    "    mod_elas_annual = mod_elas_monthly.iloc[Imax].reset_index(drop=True)\n",
    "    # Reorder columns\n",
    "    mod_elas_monthly = mod_elas_monthly[['RGIId', 'Date', 'Year', 'ELA_m', \n",
    "                                         'Cumulative_Positive_Degree_Days',\n",
    "                                         'Cumulative_Snowfall_mwe']]\n",
    "    # Save to file\n",
    "    mod_elas_annual.to_csv(mod_elas_annual_fn, index=False)\n",
    "    print('Modeled annual ELAs saved to file:', mod_elas_annual_fn)\n",
    "    \n",
    "mod_elas_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c806a0-69a1-4435-8ad9-514bdb1994a1",
   "metadata": {},
   "source": [
    "### Remotely-sensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd2503-d6ed-41e0-b2ca-7b4e058ef307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Monthly mean ELAs\n",
    "# Check if ELAs already exist in file\n",
    "obs_elas_monthly_fn = os.path.join(scm_path, 'results', 'observed_monthly_elas.csv')\n",
    "if os.path.exists(obs_elas_monthly_fn):\n",
    "    obs_elas_monthly = pd.read_csv(obs_elas_monthly_fn)\n",
    "    obs_elas_monthly['Date'] = pd.to_datetime(obs_elas_monthly['Date'])\n",
    "    print('Remotely-sensed monthly ELAs loaded from file.')\n",
    "else:\n",
    "    # Add Year and Month columns to snow cover stats\n",
    "    scs['Year'] = pd.DatetimeIndex(scs['datetime']).year\n",
    "    scs['Month'] = pd.DatetimeIndex(scs['datetime']).month\n",
    "    \n",
    "    # Calculate the mean. monthly ELA at each site\n",
    "    obs_elas_monthly = scs.groupby(by=['RGIId', 'Year', 'Month'])['ELA_from_AAR_m'].mean().reset_index()\n",
    "\n",
    "    # Add mean monthly PDDs and snowfall\n",
    "    eras['Year'] = pd.DatetimeIndex(eras['Date']).year\n",
    "    eras['Month'] = pd.DatetimeIndex(eras['Date']).month\n",
    "    eras_monthly = eras.groupby(by=['RGIId', 'Year', 'Month'])[['Cumulative_Positive_Degree_Days', \n",
    "                                                                'Cumulative_Snowfall_mwe']].mean().reset_index()\n",
    "    obs_elas_monthly = obs_elas_monthly.merge(eras_monthly, on=['RGIId', 'Year', 'Month'])\n",
    "    \n",
    "    # Add Date column\n",
    "    obs_elas_monthly['Date'] = [np.datetime64(f'{year}-{month}-01') if month >=10 else \n",
    "                                np.datetime64(f'{year}-0{month}-01')\n",
    "                                for year, month in obs_elas_monthly[['Year', 'Month']].values]\n",
    "\n",
    "    # Reorder columns\n",
    "    obs_elas_monthly = obs_elas_monthly[['RGIId', 'Date', 'Year', 'Month', 'ELA_from_AAR_m', \n",
    "                                         'Cumulative_Positive_Degree_Days', \n",
    "                                         'Cumulative_Snowfall_mwe']]\n",
    "    # Save to file\n",
    "    obs_elas_monthly.to_csv(obs_elas_monthly_fn, index=False)\n",
    "    print('Remotely-sensed monthly ELAs saved to file:', obs_elas_monthly_fn)\n",
    "\n",
    "obs_elas_monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b3e631-141f-42da-a126-d5a2b269d311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Annual ELAs\n",
    "# Check if ELAs already exist in file\n",
    "obs_elas_annual_fn = os.path.join(scm_path, 'results', 'observed_annual_elas.csv')\n",
    "if os.path.exists(obs_elas_annual_fn):\n",
    "    obs_elas_annual = pd.read_csv(obs_elas_annual_fn)\n",
    "    obs_elas_annual['Date'] = pd.to_datetime(obs_elas_annual['Date'])\n",
    "    print('Remotely-sensed annual ELAs loaded from file.')\n",
    "else:\n",
    "    # Identify indices of maximum annual ELA\n",
    "    Imax = scs.groupby(by=['RGIId', 'Year'])['ELA_from_AAR_m'].idxmax().dropna().values.astype(int)\n",
    "    obs_elas_annual = scs[['RGIId', 'datetime', 'Year', 'ELA_from_AAR_m']].iloc[Imax]\n",
    "    obs_elas_annual.rename(columns={'datetime': 'Date'}, inplace=True)\n",
    "    obs_elas_annual['Date'] = obs_elas_annual['Date'].values.astype('datetime64[D]')\n",
    "    # Grab with ERA5 data for each date\n",
    "    obs_elas_annual = obs_elas_annual.merge(eras[['RGIId', 'Date', \n",
    "                                                  'Cumulative_Positive_Degree_Days', \n",
    "                                                  'Cumulative_Snowfall_mwe']], on=['RGIId', 'Date'])\n",
    "    # Save to file\n",
    "    obs_elas_annual.to_csv(obs_elas_annual_fn, index=False)\n",
    "    print('Remotely-sensed annual ELAs saved to file:', obs_elas_annual_fn)\n",
    "\n",
    "obs_elas_annual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e415b-4392-433e-86d9-1f5acb3e41bd",
   "metadata": {},
   "source": [
    "## Compare modeled to observed ELAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bc4f5-4003-4a3b-b354-980f12070f4b",
   "metadata": {},
   "source": [
    "### Monthly ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa02b1-304e-4876-911b-54c80236728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge modeled and remotely-sensed modeled ELAs\n",
    "elas_monthly_merged = mod_elas_monthly[['RGIId', 'Date', 'ELA_m']].merge(obs_elas_monthly[['RGIId', 'Date', 'ELA_from_AAR_m']], \n",
    "                                                                         on=['RGIId', 'Date'])\n",
    "# Initialize dataframe for correlation coefficients\n",
    "corr_coefs_df = pd.DataFrame()\n",
    "# Iterate over RGI IDs\n",
    "for rgi_id in elas_monthly_merged['RGIId'].drop_duplicates().values:\n",
    "    # Subtract the minimum ELA for each site to avoid datum issues, s.t. ELAs are with respect to zero meters\n",
    "    elas_monthly_merged.loc[elas_monthly_merged['RGIId']==rgi_id, 'ELA_m'] -= mod_elas_monthly.loc[mod_elas_monthly['RGIId']==rgi_id, 'ELA_m'].min()\n",
    "    elas_monthly_merged.loc[elas_monthly_merged['RGIId']==rgi_id, 'ELA_from_AAR_m'] -= scs.loc[scs['RGIId']==rgi_id, 'ELA_from_AAR_m'].min()\n",
    "    # Calculate correlation coefficient\n",
    "    elas_monthly_merged_site = elas_monthly_merged.loc[elas_monthly_merged['RGIId']==rgi_id]\n",
    "    corr_coef_site = elas_monthly_merged_site[['ELA_from_AAR_m', 'ELA_m']].corr().values[0][1]\n",
    "    corr_coef_df = pd.DataFrame({'RGIId': [rgi_id], \n",
    "                                 'Correlation Coefficient': [corr_coef_site]})\n",
    "    corr_coefs_df = pd.concat([corr_coefs_df, corr_coef_df])\n",
    "\n",
    "# Rename columns\n",
    "corr_coefs_df.reset_index(drop=True, inplace=True)\n",
    "elas_monthly_merged.rename(columns={'ELA_m': 'ELA_mod_m', 'ELA_from_AAR_m': 'ELA_obs_m'}, inplace=True)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].hist(elas_monthly_merged['ELA_obs_m'] - elas_monthly_merged['ELA_mod_m'], bins=50)\n",
    "ax[0].set_xlabel('ELA$_{obs}$ - ELA$_{mod}$ [m]')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[1].hist(corr_coefs_df['Correlation Coefficient'], bins=50)\n",
    "ax[1].set_xlabel('Correlation Coefficients')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "elas_monthly_merged_fn = os.path.join(scm_path, 'results', 'ELAs_monthly_mod_obs_merged.csv')\n",
    "elas_monthly_merged.to_csv(elas_monthly_merged_fn, index=False)\n",
    "print('Mered monthly ELAs saved to file:', elas_monthly_merged_fn)\n",
    "corr_coefs_fn = os.path.join(scm_path, 'results', 'ELAs_monthly_correlation_coefficients.csv')\n",
    "corr_coefs_df.to_csv(corr_coefs_fn, index=False)\n",
    "print('Correlation coefficients saved to file:', corr_coefs_fn)\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f'Mean diff = {np.nanmean((elas_monthly_merged[\"ELA_obs_m\"] - elas_monthly_merged[\"ELA_mod_m\"]).values)} m')\n",
    "print(f'Std. diff = {np.nanstd((elas_monthly_merged[\"ELA_obs_m\"] - elas_monthly_merged[\"ELA_mod_m\"]).values)} m')\n",
    "print(f'Median diff = {np.nanmedian((elas_monthly_merged[\"ELA_obs_m\"] - elas_monthly_merged[\"ELA_mod_m\"]).values)} m')\n",
    "print(f'MAD diff = {MAD((elas_monthly_merged[\"ELA_obs_m\"] - elas_monthly_merged[\"ELA_mod_m\"]).values, nan_policy=\"omit\")} m')\n",
    "\n",
    "print('\\n Corr. Coef. stats:')\n",
    "print('Mean = ', np.nanmean(corr_coefs_df['Correlation Coefficient']))\n",
    "print('Std. = ', np.nanstd(corr_coefs_df['Correlation Coefficient']))\n",
    "print('Median = ', np.nanmedian(corr_coefs_df['Correlation Coefficient']))\n",
    "print('MAD = ', MAD(corr_coefs_df['Correlation Coefficient'], nan_policy=\"omit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0f5d4-7e6f-4e3d-ab43-9adb8bdbea9c",
   "metadata": {},
   "source": [
    "### Annual ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1860cb-7534-4f5d-af4e-ae1ce755a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge modeled and remotely-sensed modeled ELAs\n",
    "elas_annual_merged = obs_elas_annual[['RGIId', 'Year', 'ELA_from_AAR_m']].merge(mod_elas_annual[['RGIId', 'Year', 'ELA_m']],\n",
    "                                                                                on=['RGIId', 'Year'])\n",
    "# Initialize dataframe for correlation coefficients\n",
    "corr_coefs_df = pd.DataFrame()\n",
    "# Iterate over RGI IDs\n",
    "for rgi_id in elas_annual_merged['RGIId'].drop_duplicates().values:\n",
    "    # Subtract the minimum ELA for each site to avoid datum issues, s.t. ELAs are with respect to zero meters\n",
    "    elas_annual_merged.loc[elas_annual_merged['RGIId']==rgi_id, 'ELA_m'] -= mod_elas_monthly.loc[mod_elas_monthly['RGIId']==rgi_id, 'ELA_m'].min()\n",
    "    elas_annual_merged.loc[elas_annual_merged['RGIId']==rgi_id, 'ELA_from_AAR_m'] -= scs.loc[scs['RGIId']==rgi_id, 'ELA_from_AAR_m'].min()\n",
    "    # Calculate correlation coefficient\n",
    "    elas_annual_merged_site = elas_annual_merged.loc[elas_annual_merged['RGIId']==rgi_id]\n",
    "    corr_coef_site = elas_annual_merged_site[['ELA_from_AAR_m', 'ELA_m']].corr().values[0][1]\n",
    "    corr_coef_df = pd.DataFrame({'RGIId': [rgi_id], \n",
    "                                 'Correlation Coefficient': [corr_coef_site]})\n",
    "    corr_coefs_df = pd.concat([corr_coefs_df, corr_coef_df])\n",
    "    \n",
    "# Rename columns\n",
    "corr_coefs_df.reset_index(drop=True, inplace=True)\n",
    "elas_annual_merged.rename(columns={'ELA_m': 'ELA_mod_m', 'ELA_from_AAR_m': 'ELA_obs_m'}, inplace=True)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].hist(elas_annual_merged['ELA_obs_m'] - elas_annual_merged['ELA_mod_m'], bins=50)\n",
    "ax[0].set_xlabel('ELA$_{obs}$ - ELA$_{mod}$ [m]')\n",
    "ax[0].set_ylabel('Counts')\n",
    "ax[1].hist(corr_coefs_df['Correlation Coefficient'], bins=50)\n",
    "ax[1].set_xlabel('Correlation Coefficients')\n",
    "plt.show()\n",
    "\n",
    "# Save results\n",
    "elas_annual_merged_fn = os.path.join(scm_path, 'results', 'ELAs_annual_mod_obs_merged.csv')\n",
    "elas_annual_merged.to_csv(elas_annual_merged_fn, index=False)\n",
    "print('Mered monthly ELAs saved to file:', elas_annual_merged_fn)\n",
    "corr_coefs_fn = os.path.join(scm_path, 'results', 'ELAs_annual_correlation_coefficients.csv')\n",
    "corr_coefs_df.to_csv(corr_coefs_fn, index=False)\n",
    "print('Correlation coefficients saved to file:', corr_coefs_fn)\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f'Mean diff = {np.nanmean((elas_annual_merged[\"ELA_obs_m\"] - elas_annual_merged[\"ELA_mod_m\"]).values)} m')\n",
    "print(f'Std. diff = {np.nanstd((elas_annual_merged[\"ELA_obs_m\"] - elas_annual_merged[\"ELA_mod_m\"]).values)} m')\n",
    "print(f'Median diff = {np.nanmedian((elas_annual_merged[\"ELA_obs_m\"] - elas_annual_merged[\"ELA_mod_m\"]).values)} m')\n",
    "print(f'MAD diff = {MAD((elas_annual_merged[\"ELA_obs_m\"] - elas_annual_merged[\"ELA_mod_m\"]).values, nan_policy=\"omit\")} m')\n",
    "\n",
    "print('\\nCorr. Coef. stats:')\n",
    "print('Mean = ', np.nanmean(corr_coefs_df['Correlation Coefficient']))\n",
    "print('Std. = ', np.nanstd(corr_coefs_df['Correlation Coefficient']))\n",
    "print('Median = ', np.nanmedian(corr_coefs_df['Correlation Coefficient']))\n",
    "print('MAD = ', MAD(corr_coefs_df['Correlation Coefficient'], nan_policy=\"omit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e141e2f-0ec4-4474-b754-f15f1cee35aa",
   "metadata": {},
   "source": [
    "## Fit linear trendlines PDD sum + Snowfall sum. = ELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd58ea-3ada-42b7-8e9d-a0870114ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_Xy(df, X_cols, y_cols, scaler_type=StandardScaler()):\n",
    "    df_scaled = df.copy()\n",
    "    scaler = scaler_type.fit(df_scaled[X_cols + y_cols])\n",
    "    df_scaled[X_cols + y_cols] = scaler.transform(df_scaled[X_cols + y_cols])\n",
    "    \n",
    "    return df, scaler\n",
    "\n",
    "def subset_Xy_data(X, y, p=0.9):\n",
    "    # sort the X data by increasing PDDs\n",
    "    Iargsort = X[:, 0].argsort()\n",
    "    X_sorted, y_sorted = X[Iargsort,:], y[Iargsort]\n",
    "    # select the middle p% of the data\n",
    "    n10 = int(len(X)*(1-p)) # number of points in 20% of the data\n",
    "    X_sorted_subset = X_sorted[int(n10/2):-int(n10/2), :]\n",
    "    y_sorted_subset = y_sorted[int(n10/2):-int(n10/2)]\n",
    "\n",
    "    return X_sorted_subset, y_sorted_subset\n",
    "    \n",
    "def linear_fit(X, y):\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    score = model.score(X, y)\n",
    "    coefs = np.ravel(model.coef_)\n",
    "    return coefs, score\n",
    "    \n",
    "# Define function for K-folds cross-validation model fitting\n",
    "def kfolds_linear_fit(X, y, n_folds=5):\n",
    "    # Define K-folds\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    # Initialize parameters\n",
    "    coefs_PDD, coefs_snowfall, scores = [], [], []\n",
    "    # Iterate over fold indices\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        # Split X and y into training and testing\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Fit model to testing\n",
    "        coefs, score = linear_fit(X_train, y_train)\n",
    "        coefs_PDD.append(coefs[0])\n",
    "        coefs_snowfall.append(coefs[1])\n",
    "        scores.append(score)\n",
    "    # Calculate stats, compile in dataframe\n",
    "    df = pd.DataFrame({'coef_PDD_mean': [np.nanmean(coefs_PDD)],\n",
    "                       'coef_PDD_std': [np.nanstd(coefs_PDD)],\n",
    "                       'coef_PDD_median': [np.nanmedian(coefs_PDD)],\n",
    "                       'coef_PDD_MAD': [MAD(coefs_PDD)],\n",
    "                       'coef_snowfall_mean': [np.nanmean(coefs_snowfall)],\n",
    "                       'coef_snowfall_std': [np.nanstd(coefs_snowfall)],\n",
    "                       'coef_snowfall_median': [np.nanmedian(coefs_snowfall)],\n",
    "                       'coef_snowfall_MAD': [MAD(coefs_snowfall)],\n",
    "                       'score_mean': [np.nanmean(scores)],\n",
    "                       'score_median': [np.nanmedian(scores)]\n",
    "                      })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb0e1e2-4aa2-4a1e-a160-cf53843de739",
   "metadata": {},
   "source": [
    "### Modeled monthly ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181e0f4-1acb-47b3-8aca-f50053088c5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Scale the input data\n",
    "X_cols = ['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']\n",
    "y_cols = ['ELA_m']\n",
    "# mod_elas_monthly_scaled, scaler = scale_Xy(mod_elas_monthly, X_cols, y_cols)\n",
    "\n",
    "# Fit linear trendlines to ELAs + Snowfall = PDDs\n",
    "fits_mod_monthly_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(mod_elas_monthly['RGIId'].drop_duplicates().values):\n",
    "    # subset and merge data\n",
    "    site_df = mod_elas_monthly.loc[mod_elas_monthly_scaled['RGIId']==rgi_id]\n",
    "    # only include dates before October\n",
    "    site_df = site_df.loc[site_df['Date'].dt.month < 9]\n",
    "    # remove dates where PDDs==0\n",
    "    site_df.loc[site_df['Cumulative_Positive_Degree_Days'] > 0].reset_index(drop=True, inplace=True)\n",
    "    # prep the X and y data\n",
    "    X = site_df[X_cols].values\n",
    "    y = site_df[y_cols].values\n",
    "    # subset to 80% to mitigate the impact of snowfall\n",
    "    # X_sub, y_sub = subset_Xy_data(X, y, p=0.8)\n",
    "    # fit linear trendline\n",
    "    fit_df = kfolds_linear_fit(X, y)\n",
    "    fit_df['RGIId'] = rgi_id\n",
    "    # add RGI regions and climate cluster to df\n",
    "    for col in ['O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName']:\n",
    "        fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "    # concatenate to full dataframe\n",
    "    fits_mod_monthly_df = pd.concat([fits_mod_monthly_df, fit_df])\n",
    "\n",
    "fits_mod_monthly_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save to file\n",
    "fits_mod_monthly_fn = os.path.join(scm_path, 'results', 'linear_fit_modeled_monthly_ela_pdd_snowfall.csv')\n",
    "fits_mod_monthly_df.to_csv(fits_mod_monthly_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_mod_monthly_fn)\n",
    "fits_mod_monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d4f3e-e34c-4b43-9cab-57f4cbe25602",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "plt.hist(fits_mod_monthly_df['coef_PDD_median'], bins=100)\n",
    "ax.set_title('Modeled')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1075d-1961-447f-b10c-4fbbf219249f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print('Modeled:\\n')\n",
    "\n",
    "print('Overall PDD coef. stats:')\n",
    "print(f'\\tMedian = {np.nanmedian(fits_mod_monthly_df[\"coef_PDD_median\"])} \\tMAD = {MAD(fits_mod_monthly_df[\"coef_PDD_median\"], nan_policy=\"omit\")}')\n",
    "print(f'\\tMean = {np.nanmean(fits_mod_monthly_df[\"coef_PDD_median\"])} \\tstd. = {np.nanstd(fits_mod_monthly_df[\"coef_PDD_median\"])}\\n')\n",
    "\n",
    "print(fits_mod_monthly_df.groupby(by='Subregion')['coef_PDD_median'].median())\n",
    "print('\\n')\n",
    "print(fits_mod_monthly_df.groupby(by='clustName')['coef_PDD_median'].median())\n",
    "print('\\n')\n",
    "print(fits_mod_monthly_df.groupby(by=['Subregion', 'clustName'])['coef_PDD_median'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b1748-43cd-4329-8de0-978e502764bc",
   "metadata": {},
   "source": [
    "### Observed monthly ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0812972-6829-4948-92a1-73cf1be60205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Monthly ELAs\n",
    "X_cols = ['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']\n",
    "y_cols = ['ELA_from_AAR_m']\n",
    "\n",
    "fits_obs_monthly_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(obs_elas_monthly['RGIId'].drop_duplicates().values):\n",
    "    # subset and merge data\n",
    "    site_df = obs_elas_monthly.loc[obs_elas_monthly['RGIId']==rgi_id]\n",
    "    site_df.dropna(inplace=True)\n",
    "    # only include dates before October\n",
    "    site_df = site_df.loc[site_df['Date'].dt.month < 10]\n",
    "    # remove dates where PDD==0\n",
    "    site_df = site_df.loc[site_df['Cumulative_Positive_Degree_Days'] > 0]\n",
    "    if len(site_df) >= 2:\n",
    "        # prep the X and y data\n",
    "        X = site_df[X_cols].values\n",
    "        y = site_df[y_cols].values\n",
    "        # subset to 80% to mitigate the impact of snowfall\n",
    "        # X_sub, y_sub = subset_Xy_data(X, y, p=0.8)\n",
    "        # fit linear trendline\n",
    "        fit_df = kfolds_linear_fit(X, y)\n",
    "        fit_df['RGIId'] = rgi_id\n",
    "        # add RGI regions and climate cluster to df\n",
    "        for col in ['O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName']:\n",
    "            fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "        # concatenate to full dataframe\n",
    "        fits_obs_monthly_df = pd.concat([fits_obs_monthly_df, fit_df])\n",
    "\n",
    "fits_obs_monthly_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save to file\n",
    "fits_obs_monthly_fn = os.path.join(scm_path, 'results', 'linear_fit_observed_monthly_ela_pdd_snowfall.csv')\n",
    "fits_obs_monthly_df.to_csv(fits_obs_monthly_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_obs_monthly_fn)\n",
    "fits_obs_monthly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135c28fc-7002-4897-9f86-df9a4feb629a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fits_obs_monthly_df['coef_PDD_median'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712d424-80e6-4664-ada0-c084203e8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print('Observed:\\n')\n",
    "\n",
    "print('Overall PDD coef. stats:')\n",
    "print(f'\\tMedian = {np.nanmedian(fits_obs_monthly_df[\"coef_PDD_median\"])} \\tMAD = {MAD(fits_obs_monthly_df[\"coef_PDD_median\"], nan_policy=\"omit\")}')\n",
    "print(f'\\tMean = {np.nanmean(fits_obs_monthly_df[\"coef_PDD_median\"])} \\tstd. = {np.nanstd(fits_obs_monthly_df[\"coef_PDD_median\"])}\\n')\n",
    "\n",
    "print(fits_obs_monthly_df.groupby(by='Subregion')['coef_PDD_median'].median())\n",
    "print('\\n')\n",
    "print(fits_obs_monthly_df.groupby(by='clustName')['coef_PDD_median'].median())\n",
    "print('\\n')\n",
    "print(fits_obs_monthly_df.groupby(by=['Subregion', 'clustName'])['coef_PDD_median'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad5aa2-f2ba-47e2-a707-5455b3945d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Daily ELAs\n",
    "fits_obs_daily_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(scs['RGIId'].drop_duplicates().values):\n",
    "    obs_elas_site = scs.loc[scs['RGIId']==rgi_id]\n",
    "    obs_elas_site['datetime'] = obs_elas_site['datetime'].values.astype('datetime64[D]')\n",
    "    obs_elas_site.rename(columns={'datetime': 'Date'}, inplace=True)\n",
    "    obs_elas_site = obs_elas_site[['Date', 'ELA_from_AAR_m']]\n",
    "    eras_site = eras.loc[eras['RGIId']==rgi_id]\n",
    "    # subset and merge data\n",
    "    site_df = obs_elas_site.merge(eras_site[['Date', 'Cumulative_Positive_Degree_Days', \n",
    "                                             'Cumulative_Snowfall_mwe']], on='Date')\n",
    "    site_df.dropna(inplace=True)\n",
    "    # only include dates before October\n",
    "    site_df = site_df.loc[site_df['Date'].dt.month < 9]\n",
    "    # remove dates where PDD==0\n",
    "    site_df = site_df.loc[site_df['Cumulative_Positive_Degree_Days'] > 0]\n",
    "    if len(site_df) >= 2:\n",
    "        # fit linear trendline\n",
    "        X = site_df[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].values\n",
    "        y = site_df['ELA_from_AAR_m'].values\n",
    "        # save in dataframe\n",
    "        fit_df = kfolds_linear_fit(X, y)\n",
    "        fit_df['RGIId'] = rgi_id\n",
    "        # add RGI regions and climate cluster to df\n",
    "        for col in ['O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName']:\n",
    "            fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "        # concatenate to full dataframe\n",
    "        fits_obs_daily_df = pd.concat([fits_obs_daily_df, fit_df])\n",
    "\n",
    "fits_obs_daily_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save\n",
    "fits_obs_daily_fn = os.path.join(scm_path, 'results', 'linear_fit_observed_daily_ela_pdd_snowfall_daily.csv')\n",
    "fits_obs_daily_df.to_csv(fits_obs_daily_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_obs_daily_fn)\n",
    "fits_obs_daily_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d84ed-025b-4478-a314-68b6b23ef635",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fits_obs_daily_df['coef_PDD_median'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77250b6-b461-444f-9880-ca8c4533ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----Plot a few example linear fits for the modeled and remotely-sensed\n",
    "# Plot the trans-continental sites in the St. Elias Mtns.\n",
    "rgi_ids = aois.loc[(aois['Subregion']=='St. Elias Mtns.') \n",
    "                   & (aois['clustName']=='Transitional-Continental'), 'RGIId'].values\n",
    "for rgi_id in tqdm(rgi_ids):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12,6), sharey=True, sharex=True)\n",
    "    for i, df in enumerate([mod_elas_monthly, obs_elas_monthly]):\n",
    "        # subset and merge data\n",
    "        site_df = df.loc[df['RGIId']==rgi_id]\n",
    "        # only include dates before October\n",
    "        site_df = site_df.loc[site_df['Date'].dt.month < 9]\n",
    "        # remove dates where PDD==0\n",
    "        site_df = site_df.loc[site_df['Cumulative_Positive_Degree_Days'] > 0]\n",
    "        # prep the X and y data\n",
    "        X = site_df[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].values\n",
    "        if i==0:\n",
    "            y = site_df['ELA_m'].values\n",
    "        else:\n",
    "            y = site_df['ELA_from_AAR_m'].values\n",
    "        # subset to 80% to mitigate the impact of snowfall\n",
    "        X_sub, y_sub = subset_Xy_data(X, y, p=0.6)\n",
    "        # fit linear regression model\n",
    "        model = LinearRegression().fit(X_sub, y_sub)\n",
    "        score = model.score(X, y)\n",
    "        coefs = np.ravel(model.coef_)\n",
    "        # plot\n",
    "        ax[i].plot(X[:,0], model.predict(X), 'ok', label='Linear fit')\n",
    "        ax[i].plot(X[:,0], y, '.', label='Modeled')\n",
    "        ax[i].set_xlabel('$\\Sigma$PDD')\n",
    "        ax[i].set_ylabel('ELA [m]')\n",
    "        ax[i].legend(loc='upper left')\n",
    "        if i==0:\n",
    "            run = 'Modeled'\n",
    "        else:\n",
    "            run = 'Observed'\n",
    "        ax[i].set_title(f'{run}\\nELA coef = {np.round(coefs[0],3)}, Score = {np.round(score, 3)}')\n",
    "    fig.suptitle(rgi_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8908aeb-77c9-4e83-89ca-f6c982dec41f",
   "metadata": {},
   "source": [
    "### Modeled annual ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764cff56-6fa2-41dc-8ed5-02630c09ec1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_mod_annual_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(mod_elas_annual['RGIId'].drop_duplicates().values):\n",
    "    # Subset annual ELAs to site\n",
    "    mod_elas_annual_site = mod_elas_annual.loc[mod_elas_annual['RGIId']==rgi_id]\n",
    "    # Split data into X and y\n",
    "    X = mod_elas_annual_site[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].values\n",
    "    y = mod_elas_annual_site['ELA_m'].values\n",
    "    # Fit linear regression w/ K-folds\n",
    "    n_folds = len(X)\n",
    "    fit_df = kfolds_linear_fit(X, y, n_folds=n_folds)\n",
    "    fit_df['RGIId'] = rgi_id\n",
    "    # add RGI regions and climate cluster to df\n",
    "    for col in ['O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName']:\n",
    "        fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "    # concatenate to full dataframe\n",
    "    fits_mod_annual_df = pd.concat([fits_mod_annual_df, fit_df])\n",
    "fits_mod_annual_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save to file\n",
    "fits_mod_annual_fn = os.path.join(scm_path, 'results', 'linear_fit_modeled_annual_ela_pdd_snowfall.csv')\n",
    "fits_mod_annual_df.to_csv(fits_mod_annual_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_mod_annual_fn)\n",
    "fits_mod_annual_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d41e418-7cea-4791-8fff-3e5b6009fd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fits_mod_annual_df['coef_PDD_median'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfecacf-630f-4f4d-867e-6a64239c5d1b",
   "metadata": {},
   "source": [
    "### Observed annual ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf48da-e155-4329-9321-006c663d846c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fits_obs_annual_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(obs_elas_annual['RGIId'].drop_duplicates().values):\n",
    "    # Subset annual ELAs to site\n",
    "    obs_elas_annual_site = obs_elas_annual.loc[obs_elas_annual['RGIId']==rgi_id]\n",
    "    # Split data into X and y\n",
    "    X = obs_elas_annual_site[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].values\n",
    "    y = obs_elas_annual_site['ELA_from_AAR_m'].values\n",
    "    # Fit linear regression w/ K-folds\n",
    "    n_folds = len(X)\n",
    "    if n_folds > 1:\n",
    "        fit_df = kfolds_linear_fit(X, y, n_folds=n_folds)\n",
    "        fit_df['RGIId'] = rgi_id\n",
    "        # add RGI regions and climate cluster to df\n",
    "        for col in ['O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName']:\n",
    "            fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "        # concatenate to full dataframe\n",
    "        fits_obs_annual_df = pd.concat([fits_obs_annual_df, fit_df])\n",
    "fits_obs_annual_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save to file\n",
    "fits_obs_annual_fn = os.path.join(scm_path, 'results', 'linear_fit_observed_annual_ela_pdd_snowfall.csv')\n",
    "fits_obs_annual_df.to_csv(fits_obs_annual_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_obs_annual_fn)\n",
    "fits_obs_annual_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb27d1d6-e4eb-40e1-a596-750a794bc365",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X[:,0], y, '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a4e6eb-94d1-4895-b40c-6ca052967a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(fits_obs_annual_df['coef_PDD_median'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e5aef7-304a-492e-8813-c6d316475b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
