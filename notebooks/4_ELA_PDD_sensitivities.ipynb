{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57798819-82ed-477d-81ad-a1429bad6670",
   "metadata": {},
   "source": [
    "# Assess ELAs from observations and modeled conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d102a4-ef90-4891-aae6-eefcfb4f0f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import xarray as xr\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from scipy.interpolate import CubicSpline\n",
    "import sys\n",
    "import seaborn as sns\n",
    "# Suppress future warning from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import matplotlib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import ruptures as rpt\n",
    "from scipy import optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cf1041-b03b-41e1-a43e-850ccfaef58f",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "# scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "707e7158-398e-4d53-aeb7-39190bdf2347",
   "metadata": {},
   "source": [
    "## Load glacier boundaries, ERA5-Land data, and compiled snow cover stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e24c99-f8a5-4e13-824f-6621f227b6a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Load glacier boundaries with climate clusters\n",
    "aois_fn = os.path.join(scm_path, 'compiled_data', 'all_aois_climate_cluster.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All AOIs with climate clusters loaded from file.')\n",
    "\n",
    "# -----Load ERA data\n",
    "eras_fn = os.path.join(scm_path, 'compiled_data', 'all_era_data.csv')\n",
    "eras = pd.read_csv(eras_fn)\n",
    "# format dates as datetimes\n",
    "eras['Date'] = pd.to_datetime(eras['Date'])\n",
    "# rename \"site_name\" column to \"RGIId\"\n",
    "eras.rename(columns={'site_name': 'RGIId'}, inplace=True)\n",
    "print('All ERA data loaded from file.')\n",
    "\n",
    "# -----Load compiled snow cover statts\n",
    "scs_fn = os.path.join(scm_path, 'compiled_data', 'all_snow_cover_stats.csv')\n",
    "scs = pd.read_csv(scs_fn)\n",
    "scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "print('All snow cover stats loaded from file.')\n",
    "scs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f39a44a-f3cf-495d-a9ec-8071606c734e",
   "metadata": {},
   "source": [
    "## Estimate and save ELAs\n",
    "\n",
    "### Modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0244b0cf-72ec-41d6-af5d-e74f63ca7ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Piece-wise linear fit function for extrapolating the ELA when SMB < 0 everywhere\n",
    "def linear_fit(x, m, b):\n",
    "    return m*x + b\n",
    "    \n",
    "def extrapolate_ela_linear(X,y, Iend=8):\n",
    "    # optimize the linear fit\n",
    "    p, e = optimize.curve_fit(linear_fit, X[0:Iend+1], y[0:Iend+1])\n",
    "    # extrapolate where y=0\n",
    "    ela = linear_fit(0, *p)\n",
    "    return ela\n",
    "\n",
    "def extrapolate_ela_piecewise_linear(X,y):\n",
    "    # identify breakpoints\n",
    "    algo = rpt.Pelt(model=\"rbf\").fit(signal)\n",
    "    result = algo.predict(pen=10)\n",
    "                                 \n",
    "def extrapolate_ela_cubic_spline(X,y):\n",
    "    # check that X is increasing\n",
    "    if X[1] < X[0]:\n",
    "        spline = CubicSpline(np.flip(X), np.flip(y), bc_type='natural')\n",
    "    else:\n",
    "        spline = CubicSpline(X, y, bc_type='natural')\n",
    "    ela = spline(0)\n",
    "    \n",
    "    return ela"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422c66ac-8b4c-4c92-8a31-38a4bb1d5071",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mod_elas_fn = os.path.join(scm_path, 'Rounce_et_al_2023', 'modeled_elas.csv')\n",
    "if os.path.exists(mod_elas_fn):\n",
    "    mod_elas = pd.read_csv(mod_elas_fn, index_col=0)\n",
    "    mod_elas.index = pd.DatetimeIndex(mod_elas.index)\n",
    "    print('Modeled ELAs loaded from file.')\n",
    "else:\n",
    "    \n",
    "    # load binned model data\n",
    "    bin_fns = sorted(glob.glob(os.path.join(scm_path, 'Rounce_et_al_2023', 'binned', '*.nc')))\n",
    "    \n",
    "    # remove binned file names for sites without snow cover observations\n",
    "    aoi_ids = [x[7:] for x in aois['RGIId'].drop_duplicates().values]\n",
    "    bin_fns = [x for x in bin_fns if os.path.basename(x)[0:7] in aoi_ids]\n",
    "\n",
    "    # iterate over binned file names\n",
    "    i=0\n",
    "    for bin_fn in tqdm(bin_fns):\n",
    "        # open binned data\n",
    "        bin = xr.open_dataset(bin_fn)\n",
    "        rgi_id = bin.RGIId.data[0] # grab RGI ID\n",
    "\n",
    "        # grab data variables\n",
    "        h = bin.bin_surface_h_initial.data[0] # surface elevation [m]\n",
    "        b_sum = np.zeros((len(bin.time.data), len(h))) # cumulative SMB\n",
    "        times = [np.datetime64(x) for x in bin.time.data] # datetimes\n",
    "        months = list(pd.DatetimeIndex(times).month) # months of each datetime\n",
    "        elas = np.zeros(len(times)) # initialize transient ELAs\n",
    "\n",
    "        # iterate over each time period\n",
    "        for j, time in enumerate(times):\n",
    "            # subset binned data to time\n",
    "            bin_time = bin.isel(time=j)\n",
    "            # grab the SMB \n",
    "            b_sum[j,:] = bin_time.bin_massbalclim_monthly.data[0]\n",
    "            # add the previous SMB (restart the count in October)\n",
    "            if months[j] != 10: \n",
    "                b_sum[j,:] += b_sum[j-1,:]\n",
    "            # If all SMB > 0, ELA = minimum elevation\n",
    "            if all(b_sum[j,:] > 0):\n",
    "                elas[j] = np.min(h)\n",
    "            # If SMB is > 0 and < 0 in some places, linearly interpolate ELA\n",
    "            elif any(b_sum[j,:] < 0) & any(b_sum[j,:] > 0):\n",
    "                elas[j] = np.interp(0, np.flip(b_sum[j,:]), np.flip(h))\n",
    "            # If SMB < 0 everywhere, fit a piecewise linear fit and extrapolate for SMB=0\n",
    "            elif all(b_sum[j,:] < 0):\n",
    "                X, y = b_sum[j,:], h\n",
    "                elas[j] = extrapolate_ela_linear(X, y, Iend=5)\n",
    "        \n",
    "        # compile in dataframe\n",
    "        if i==0:\n",
    "            mod_elas = pd.DataFrame({'Date': times, rgi_id: elas})\n",
    "        else:\n",
    "            df = pd.DataFrame({'Date': times, rgi_id: elas})\n",
    "            mod_elas = mod_elas.merge(df, on='Date')\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    # save to file\n",
    "    mod_elas.set_index('Date', inplace=True)\n",
    "    mod_elas.to_csv(mod_elas_fn, index=True)\n",
    "    print('Modeled ELAs saved to file:', mod_elas_fn)\n",
    "\n",
    "# Subset to after 2013\n",
    "mod_elas = mod_elas.loc[pd.DatetimeIndex(mod_elas.index).year >= 2013]\n",
    "# Subset to April-Sept\n",
    "mod_elas = mod_elas.loc[(pd.DatetimeIndex(mod_elas.index).month >= 3) \n",
    "                        & (pd.DatetimeIndex(mod_elas.index).month <=9)]\n",
    "\n",
    "mod_elas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c806a0-69a1-4435-8ad9-514bdb1994a1",
   "metadata": {},
   "source": [
    "### Remotely-sensed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fd2503-d6ed-41e0-b2ca-7b4e058ef307",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obs_elas_fn = os.path.join(scm_path, 'compiled_data', 'observed_elas.csv')\n",
    "\n",
    "# Check if ELAs already exist in file\n",
    "if os.path.exists(obs_elas_fn):\n",
    "    obs_elas = pd.read_csv(obs_elas_fn, index_col=0)\n",
    "    obs_elas.set_index(pd.DatetimeIndex(obs_elas.index), inplace=True)\n",
    "    print('Remotely-sensed ELAs loaded from file.')\n",
    "else:\n",
    "    # Add Year and Month columns to snow cover stats\n",
    "    scs['Year'] = scs['datetime'].dt.year.values\n",
    "    scs['Month'] = scs['datetime'].dt.month.values\n",
    "    \n",
    "    # Calculate the mean. monthly ELA at each site\n",
    "    obs_elas = scs.groupby(by=['RGIId', 'Year', 'Month'])['ELA_from_AAR_m'].mean().reset_index()\n",
    "\n",
    "    # Add Date column\n",
    "    obs_elas['Date'] = [np.datetime64(f'{year}-{month}-01') if month >=10 else \n",
    "                        np.datetime64(f'{year}-0{month}-01')\n",
    "                        for year, month in obs_elas[['Year', 'Month']].values]\n",
    "    \n",
    "    # Restructure so that Date is the index and each column it the RGIId\n",
    "    obs_elas = obs_elas.pivot_table(index='Date', columns='RGIId', values='ELA_from_AAR_m')\n",
    "    \n",
    "    # Save to file\n",
    "    obs_elas.to_csv(obs_elas_fn, index=True)\n",
    "    print('Remotely-sensed ELAs saved to file:', obs_elas_fn)\n",
    "\n",
    "obs_elas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6e415b-4392-433e-86d9-1f5acb3e41bd",
   "metadata": {},
   "source": [
    "## Compare modeled to observed ELAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087bc4f5-4003-4a3b-b354-980f12070f4b",
   "metadata": {},
   "source": [
    "### Transient ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fa02b1-304e-4876-911b-54c80236728f",
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_transient_diff_df = pd.DataFrame()\n",
    "\n",
    "# Add Year columns\n",
    "obs_elas['Year'] = obs_elas.index.year\n",
    "mod_elas['Year'] = mod_elas.index.year\n",
    "\n",
    "i=0\n",
    "for rgi_id in tqdm(list(mod_elas.columns[0:194])):\n",
    "    # Subset data to site\n",
    "    obs_elas_site = obs_elas[[rgi_id, 'Year']].reset_index()\n",
    "    obs_elas_site.loc[obs_elas_site[rgi_id] > 1e10, rgi_id] = np.nan\n",
    "    mod_elas_site = mod_elas[[rgi_id, 'Year']].reset_index()\n",
    "\n",
    "    # Subset to 2013-2022\n",
    "    obs_elas_site = obs_elas_site.loc[(obs_elas_site['Year'] <= 2022)]\n",
    "    mod_elas_site = mod_elas_site.loc[(mod_elas_site['Year'] <= 2022)]\n",
    "    \n",
    "    # Subtract minimum ELA to directly compare\n",
    "    obs_elas_site_relative, mod_elas_site_relative = obs_elas_site.copy(), mod_elas_site.copy()\n",
    "    obs_elas_site_relative[rgi_id] -= np.nanmin(scs.loc[scs['RGIId']==rgi_id, 'ELA_from_AAR_m'])\n",
    "    mod_elas_site_relative[rgi_id] -= np.nanmin(mod_elas_site[rgi_id])\n",
    "\n",
    "    # calculate differences\n",
    "    merged = obs_elas_site_relative.merge(mod_elas_site_relative, on=['Date', 'Year'], suffixes=['_obs', '_mod'])\n",
    "    merged['ELA_obs-ELA_mod [m]'] = merged[rgi_id + '_obs'] - merged[rgi_id + '_mod']\n",
    "\n",
    "    # Add to full dataframe\n",
    "    diff_df = merged['ELA_obs-ELA_mod [m]'].reset_index()\n",
    "    diff_df['RGIId'] = rgi_id\n",
    "    elas_transient_diff_df = pd.concat([elas_transient_diff_df, diff_df])\n",
    "\n",
    "    i+=1\n",
    "\n",
    "# Drop Year columns\n",
    "obs_elas.drop(columns='Year', inplace=True)\n",
    "mod_elas.drop(columns='Year', inplace=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(elas_transient_diff_df.groupby('RGIId')['ELA_obs-ELA_mod [m]'].mean().values, bins=50)\n",
    "plt.xlabel('ELA$_{obs}$ - ELA$_{mod}$ [m]')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Transient ELAs')\n",
    "plt.show()\n",
    "\n",
    "print('Difference stats:')\n",
    "print(f'Mean diff = {elas_transient_diff_df[\"ELA_obs-ELA_mod [m]\"].mean()} m')\n",
    "print(f'Std. diff = {elas_transient_diff_df[\"ELA_obs-ELA_mod [m]\"].std()} m')\n",
    "print(f'Median diff = {elas_transient_diff_df[\"ELA_obs-ELA_mod [m]\"].median()} m')\n",
    "print(f'MAD diff = {MAD(elas_transient_diff_df[\"ELA_obs-ELA_mod [m]\"].values, nan_policy=\"omit\")} m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa0f5d4-7e6f-4e3d-ab43-9adb8bdbea9c",
   "metadata": {},
   "source": [
    "### Annual ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1860cb-7534-4f5d-af4e-ae1ce755a2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_diff_df = pd.DataFrame()\n",
    "\n",
    "# Add Year columns\n",
    "obs_elas['Year'] = obs_elas.index.year\n",
    "mod_elas['Year'] = mod_elas.index.year\n",
    "\n",
    "# Calculate annual max ELA\n",
    "obs_elas_max = obs_elas.groupby('Year').max()\n",
    "mod_elas_max = mod_elas.groupby('Year').max()\n",
    "\n",
    "i=0\n",
    "for rgi_id in tqdm(list(mod_elas.columns[0:194])):\n",
    "    # Subset data to site\n",
    "    obs_elas_site = obs_elas_max[rgi_id].reset_index()\n",
    "    obs_elas_site.loc[obs_elas_site[rgi_id] > 1e30, rgi_id] = np.nan\n",
    "    mod_elas_site = mod_elas_max[rgi_id].reset_index()\n",
    "\n",
    "    # Subset to 2016-2022\n",
    "    obs_elas_site = obs_elas_site.loc[(obs_elas_site['Year'] >= 2017) & (obs_elas_site['Year'] <= 2022)]\n",
    "    mod_elas_site = mod_elas_site.loc[(mod_elas_site['Year'] >= 2017) & (mod_elas_site['Year'] <= 2022)]\n",
    "\n",
    "    # Subtract minimum ELA to directly compare\n",
    "    obs_elas_site_relative, mod_elas_site_relative = obs_elas_site.copy(), mod_elas_site.copy()\n",
    "    obs_elas_site_relative[rgi_id] -= np.nanmin(scs.loc[scs['RGIId']==rgi_id, 'ELA_from_AAR_m'])\n",
    "    mod_elas_site_relative[rgi_id] -= np.nanmin(mod_elas[rgi_id])\n",
    "\n",
    "    # calculate differences\n",
    "    merged = obs_elas_site_relative.merge(mod_elas_site_relative, on='Year', suffixes=['_obs', '_mod'])\n",
    "    merged['ELA_obs-ELA_mod [m]'] = merged[rgi_id + '_obs'] - merged[rgi_id + '_mod']\n",
    "\n",
    "    # Add to full dataframe\n",
    "    diff_df = merged['ELA_obs-ELA_mod [m]'].reset_index()\n",
    "    diff_df['RGIId'] = rgi_id\n",
    "    elas_diff_df = pd.concat([elas_diff_df, diff_df])\n",
    "\n",
    "    i+=1\n",
    "\n",
    "# Drop Year columns\n",
    "obs_elas.drop(columns='Year', inplace=True)\n",
    "mod_elas.drop(columns='Year', inplace=True)\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(elas_diff_df.groupby('RGIId')['ELA_obs-ELA_mod [m]'].mean().values, bins=50)\n",
    "plt.xlabel('ELA$_{obs}$ - ELA$_{mod}$ [m]')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('ELAs')\n",
    "plt.show()\n",
    "\n",
    "print('Difference stats:')\n",
    "print(f'Mean diff = {elas_diff_df[\"ELA_obs-ELA_mod [m]\"].mean()} m')\n",
    "print(f'Std. diff = {elas_diff_df[\"ELA_obs-ELA_mod [m]\"].std()} m')\n",
    "print(f'Median diff = {elas_diff_df[\"ELA_obs-ELA_mod [m]\"].median()} m')\n",
    "print(f'MAD diff = {MAD(elas_diff_df[\"ELA_obs-ELA_mod [m]\"].values, nan_policy=\"omit\")} m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e141e2f-0ec4-4474-b754-f15f1cee35aa",
   "metadata": {},
   "source": [
    "## Fit linear trendlines PDD sum + Snowfall sum. = ELA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407202bc-2700-4d6b-829d-d93c86ab2cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "int(len(X) * 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16296188-84a6-4def-924b-cb5d2310234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(X_sorted_subset[:,0], X_sorted_subset[:,1], 'ok')\n",
    "plt.plot(X[:,0], X[:,1], '.')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64fd58ea-3ada-42b7-8e9d-a0870114ad34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_Xy_data(X, y, p=0.9):\n",
    "    # sort the X data by increasing PDDs\n",
    "    Iargsort = X[:, 0].argsort()\n",
    "    X_sorted, y_sorted = X[Iargsort,:], y[Iargsort]\n",
    "    # select the middle p% of the data\n",
    "    n10 = int(len(X)*(1-p)) # number of points in 20% of the data\n",
    "    X_sorted_subset = X_sorted[int(n10/2):-int(n10/2), :]\n",
    "    y_sorted_subset = y_sorted[int(n10/2):-int(n10/2)]\n",
    "\n",
    "    return X_sorted_subset, y_sorted_subset\n",
    "    \n",
    "def linear_fit(X, y):\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    score = model.score(X, y)\n",
    "    coefs = model.coef_\n",
    "    return coefs, score\n",
    "    \n",
    "# Define function for K-folds cross-validation model fitting\n",
    "def kfolds_linear_fit(X, y, n_folds=5):\n",
    "    # Define K-folds\n",
    "    kf = KFold(n_splits=n_folds)\n",
    "    # Initialize parameters\n",
    "    coefs_PDD, coefs_snowfall, scores = [], [], []\n",
    "    # Iterate over fold indices\n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        # Split X and y into training and testing\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        # Fit model to testing\n",
    "        coefs, score = linear_fit(X_train, y_train)\n",
    "        coefs_PDD.append(coefs[0])\n",
    "        coefs_snowfall.append(coefs[1])\n",
    "        scores.append(score)\n",
    "    # Calculate stats, compile in dataframe\n",
    "    df = pd.DataFrame({'coef_PDD_mean': [np.nanmean(coefs_PDD)],\n",
    "                       'coef_PDD_std': [np.nanstd(coefs_PDD)],\n",
    "                       'coef_PDD_median': [np.nanmedian(coefs_PDD)],\n",
    "                       'coef_PDD_MAD': [MAD(coefs_PDD)],\n",
    "                       'coef_snowfall_mean': [np.nanmean(coefs_snowfall)],\n",
    "                       'coef_snowfall_std': [np.nanstd(coefs_snowfall)],\n",
    "                       'coef_snowfall_median': [np.nanmedian(coefs_snowfall)],\n",
    "                       'coef_snowfall_MAD': [MAD(coefs_snowfall)],\n",
    "                       'score_mean': [np.nanmean(scores)],\n",
    "                       'score_median': [np.nanmedian(scores)]\n",
    "                      })\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac072e1-bfdd-4288-b5be-6aefa8870c06",
   "metadata": {},
   "source": [
    "### Restructure ERA5-Land data and resample to monthly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0b41d5-5220-4e00-9b16-bc6c82302cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add Year and month columns to ERA and modeled ELA data\n",
    "eras['Year'] = eras['Date'].dt.year\n",
    "eras['Month'] = eras['Date'].dt.month\n",
    "\n",
    "# Resample ERA data to monthly resolution to match modeled data\n",
    "eras_monthly = eras.groupby(by=['RGIId', 'Year', 'Month'])[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].mean().reset_index()\n",
    "eras_monthly\n",
    "\n",
    "# Remove dates outside 2013-2022\n",
    "eras = eras.loc[(eras['Year'] >= 2013) & (eras['Year'] <= 2022)]\n",
    "\n",
    "# Add Date column\n",
    "eras_monthly['Date'] = [np.datetime64(f'{year}-{month}-01') if month >= 10 else\n",
    "                        np.datetime64(f'{year}-0{month}-01') \n",
    "                        for year, month in eras_monthly[['Year', 'Month']].values]\n",
    "\n",
    "# Rearrange dataframe to match ELAs\n",
    "pdds_monthly_df = eras_monthly.pivot_table(index='Date', columns='RGIId', values='Cumulative_Positive_Degree_Days')\n",
    "sf_monthly_df = eras_monthly.pivot_table(index='Date', columns='RGIId', values='Cumulative_Snowfall_mwe')\n",
    "pdds_monthly_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb0e1e2-4aa2-4a1e-a160-cf53843de739",
   "metadata": {},
   "source": [
    "### Modeled ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9181e0f4-1acb-47b3-8aca-f50053088c5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit linear trendlines to ELAs + Snowfall = PDDs\n",
    "fits_mod_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(mod_elas.columns):\n",
    "    # subset and merge data\n",
    "    site_df = (pdds_monthly_df[rgi_id].reset_index().merge(sf_monthly_df[rgi_id].reset_index(), \n",
    "                                                    on='Date', suffixes=['_PDD', '_SF']).merge(mod_elas[rgi_id].reset_index(), \n",
    "                                                                                               on='Date'))\n",
    "    site_df.dropna(inplace=True)\n",
    "    # only include dates before October\n",
    "    site_df = site_df.loc[site_df['Date'].dt.month < 9]\n",
    "    # remove dates where PDDs==0\n",
    "    site_df.loc[site_df[rgi_id+'_PDD'] > 0].reset_index(drop=True, inplace=True)\n",
    "    # prep the X and y data\n",
    "    X = site_df[[rgi_id+'_PDD', rgi_id+'_SF']].values\n",
    "    y = site_df[rgi_id].values\n",
    "    # subset to 80% to mitigate the impact of snowfall\n",
    "    X_sub, y_sub = subset_Xy_data(X, y, p=0.8)\n",
    "    # fit linear trendline\n",
    "    fit_df = kfolds_linear_fit(X_sub, y_sub)\n",
    "    fit_df['RGIId'] = rgi_id\n",
    "    # add RGI regions and climate cluster to df\n",
    "    for col in ['O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName']:\n",
    "        fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "    # concatenate to full dataframe\n",
    "    fits_mod_df = pd.concat([fits_mod_df, fit_df])\n",
    "    # plot\n",
    "    # fig, ax = plt.subplots()\n",
    "    # ax.plot(X[:,0], y, '.')\n",
    "    # ax.set_xlabel('$\\Sigma$PDDs')\n",
    "    # ax.set_ylabel('Transient ELA [m]')\n",
    "    # plt.show()\n",
    "fits_mod_df.reset_index(drop=True, inplace=True)\n",
    "fits_mod_df\n",
    "\n",
    "# Save\n",
    "fits_mod_fn = os.path.join(scm_path, 'results', 'linear_fit_ela_modeled_pdd_snowfall.csv')\n",
    "fits_mod_df.to_csv(fits_mod_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_mod_fn)\n",
    "fits_mod_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d4f3e-e34c-4b43-9cab-57f4cbe25602",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10,12))\n",
    "fits_mod_df.loc[fits_mod_df['coef_PDD_median'] > 1e10, 'coef_PDD_median'] = np.nan\n",
    "sns.histplot(data=fits_mod_df, x='coef_PDD_median', multiple='stack', hue='clustName', ax=ax[0])\n",
    "sns.histplot(data=fits_mod_df, x='coef_PDD_median', multiple='stack', hue='Subregion', ax=ax[1])\n",
    "ax[0].set_title('Modeled')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede1075d-1961-447f-b10c-4fbbf219249f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print('Modeled:\\n')\n",
    "\n",
    "print('Overall PDD coef. stats:')\n",
    "print(f'\\tMedian = {np.nanmedian(fits_mod_df[\"coef_PDD_median\"])} \\tMAD = {MAD(fits_mod_df[\"coef_PDD_median\"], nan_policy=\"omit\")}')\n",
    "print(f'\\tMean = {np.nanmean(fits_mod_df[\"coef_PDD_median\"])} \\tstd. = {np.nanstd(fits_mod_df[\"coef_PDD_median\"])}\\n')\n",
    "\n",
    "print(fits_mod_df.groupby(by='Subregion')['coef_PDD_median'].median())\n",
    "print('\\n')\n",
    "print(fits_mod_df.groupby(by='clustName')['coef_PDD_median'].median())\n",
    "print('\\n')\n",
    "print(fits_mod_df.groupby(by=['Subregion', 'clustName'])['coef_PDD_median'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b1748-43cd-4329-8de0-978e502764bc",
   "metadata": {},
   "source": [
    "### Observed ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0812972-6829-4948-92a1-73cf1be60205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Monthly ELAs\n",
    "fits_obs_df = pd.DataFrame()\n",
    "for rgi_id in tqdm(obs_elas.columns):\n",
    "    # subset and merge data\n",
    "    site_df = (pdds_monthly_df[rgi_id].reset_index().merge(sf_monthly_df[rgi_id].reset_index(), \n",
    "                                                   on='Date', suffixes=['_PDD', '_SF']).merge(obs_elas[rgi_id].reset_index(), \n",
    "                                                                                              on='Date'))\n",
    "    site_df.dropna(inplace=True)\n",
    "    # only include dates before October\n",
    "    site_df = site_df.loc[site_df['Date'].dt.month < 10]\n",
    "    # remove dates where PDD==0\n",
    "    site_df = site_df.loc[site_df[rgi_id+'_PDD'] > 0]\n",
    "    if len(site_df) >= 2:\n",
    "        # prep the X and y data\n",
    "        X = site_df[[rgi_id+'_PDD', rgi_id+'_SF']].values\n",
    "        y = site_df[rgi_id].values\n",
    "        # subset to 80% to mitigate the impact of snowfall\n",
    "        X_sub, y_sub = subset_Xy_data(X, y, p=0.8)\n",
    "        # fit linear trendline\n",
    "        fit_df = kfolds_linear_fit(X, y)\n",
    "        fit_df['RGIId'] = rgi_id\n",
    "        # add RGI regions and climate cluster to df\n",
    "        for col in ['O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName']:\n",
    "            fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "        # concatenate to full dataframe\n",
    "        fits_obs_df = pd.concat([fits_obs_df, fit_df])\n",
    "\n",
    "fits_obs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Save\n",
    "fits_obs_fn = os.path.join(scm_path, 'results', 'linear_fit_ela_observed_pdd_snowfall_monthly.csv')\n",
    "fits_obs_df.to_csv(fits_obs_fn, index=False)\n",
    "print('Linear fits saved to file:', fits_obs_fn)\n",
    "fits_obs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad5aa2-f2ba-47e2-a707-5455b3945d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -----Daily ELAs\n",
    "# fits_obs_df = pd.DataFrame()\n",
    "# for rgi_id in tqdm(scs['RGIId'].drop_duplicates().values):\n",
    "#     elas_site = scs.loc[scs['RGIId']==rgi_id]\n",
    "#     elas_site['datetime'] = elas_site['datetime'].values.astype('datetime64[D]')\n",
    "#     elas_site.rename(columns={'datetime': 'Date'}, inplace=True)\n",
    "#     elas_site = elas_site[['Date', 'ELA_from_AAR_m']]\n",
    "#     eras_site = eras.loc[eras['RGIId']==rgi_id]\n",
    "#     # subset and merge data\n",
    "#     site_df = (eras_site[['Date', 'Cumulative_Positive_Degree_Days', \n",
    "#                           'Cumulative_Snowfall_mwe']]).merge(elas_site, on='Date')\n",
    "#     site_df.dropna(inplace=True)\n",
    "#     # only include dates before October\n",
    "#     site_df = site_df.loc[site_df['Date'].dt.month < 10]\n",
    "#     # remove dates where PDD==0\n",
    "#     site_df = site_df.loc[site_df['Cumulative_Positive_Degree_Days'] > 0]\n",
    "#     if len(site_df) >= 2:\n",
    "#         # fit linear trendline\n",
    "#         X = site_df[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].values\n",
    "#         y = site_df['ELA_from_AAR_m'].values\n",
    "#         # save in dataframe\n",
    "#         fit_df = kfolds_linear_fit(X, y)\n",
    "#         fit_df['RGIId'] = rgi_id\n",
    "#         # add RGI regions and climate cluster to df\n",
    "#         for col in ['O1Region', 'O2Region', 'Subregion', 'cluster', 'clustName']:\n",
    "#             fit_df[col] = [aois.loc[aois['RGIId']==rgi_id, col].values[0]]\n",
    "#         # concatenate to full dataframe\n",
    "#         fits_obs_df = pd.concat([fits_obs_df, fit_df])\n",
    "\n",
    "# fits_obs_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# # Save\n",
    "# fits_obs_fn = os.path.join(scm_path, 'results', 'linear_fit_ela_observed_pdd_snowfall_daily.csv')\n",
    "# fits_obs_df.to_csv(fits_obs_fn, index=False)\n",
    "# print('Linear fits saved to file:', fits_obs_fn)\n",
    "# fits_obs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250d84ed-025b-4478-a314-68b6b23ef635",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10,12))\n",
    "fits_obs_df.loc[np.abs(fits_obs_df['coef_PDD_median']) > 10, 'coef_PDD_median'] = np.nan\n",
    "sns.histplot(data=fits_obs_df, x='coef_PDD_median', multiple='stack', hue='clustName', ax=ax[0])\n",
    "sns.histplot(data=fits_obs_df, x='coef_PDD_median', multiple='stack', hue='Subregion', ax=ax[1])\n",
    "ax[0].set_title('Observed')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b712d424-80e6-4664-ada0-c084203e8672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "print('Observed:\\n')\n",
    "\n",
    "print('Overall PDD coef. stats:')\n",
    "print(f'\\tMedian = {np.nanmedian(fits_obs_df[\"coef_PDD_median\"])} \\tMAD = {MAD(fits_obs_df[\"coef_PDD_median\"], nan_policy=\"omit\")}')\n",
    "print(f'\\tMean = {np.nanmean(fits_obs_df[\"coef_PDD_median\"])} \\tstd. = {np.nanstd(fits_obs_df[\"coef_PDD_median\"])}\\n')\n",
    "\n",
    "print(fits_obs_df.groupby(by='Subregion')['coef_PDD_median'].median())\n",
    "print('\\n')\n",
    "print(fits_obs_df.groupby(by='clustName')['coef_PDD_median'].median())\n",
    "print('\\n')\n",
    "print(fits_obs_df.groupby(by=['Subregion', 'clustName'])['coef_PDD_median'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77250b6-b461-444f-9880-ca8c4533ed8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----Plot a few example linear fits for the modeled and remotely-sensed\n",
    "# Plot the trans-continental sites in the St. Elias Mtns.\n",
    "rgi_ids = aois.loc[(aois['Subregion']=='St. Elias Mtns.') \n",
    "                   & (aois['clustName']=='Transitional-Continental'), 'RGIId'].values\n",
    "for rgi_id in tqdm(rgi_ids):\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(12,6), sharey=True, sharex=True)\n",
    "    for i, df in enumerate([mod_elas, obs_elas]):\n",
    "        # subset and merge data\n",
    "        site_df = (pdds_monthly_df[rgi_id].reset_index().merge(sf_monthly_df[rgi_id].reset_index(), \n",
    "                                                    on='Date', suffixes=['_PDD', '_SF']).merge(df[rgi_id].reset_index(), \n",
    "                                                                                               on='Date'))\n",
    "        site_df.dropna(inplace=True)\n",
    "        # only include dates before October\n",
    "        site_df = site_df.loc[site_df['Date'].dt.month < 9]\n",
    "        # remove dates where PDD==0\n",
    "        site_df = site_df.loc[site_df[rgi_id+'_PDD'] > 0]\n",
    "        # prep the X and y data\n",
    "        X = site_df[[rgi_id+'_PDD', rgi_id+'_SF']].values\n",
    "        y = site_df[rgi_id].values\n",
    "        # subset to 80% to mitigate the impact of snowfall\n",
    "        X_sub, y_sub = subset_Xy_data(X, y, p=0.8)\n",
    "        # fit linear regression model\n",
    "        model = LinearRegression().fit(X, y)\n",
    "        score = model.score(X, y)\n",
    "        coefs = model.coef_\n",
    "        # plot\n",
    "        ax[i].plot(X[:,0], model.predict(X), 'ok', label='Linear fit')\n",
    "        ax[i].plot(X[:,0], y, '.', label='Modeled')\n",
    "        ax[i].set_xlabel('$\\Sigma$PDD')\n",
    "        ax[i].set_ylabel('ELA [m]')\n",
    "        ax[i].legend(loc='upper left')\n",
    "        if i==0:\n",
    "            run = 'Modeled'\n",
    "        else:\n",
    "            run = 'Observed'\n",
    "        ax[i].set_title(f'{run}\\nELA coef = {np.round(coefs[0],3)}, Score = {np.round(score, 3)}')\n",
    "    fig.suptitle(rgi_id)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ea1d20-688f-47b4-b7e2-7f06367053c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
