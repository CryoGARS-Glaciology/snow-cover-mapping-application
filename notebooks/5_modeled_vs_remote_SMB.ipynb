{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa19aea-f65a-4694-aa1b-1c1c24efbf58",
   "metadata": {},
   "source": [
    "# Estimate differences in modeled and remotely-sensed SMB\n",
    "\n",
    "1. Monthly snowline altitudes (SLAs)\n",
    "2. Equilibrium line altitudes (ELAs)\n",
    "3. Modeled surface mass balance (SMB) at the remotely-sensed snowline\n",
    "4. Melt factors of snow ($f_{snow}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10b4fc-0d6f-45af-b2ac-9c92269f5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from scipy import optimize\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde12fe-197d-41a7-8ec0-5b0d34956a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for inputs and outputs\n",
    "scm_dir = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "model_dir = os.path.join(scm_dir, 'Rounce_et_al_2023')\n",
    "# Load glacier boundaries for RGI IDs\n",
    "aois_fn = os.path.join(scm_dir, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc2430",
   "metadata": {},
   "source": [
    "## 1. Monthly snowline altitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2c326-e946-4ec3-b453-2149a351f311",
   "metadata": {},
   "source": [
    "### Modeled SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b26c4-b13f-4f05-b233-bd6e773bfdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for linearly extrapolating the ELA when modeled SMB < 0 everywhere\n",
    "def linear_fit(x, m, b):\n",
    "    return m*x + b\n",
    "    \n",
    "def extrapolate_ela_linear(X,y, Iend=8):\n",
    "    # optimize the linear fit\n",
    "    p, e = optimize.curve_fit(linear_fit, X[0:Iend+1], y[0:Iend+1])\n",
    "    # extrapolate where y=0\n",
    "    ela = linear_fit(0, *p)\n",
    "    return ela\n",
    "\n",
    "# Check if file already exists\n",
    "slas_mod_fn = os.path.join(scm_dir, 'analysis', 'monthly_SLAs_modeled.csv')\n",
    "if not os.path.exists(slas_mod_fn):\n",
    "    # load binned model data\n",
    "    bin_fns = sorted(glob.glob(os.path.join(model_dir, 'glac_SMB_binned', '*.nc')))\n",
    "    \n",
    "    # remove binned file names for sites without snow cover observations\n",
    "    aoi_ids = [x[7:] for x in sorted(aois['RGIId'].drop_duplicates().values)]\n",
    "    bin_fns = [x for x in bin_fns if os.path.basename(x)[0:7] in aoi_ids]\n",
    "\n",
    "    # initialize dataframe for results\n",
    "    slas_mod = pd.DataFrame()\n",
    "\n",
    "    # iterate over binned file names\n",
    "    i=0\n",
    "    for bin_fn in tqdm(bin_fns):\n",
    "        # open binned data\n",
    "        bin = xr.open_dataset(bin_fn)\n",
    "        rgi_id = bin.RGIId.data[0] # grab RGI ID\n",
    "\n",
    "        # grab data variables\n",
    "        h = bin.bin_surface_h_initial.data[0] # surface elevation [m]\n",
    "        b_sum = np.zeros((len(bin.time.data), len(h))) # cumulative SMB\n",
    "        times = [np.datetime64(x) for x in bin.time.data] # datetimes\n",
    "        months = list(pd.DatetimeIndex(times).month) # months of each datetime\n",
    "        slas = np.zeros(len(times)) # initialize SLAs\n",
    "\n",
    "        # iterate over each time period\n",
    "        for j, time in enumerate(times):\n",
    "            # subset binned data to time\n",
    "            bin_time = bin.isel(time=j)\n",
    "            # grab the SMB \n",
    "            b_sum[j,:] = bin_time.bin_massbalclim_monthly.data[0]\n",
    "            # add the previous SMB (restart the count in October)\n",
    "            if months[j] != 10: \n",
    "                b_sum[j,:] += b_sum[j-1,:]\n",
    "            # If all SMB > 0, ELA = minimum elevation\n",
    "            if all(b_sum[j,:] > 0):\n",
    "                slas[j] = np.min(h)\n",
    "            # If SMB is > 0 and < 0 in some places, linearly interpolate ELA\n",
    "            elif any(b_sum[j,:] < 0) & any(b_sum[j,:] > 0):\n",
    "                slas[j] = np.interp(0, np.flip(b_sum[j,:]), np.flip(h))\n",
    "            # If SMB < 0 everywhere, fit a piecewise linear fit and extrapolate for SMB=0\n",
    "            elif all(b_sum[j,:] < 0):\n",
    "                X, y = b_sum[j,:], h\n",
    "                slas[j] = extrapolate_ela_linear(X, y, Iend=5)\n",
    "            else:\n",
    "                print('issue')\n",
    "\n",
    "        # compile in dataframe\n",
    "        df = pd.DataFrame({'Date': times,\n",
    "                           'SLA_mod_m': slas})\n",
    "        \n",
    "        # Because each SMB value represents the total SMB for each month, add 1 month to the dates\n",
    "        df['Date'] = df['Date'] + pd.DateOffset(months=1)\n",
    "        df['RGIId'] = rgi_id\n",
    "\n",
    "        slas_mod = pd.concat([slas_mod, df])\n",
    "            \n",
    "        i+=1\n",
    "\n",
    "    # Rearrange columns\n",
    "    slas_mod = slas_mod[['RGIId', 'Date', 'SLA_mod_m']]\n",
    "    # Save to file\n",
    "    slas_mod.to_csv(slas_mod_fn, index=False)\n",
    "    print('Modeled monthly SLAs saved to file:', slas_mod_fn)\n",
    "\n",
    "else:\n",
    "    \n",
    "    slas_mod = pd.read_csv(slas_mod_fn)\n",
    "    slas_mod['Date'] = pd.DatetimeIndex(slas_mod['Date'])\n",
    "    print('Modeled monthly SLAs loaded from file.')\n",
    "\n",
    "slas_mod.reset_index(drop=True, inplace=True)\n",
    "slas_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d98cc",
   "metadata": {},
   "source": [
    "### Remotely-sensed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c287520",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_obs_fn = os.path.join(scm_dir, 'analysis', 'monthly_SLAs_observed.csv')\n",
    "if not os.path.exists(slas_obs_fn):\n",
    "    # iterate over RGI IDs\n",
    "    slas_obs = pd.DataFrame()\n",
    "    for rgi_id in tqdm(sorted(aois['RGIId'].drop_duplicates().values)):\n",
    "        scs_fn = os.path.join(scm_dir, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "        scs = pd.read_csv(scs_fn)\n",
    "        scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "        scs['Year'] = scs['datetime'].dt.year\n",
    "        scs['Month'] = scs['datetime'].dt.month\n",
    "        scs['Day'] = scs['datetime'].dt.day\n",
    "        # Filter data to within one week of the first of each month\n",
    "        scs_filtered = scs[(scs['Day'] >= 25) | (scs['Day'] <= 7)]\n",
    "        # Grab monthly snowline\n",
    "        Imonths = []\n",
    "        dates = []\n",
    "        for year, month in scs_filtered[['Year', 'Month']].drop_duplicates().values:\n",
    "            first_of_month = pd.Timestamp(year=year, month=month, day=1)\n",
    "            # identify closest observation to this date\n",
    "            scs_filtered.loc[:, 'diff'] = np.abs(scs_filtered.loc[:, 'datetime'] - first_of_month)\n",
    "            Imonths.append(scs_filtered['diff'].idxmin())\n",
    "            # save date \n",
    "            dates.append(pd.Timestamp(f\"{year}-{month}-01\"))\n",
    "        scs_monthly = scs.iloc[Imonths].reset_index(drop=True)\n",
    "\n",
    "        # add date column that is first of month\n",
    "        scs_monthly['Date'] = dates\n",
    "\n",
    "        # concatenate to full dataframe\n",
    "        slas_obs = pd.concat([slas_obs, scs_monthly])\n",
    "\n",
    "    # select relevant columns\n",
    "    slas_obs.rename(columns={'ELA_from_AAR_m': 'SLA_obs_m'}, inplace=True)\n",
    "    slas_obs = slas_obs[['RGIId', 'Date', 'SLA_obs_m']]\n",
    "    slas_obs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # save to file\n",
    "    slas_obs.to_csv(slas_obs_fn, index=False)\n",
    "    print('Remotely-sensed monthly SLAs saved to file:', slas_obs_fn)\n",
    "\n",
    "else:  \n",
    "    slas_obs = pd.read_csv(slas_obs_fn)\n",
    "    slas_obs['Date'] = pd.to_datetime(slas_obs['Date'])\n",
    "    print('Remotely-sensed monthly SLAs loaded from file.')\n",
    "\n",
    "slas_obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512e6fe",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7383829",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grab minimum glacier elevations for standardizing ###\n",
    "# Define output file name\n",
    "min_sla_obs_fn = os.path.join(scm_dir, 'analysis', 'minimum_glacier_elevations_observed.csv')\n",
    "if not os.path.exists(min_sla_obs_fn):\n",
    "    # load AOIs\n",
    "    aois = gpd.read_file(aois_fn)\n",
    "    min_slas_obs = pd.DataFrame()\n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load snowlines\n",
    "        scs_fn = os.path.join(scm_dir, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "        scs = pd.read_csv(scs_fn)\n",
    "        # Remove any wonky values\n",
    "        scs.loc[np.abs(scs['ELA_from_AAR_m']) > 1e10] = np.nan\n",
    "        # Get minimum snowline altitude\n",
    "        min_sla = scs['ELA_from_AAR_m'].min()\n",
    "        # Add to dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id], 'SLA_obs_m_min': [min_sla]})\n",
    "        min_slas_obs = pd.concat([min_slas_obs, df], axis=0)\n",
    "\n",
    "    # Save to file\n",
    "    min_slas_obs.reset_index(drop=True, inplace=True)\n",
    "    min_slas_obs.to_csv(min_sla_obs_fn, index=False)\n",
    "    print('Minimum remotely-sensed glacier elevations saved to file:', min_sla_obs_fn)\n",
    "    \n",
    "else:\n",
    "    min_slas_obs = pd.read_csv(min_sla_obs_fn)\n",
    "    print('Minimum remotely-sensed glacier elevations loaded.')\n",
    "\n",
    "min_slas_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file\n",
    "slas_merged_fn = os.path.join(scm_dir, 'analysis', 'monthly_SLAs_modeled_observed_merged.csv')\n",
    "if not os.path.exists(slas_merged_fn):\n",
    "\n",
    "    # Merge modeled and remotely-sensed ELAs\n",
    "    slas_merged = slas_mod[['RGIId', 'Date', 'SLA_mod_m']].merge(slas_obs[['RGIId', 'Date', 'SLA_obs_m']],\n",
    "                                                                 on=['RGIId', 'Date'])\n",
    "    # Remove 2023 values (no modeled data in 2023)\n",
    "    slas_merged = slas_merged.loc[pd.DatetimeIndex(slas_merged['Date']).year < 2023]\n",
    "    \n",
    "    # Remove observations outside May - September\n",
    "    slas_merged = slas_merged.loc[(pd.DatetimeIndex(slas_merged['Date']).month >=5) \n",
    "                                  & (pd.DatetimeIndex(slas_merged['Date']).month <=9)]\n",
    "\n",
    "    # Subtract the minimum snowline altitudes to mitigate datum issues, s.t. SLAs are w.r.t. 0 m. \n",
    "    for rgi_id in slas_merged['RGIId'].drop_duplicates().values:\n",
    "        min_sla_obs = min_slas_obs.loc[min_slas_obs['RGIId']==rgi_id, 'SLA_obs_m_min'].values[0]\n",
    "        slas_merged.loc[slas_merged['RGIId']==rgi_id, 'SLA_obs_m'] -= min_sla_obs\n",
    "        min_sla_mod = slas_mod.loc[slas_mod['RGIId']==rgi_id, 'SLA_mod_m'].min()\n",
    "        slas_merged.loc[slas_merged['RGIId']==rgi_id, 'SLA_mod_m'] -= min_sla_mod\n",
    "\n",
    "    # Save results\n",
    "    slas_merged.to_csv(slas_merged_fn, index=False)\n",
    "    print('Merged monthly SLAs saved to file:', slas_merged_fn)\n",
    "\n",
    "else:\n",
    "    slas_merged = pd.read_csv(slas_merged_fn)\n",
    "    print('Merged monthly SLAs loaded.')\n",
    "\n",
    "\n",
    "slas_merged['SLA_mod-obs_m'] = slas_merged['SLA_mod_m'] - slas_merged['SLA_obs_m']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.hist(slas_merged['SLA_mod-obs_m'], bins=50)\n",
    "ax.set_xlabel('SLA$_{mod}$ - SLA$_{obs}$ [m]')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f'Mean diff = {np.nanmean((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'Std. diff = {np.nanstd((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'Median diff = {np.nanmedian((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'MAD diff = {MAD((slas_merged[\"SLA_mod-obs_m\"]).values, nan_policy=\"omit\")} m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2ecd9",
   "metadata": {},
   "source": [
    "## 2. ELAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad5f0a-21a1-4cc9-9ae5-0d82dc296576",
   "metadata": {},
   "source": [
    "### Modeled ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c401e-1035-42af-a623-2d6d03f8365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_mod_fn = os.path.join(scm_dir, 'analysis', 'annual_ELAs_modeled.csv')\n",
    "if not os.path.exists(elas_mod_fn):\n",
    "    # Add Year column\n",
    "    slas_mod['Year'] = pd.DatetimeIndex(slas_mod['Date']).year\n",
    "    # Identify the row of maximum ELA for each site and each year\n",
    "    Imax = slas_mod.groupby(by=['RGIId', 'Year'])['SLA_mod_m'].idxmax().values\n",
    "    elas_mod = slas_mod.iloc[Imax].reset_index(drop=True)\n",
    "    elas_mod.rename(columns={'SLA_mod_m': 'ELA_mod_m'}, inplace=True)\n",
    "    # Reorder columns\n",
    "    elas_mod = elas_mod[['RGIId', 'Date', 'Year', 'ELA_mod_m']]\n",
    "    # Save to file\n",
    "    elas_mod.to_csv(elas_mod_fn, index=False)\n",
    "    print('Modeled annual ELAs saved to file:', elas_mod_fn)\n",
    "else:\n",
    "    elas_mod = pd.read_csv(elas_mod_fn)\n",
    "    elas_mod['Date'] = pd.to_datetime(elas_mod['Date'])\n",
    "    print('Modeled annual ELAs loaded from file.')\n",
    "    \n",
    "elas_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a61b4",
   "metadata": {},
   "source": [
    "### Remotely-sensed ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_obs_fn = os.path.join(scm_dir, 'analysis', 'annual_ELAs_observed.csv')\n",
    "if not os.path.exists(elas_obs_fn):\n",
    "    # iterate over sites\n",
    "    elas_obs = pd.DataFrame()\n",
    "    for rgi_id in tqdm(slas_obs['RGIId'].drop_duplicates().values):\n",
    "        # Subset to site\n",
    "        slas_obs_site = slas_obs.loc[slas_obs['RGIId']==rgi_id].reset_index(drop=True)\n",
    "        # Subset to 2016–2023\n",
    "        slas_obs_site = slas_obs_site.loc[slas_obs_site['Date'].dt.year >= 2016].reset_index(drop=True)\n",
    "        # identify maximum annual SLA\n",
    "        imax = slas_obs_site.groupby(slas_obs_site['Date'].dt.year)['SLA_obs_m'].idxmax().values\n",
    "        df = slas_obs_site.iloc[imax]\n",
    "        # concatenate to full dataframe\n",
    "        elas_obs = pd.concat([elas_obs, df])\n",
    "    elas_obs.reset_index(drop=True, inplace=True)\n",
    "    elas_obs.rename(columns={'SLA_obs_m': 'ELA_obs_m'}, inplace=True)\n",
    "\n",
    "    # save to file\n",
    "    elas_obs.to_csv(elas_obs_fn, index=False)\n",
    "    print('Remotely-sensed ELAs saved to file:', elas_obs_fn)\n",
    "\n",
    "else:\n",
    "    elas_obs = pd.read_csv(elas_obs_fn)\n",
    "    elas_obs['Date'] = pd.to_datetime(elas_obs['Date'])\n",
    "    print('Remotely-sensed ELAs loaded from file.')\n",
    "\n",
    "elas_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739099a-00f6-428a-96ed-63a9a7fd655c",
   "metadata": {},
   "source": [
    "### Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bb9a5-a22f-41a1-9f5c-31f002300a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file name\n",
    "elas_merged_fn = os.path.join(scm_dir, 'analysis', 'annual_ELAs_modeled_observed_merged.csv')\n",
    "if not os.path.exists(elas_merged_fn):\n",
    "\n",
    "    # Merge modeled and remotely-sensed modeled ELAs\n",
    "    elas_obs['Year'] = elas_obs['Date'].dt.year\n",
    "    elas_merged = elas_obs[['RGIId', 'Year', 'ELA_obs_m']].merge(elas_mod[['RGIId', 'Year', 'ELA_mod_m']],\n",
    "                                                                 on=['RGIId', 'Year'])\n",
    "    \n",
    "    # Subset to 2016–2022 (no modeled data in 2023)\n",
    "    elas_merged = elas_merged.loc[(elas_merged['Year'] >= 2016) \n",
    "                                                & (elas_merged['Year'] < 2023)]\n",
    "        \n",
    "    # Save results\n",
    "    elas_merged.to_csv(elas_merged_fn, index=False)\n",
    "    print('Merged annual ELAs saved to file:', elas_merged_fn)\n",
    "\n",
    "else:\n",
    "    elas_merged = pd.read_csv(elas_merged_fn)\n",
    "    print('Merged annual ELAs loaded.')\n",
    "    \n",
    "# Calculate difference\n",
    "elas_merged['ELA_mod-obs_m'] = elas_merged['ELA_mod_m'] - elas_merged['ELA_obs_m']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.hist(elas_merged['ELA_mod-obs_m'], bins=50)\n",
    "ax.set_xlabel('ELA$_{mod}$ - ELA$_{obs}$ [m]')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f\"Mean diff = {np.nanmean(elas_merged['ELA_mod-obs_m'])} m\")\n",
    "print(f\"Std. diff = {np.nanstd(elas_merged['ELA_mod-obs_m'])} m\")\n",
    "print(f\"Median diff = {np.nanmedian(elas_merged['ELA_mod-obs_m'])} m\")\n",
    "print(f\"MAD diff = {MAD(elas_merged['ELA_mod-obs_m'], nan_policy='omit')} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389089dc",
   "metadata": {},
   "source": [
    "## 3. Modeled SMB at remotely-sensed snowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b936a9e-befa-434a-ab15-a3ef98b6d092",
   "metadata": {},
   "outputs": [],
   "source": [
    "sla_obs_smb_mod_fn = os.path.join(scm_dir, 'analysis', 'modeled_SMB_at_observed_SLA.csv')\n",
    "if not os.path.exists(sla_obs_smb_mod_fn):\n",
    "    # Intialize dataframe\n",
    "    sla_obs_smb_mod = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over RGI IDs\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        mod_smb_fn = glob.glob(os.path.join(model_dir, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "        mod_smb = xr.open_dataset(mod_smb_fn)\n",
    "        slas_obs_site = slas_obs.loc[slas_obs['RGIId']==rgi_id]\n",
    "\n",
    "        # grab data variables\n",
    "        h = mod_smb.bin_surface_h_initial.data[0] # surface elevation [m]\n",
    "        b_sum = np.zeros((len(mod_smb.time.data), len(h))) # cumulative SMB\n",
    "        dts = [pd.Timestamp(np.datetime64(x)) for x in mod_smb.time.data] # datetimes\n",
    "        months = list(pd.DatetimeIndex(dts).month) # months of each datetime\n",
    "\n",
    "        # iterate over each time period\n",
    "        for j, dt in enumerate(dts):\n",
    "            # subset binned data to time\n",
    "            mod_smb_time = mod_smb.isel(time=j)\n",
    "            # grab the SMB \n",
    "            b_sum[j,:] = mod_smb_time.bin_massbalclim_monthly.data[0]\n",
    "            # add the previous SMB (restart the count in October)\n",
    "            if months[j] != 10: \n",
    "                b_sum[j,:] += b_sum[j-1,:]\n",
    "            # grab observed SLA\n",
    "            sla = slas_obs_site.loc[slas_obs_site['Date']==dt, 'SLA_obs_m'].values\n",
    "            if len(sla) > 0:\n",
    "                # interpolate SMB at SLA\n",
    "                smb_sla = np.interp(sla[0], h, b_sum[j,:])\n",
    "                df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                                   'Date': [dt],\n",
    "                                   'SMB_at_SLA_mwe': [smb_sla]})\n",
    "                # add to full dataframe\n",
    "                sla_obs_smb_mod = pd.concat([sla_obs_smb_mod, df], axis=0)\n",
    "                \n",
    "    # Save to file\n",
    "    sla_obs_smb_mod.reset_index(drop=True, inplace=True)\n",
    "    sla_obs_smb_mod.to_csv(sla_obs_smb_mod_fn, index=False)\n",
    "    print('SMB at SLA saved to file:', sla_obs_smb_mod_fn)\n",
    "\n",
    "else:\n",
    "    sla_obs_smb_mod = pd.read_csv(sla_obs_smb_mod_fn)\n",
    "    print('SMB at SLA loaded from file.')\n",
    "    \n",
    "sla_obs_smb_mod\n",
    "    \n",
    "                \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f90cc",
   "metadata": {},
   "source": [
    "## 4. Melt factors of snow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413e507",
   "metadata": {},
   "source": [
    "### Modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if already exists in file\n",
    "fsnow_mod_fn = os.path.join(scm_dir, 'analysis', 'fsnow_modeled.csv')\n",
    "if not os.path.exists(fsnow_mod_fn):\n",
    "    print('Compiling modeled melt factors of snow')\n",
    "    # Load AOIs for RGI IDs\n",
    "    aois_fn = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/analysis/all_aois.shp'\n",
    "    aois = gpd.read_file(aois_fn)\n",
    "    modelprm_dir = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/Rounce_et_al_2023/modelprms'\n",
    "    # Initialize dataframe\n",
    "    fsnow_mod = pd.DataFrame()\n",
    "    # Iterate over RGI IDs\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load model parameters\n",
    "        modelprm_fn = os.path.join(modelprm_dir, f\"{rgi_id.replace('RGI60-0','')}-modelprms_dict.pkl\")\n",
    "        modelprm = pd.read_pickle(modelprm_fn)\n",
    "        # Take the median of MCMC fsnow results (not much different than the mean)\n",
    "        ddfsnow_mcmc = np.array(modelprm['MCMC']['ddfsnow']['chain_0'])\n",
    "        df = pd.DataFrame({\"RGIId\": [rgi_id],\n",
    "                           \"fsnow_mod\": [np.median(ddfsnow_mcmc)]})\n",
    "        # Concatenate df to full dataframe\n",
    "        fsnow_mod = pd.concat([fsnow_mod, df])\n",
    "    # Save to file\n",
    "    fsnow_mod.reset_index(drop=True, inplace=True)\n",
    "    fsnow_mod.to_csv(fsnow_mod_fn, index=False)\n",
    "    print('Compiled melt factors of snow saved to file:', fsnow_mod_fn)\n",
    "\n",
    "else:\n",
    "    fsnow_mod = pd.read_csv(fsnow_mod_fn)\n",
    "    print('Compiled melt factors of snow loaded from file.')\n",
    "\n",
    "plt.hist(fsnow_mod['fsnow_mod'], bins=np.linspace(0, 0.006, 50))\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8542d2d6",
   "metadata": {},
   "source": [
    "### Observed\n",
    "\n",
    "$Accumulation - Melt = SMB$\n",
    "\n",
    "$\\Sigma(Precip_{solid}) - \\Sigma(PDDs)*f_{snow} = SMB$\n",
    "\n",
    "At the snowline, SMB = 0. Solve for $f_{snow}$ at observed snowline altitudes.\n",
    "\n",
    "$f_{snow} = \\frac{\\Sigma Precip_{solid} (h_{sl})}{\\Sigma PDDs (h_{sl})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c695629",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsnow_obs_fn = os.path.join(scm_dir, 'analysis', 'fsnow_observed.csv')\n",
    "if not os.path.exists(fsnow_obs_fn):\n",
    "\n",
    "    # Load calculated lapse rates\n",
    "    lapse_fn = os.path.join(model_dir, 'ERA5_lapserates_monthly.nc')\n",
    "    lapse = xr.open_dataset(lapse_fn)\n",
    "\n",
    "    # Load ERA5 geopotential, convert to geoidal height\n",
    "    gp_fn = os.path.join(model_dir, 'ERA5_geopotential.nc')\n",
    "    gp = xr.open_dataset(gp_fn).squeeze()\n",
    "    gp['h'] = gp['z'] / 9.81\n",
    "    # gp['h'].plot(vmin=-100, vmax=5e3)\n",
    "\n",
    "    # Load ERA5 monthly temperatures and precipitation\n",
    "    temp_fn = os.path.join(model_dir, 'ERA5_temp_monthly.nc')\n",
    "    temp = xr.open_dataset(temp_fn)\n",
    "    precip_fn = os.path.join(model_dir, 'ERA5_totalprecip_monthly.nc')\n",
    "    precip = xr.open_dataset(precip_fn)\n",
    "\n",
    "    # Iterate over glaciers\n",
    "    fsnow_obs = pd.DataFrame()\n",
    "    for rgi_id in tqdm(slas_mod['RGIId'].drop_duplicates().values):\n",
    "\n",
    "        # Load monthly remotely-sensed SLAs\n",
    "        slas_obs_glacier = slas_obs.loc[slas_obs['RGIId']==rgi_id]\n",
    "        slas_obs_glacier = slas_obs_glacier.loc[slas_obs_glacier['Date'] <= temp.time.max().values]\n",
    "\n",
    "        # Load modeled monthly SMB\n",
    "        smb_fn = glob.glob(os.path.join(model_dir, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "        smb = xr.open_dataset(smb_fn)\n",
    "\n",
    "        # Subset model files to glacier\n",
    "        lat, lon = smb.CenLat.values[0], smb.CenLon.values[0] + 360\n",
    "        gp_glacier = gp.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "        lapse_glacier = lapse.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "        temp_glacier = temp.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "        precip_glacier = precip.sel(latitude=lat, longitude=lon, method='nearest')\n",
    "        # average over the \"expver\" dimension\n",
    "        temp_glacier = temp_glacier.mean(dim='expver') \n",
    "        precip_glacier = precip_glacier.mean(dim='expver') \n",
    "        # subset to 2012 on\n",
    "        temp_glacier = temp_glacier.sel(time=slice(\"2012-10-01\", None))\n",
    "        precip_glacier = precip_glacier.sel(time=slice(\"2012-10-01\", None))\n",
    "\n",
    "        # Convert temperatures in K to C\n",
    "        temp_glacier['t2m_C'] = temp_glacier['t2m'] - 273.15\n",
    "        \n",
    "        # # Resample temperatures to daily for PDD calculations\n",
    "        # temp_glacier = temp_glacier.resample('1D').interpolate(\"linear\")\n",
    "\n",
    "        # Difference ERA5 heights from glacier elevations\n",
    "        h = smb.bin_surface_h_initial\n",
    "        elev_diff = h - gp_glacier['h'] - 2 # account for 2m temperature\n",
    "\n",
    "        # Apply lapse rates to temperatures\n",
    "        temp_glacier['t2m_C_adj'] = temp_glacier['t2m_C'] + (lapse_glacier['lapserate'] * elev_diff)\n",
    "\n",
    "        # Calculate PDDs\n",
    "        temp_glacier['PDD'] = xr.where(temp_glacier['t2m_C_adj'] > 0, temp_glacier['t2m_C_adj'], 0) # by month\n",
    "        temp_glacier['PDD'] = temp_glacier['PDD'] * temp_glacier.time.dt.days_in_month\n",
    "        def water_year_da(time):\n",
    "            year = time.dt.year\n",
    "            return xr.where(time.dt.month >= 10, year, year - 1)\n",
    "        temp_glacier = temp_glacier.assign_coords(water_year=water_year_da(temp_glacier['time']))\n",
    "        temp_glacier['PDD_cumsum'] = temp_glacier['PDD'].groupby(\"water_year\").cumsum(dim=\"time\")\n",
    "\n",
    "        # Estimate snow as precipitation when temperatures are positive\n",
    "        precip_glacier['snow'] = xr.where(temp_glacier['t2m_C_adj'] < 0, precip_glacier['tp'], 0) # per month\n",
    "        precip_glacier['snow'] = precip_glacier['snow'] * precip_glacier.time.dt.days_in_month # per day\n",
    "\n",
    "        # Calculate cumulative annual snowfall\n",
    "        precip_glacier = precip_glacier.assign_coords(water_year=water_year_da(precip_glacier['time']))\n",
    "        precip_glacier['snow_cumsum'] = precip_glacier['snow'].groupby(\"water_year\").cumsum(dim=\"time\")\n",
    "\n",
    "        # Estimate melt factors of snow\n",
    "        fsnow_df = pd.DataFrame()\n",
    "        dates = slas_obs_glacier['Date'].values\n",
    "        fsnows = np.zeros(len(dates))\n",
    "        h_adj = h - h.min() # remove minimum elevation for comparison with observed\n",
    "        for j, date in enumerate(dates):\n",
    "            sla_obs_date = slas_obs_glacier.loc[slas_obs_glacier['Date']==date, 'SLA_obs_m'].values[0]\n",
    "            sla_obs_date -= min_slas_obs.loc[min_slas_obs['RGIId']==rgi_id, 'SLA_obs_m_min'].values[0]\n",
    "            pdd_sum = np.interp(sla_obs_date, \n",
    "                                h.values.ravel(), \n",
    "                                temp_glacier.sel(time=date)['PDD_cumsum'].values.ravel())\n",
    "            snow_sum = np.interp(sla_obs_date, \n",
    "                                 h.values.ravel(), \n",
    "                                 precip_glacier.sel(time=date)['snow_cumsum'].values.ravel())\n",
    "            if pdd_sum==0:\n",
    "                fsnows[j] = 0\n",
    "            else:\n",
    "                fsnows[j] = snow_sum / pdd_sum\n",
    "\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                           'fsnow_obs': [np.nanmedian(fsnows)]})   \n",
    "        fsnow_obs = pd.concat([fsnow_obs, df], axis=0)\n",
    "        \n",
    "    # Save to file\n",
    "    fsnow_obs.to_csv(fsnow_obs_fn, index=False)\n",
    "    print('Observed melt factors of snow saved to file:', fsnow_obs_fn)\n",
    "\n",
    "else:\n",
    "    fsnow_obs = pd.read_csv(fsnow_obs_fn)\n",
    "    print('Observed melt factors of snow loaded from file.')\n",
    "    \n",
    "plt.hist(fsnow_obs['fsnow_obs'], bins=np.linspace(0, 0.0006, 50))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81207594",
   "metadata": {},
   "outputs": [],
   "source": [
    "precip_glacier.mean(dim='bin')['snow_cumsum'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf55d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_glacier.mean(dim='bin')['PDD_cumsum'].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c3d3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier-snow-cover-mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
