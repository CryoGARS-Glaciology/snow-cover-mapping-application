{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa19aea-f65a-4694-aa1b-1c1c24efbf58",
   "metadata": {},
   "source": [
    "# Compare modeled and remotely-sensed surface mass balance\n",
    "\n",
    "Requires mass balance model outputs at each site from [PyGEM](https://github.com/PyGEM-Community/PyGEM) (Rounce et al., 2023), which can be downloaded form the Carnegie Mellon data repository.\n",
    "\n",
    "The files downloaded for this work were:\n",
    "- Monthly surface mass balance along glacier centerlines for 2000â€“2022: \"binned\", downloaded from [global_ERA5_2000_2022](https://cmu.app.box.com/s/rzk8aeasg40dd3p0xr3yngkc5c0m8kxt/folder/251139952066)\n",
    "- Calibrated model parameters (degree-day factors of snow and temperature biases): \"{RGI ID}_modelprms_dict.json\" files downloaded from [pygem_datasets > Calibration](https://cmu.app.box.com/s/p8aiby5s9f3n6ycgmhknbgo4htk3pn9j/folder/298954564072)\n",
    "\n",
    "All files were placed in a folder called \"Rounce_et_al_2023\", defined with the `model_path` variable below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd10b4fc-0d6f-45af-b2ac-9c92269f5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bde12fe-197d-41a7-8ec0-5b0d34956a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths for inputs and outputs\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "model_path = os.path.join(scm_path, 'Rounce_et_al_2023')\n",
    "out_path = os.path.join(scm_path, 'analysis')\n",
    "code_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis'\n",
    "\n",
    "# Load glacier boundaries for RGI IDs\n",
    "aois_fn = os.path.join(scm_path, 'analysis', 'AOIs.gpkg')\n",
    "aois = gpd.read_file(aois_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc2430",
   "metadata": {},
   "source": [
    "## 1. Monthly snowline altitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d98cc",
   "metadata": {},
   "source": [
    "### Remotely-sensed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c287520",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_obs_fn = os.path.join(out_path, 'monthly_SLAs_observed.nc')\n",
    "if not os.path.exists(slas_obs_fn):\n",
    "    # Initialize a list to store DataFrames\n",
    "    slas_obs_list = []\n",
    "    \n",
    "    # Iterate over RGI IDs\n",
    "    for rgi_id in tqdm(sorted(aois['RGIId'].drop_duplicates().values)):\n",
    "        scs_fn = os.path.join(scm_path, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "        scs = pd.read_csv(scs_fn)\n",
    "        scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "        scs['Year'] = scs['datetime'].dt.year\n",
    "        scs['Month'] = scs['datetime'].dt.month\n",
    "        scs['Day'] = scs['datetime'].dt.day\n",
    "        \n",
    "        # Filter data to within one week of the first of each month\n",
    "        scs_filtered = scs[(scs['Day'] >= 25) | (scs['Day'] <= 7)]\n",
    "        \n",
    "        # Grab monthly snowline altitude\n",
    "        Imonths = []\n",
    "        dates = []\n",
    "        for year, month in scs_filtered[['Year', 'Month']].drop_duplicates().values:\n",
    "            first_of_month = pd.Timestamp(year=year, month=month, day=1)\n",
    "            \n",
    "            # Identify closest observation to this date\n",
    "            scs_filtered.loc[:, 'diff'] = np.abs(scs_filtered.loc[:, 'datetime'] - first_of_month)\n",
    "            Imonths.append(scs_filtered['diff'].idxmin())\n",
    "            dates.append(first_of_month)\n",
    "        \n",
    "        scs_monthly = scs.iloc[Imonths].reset_index(drop=True)\n",
    "        scs_monthly['Date'] = dates\n",
    "        scs_monthly['RGIId'] = rgi_id\n",
    "        slas_obs_list.append(scs_monthly[['RGIId', 'Date', 'SLA_from_AAR_m']])\n",
    "    \n",
    "    # Combine all DataFrames\n",
    "    slas_obs = pd.concat(slas_obs_list)\n",
    "    slas_obs.rename(columns={'SLA_from_AAR_m': 'SLA_obs_m'}, inplace=True)\n",
    "    \n",
    "    # Create xarray Dataset\n",
    "    slas_obs_pivot = slas_obs.pivot(index='Date', columns='RGIId', values='SLA_obs_m')\n",
    "    slas_obs_pivot = slas_obs_pivot.sort_index()\n",
    "    \n",
    "    # Convert to xarray Dataset\n",
    "    slas_obs_xr = xr.Dataset(\n",
    "        {\"SLA_obs_m\": (['time', 'RGIId'], slas_obs_pivot.values)},\n",
    "        coords={\"time\": slas_obs_pivot.index.values,\n",
    "                \"RGIId\": slas_obs_pivot.columns.values}\n",
    "    )\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    slas_obs_xr.to_netcdf(slas_obs_fn)\n",
    "    print('Remotely-sensed monthly SLAs saved to file:', slas_obs_fn)\n",
    "\n",
    "else:\n",
    "    # Load from file\n",
    "    slas_obs_xr = xr.load_dataset(slas_obs_fn)\n",
    "    print('Remotely-sensed monthly SLAs loaded from file.')\n",
    "\n",
    "slas_obs_xr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2c326-e946-4ec3-b453-2149a351f311",
   "metadata": {},
   "source": [
    "### Modeled SLAs and SMB at observed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b26c4-b13f-4f05-b233-bd6e773bfdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file already exists\n",
    "slas_mod_fn = os.path.join(out_path, 'monthly_SLAs_modeled.nc')\n",
    "if not os.path.exists(slas_mod_fn):\n",
    "    \n",
    "    # Initialize a list to store DataFrames\n",
    "    slas_mod_list = []\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load modeled monthly SMB\n",
    "        smb_fn = glob.glob(os.path.join(model_path, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "        smb = xr.open_dataset(smb_fn)\n",
    "        # calculate cumulative SMB\n",
    "        def water_year(date):\n",
    "            if date.month >= 10:\n",
    "                return date.year\n",
    "            else:\n",
    "                return date.year - 1\n",
    "        smb = smb.assign_coords({'water_year': (['time'], [water_year(t) for t in smb.time.values])})\n",
    "        smb['bin_massbalclim_monthly_cumsum'] = smb['bin_massbalclim_monthly'].groupby('water_year').cumsum()\n",
    "        smb['time'] = smb.time.values.astype('datetime64[D]')\n",
    "        h = smb['bin_surface_h_initial'].data.ravel()\n",
    "        \n",
    "        # Interpolate modeled SLA as where SMB = 0 and SMB at the observed SLA\n",
    "        slas = np.nan * np.zeros(len(smb.time.data))\n",
    "        smb_at_slas = np.nan * np.zeros(len(smb.time.data))\n",
    "        for j, t in enumerate(smb.time.data):\n",
    "            smb_time = smb.sel(time=t)['bin_massbalclim_monthly_cumsum'].data[0]\n",
    "            # when SMB <= 0 everywhere, set SLA to maximum glacier elevation\n",
    "            if np.all(smb_time <= 0):\n",
    "                slas[j] = np.max(h)\n",
    "            # when SMB >= 0 everywhere, set SLA to minimum glacier elevation\n",
    "            elif np.all(smb_time >= 0):\n",
    "                slas[j] = np.min(h)\n",
    "            # otherwise, linearly interpolate SLA\n",
    "            else:\n",
    "                sorted_indices = np.argsort(h)\n",
    "                slas[j] = np.interp(0, smb_time[sorted_indices], h[sorted_indices])\n",
    "            # interpolate the modeled SMB at the observed SLA\n",
    "            sla_obs = slas_obs.loc[(slas_obs['RGIId']==rgi_id) & (slas_obs['Date']==t), 'SLA_obs_m']\n",
    "            if len(sla_obs) > 0:\n",
    "                smb_at_slas[j] = np.interp(sla_obs.values[0], h, smb_time)\n",
    "\n",
    "        # Save results in dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id]*len(smb.time.data),\n",
    "                            'Date': smb.time.data,\n",
    "                            'SLA_mod_m': slas,\n",
    "                            'SMB_at_SLA_obs_mwe': smb_at_slas})\n",
    "        # concatenate to dataframe list\n",
    "        slas_mod_list.append(df)\n",
    "        \n",
    "    # Combine all DataFrames\n",
    "    slas_mod = pd.concat(slas_mod_list)\n",
    "    \n",
    "    # Create xarray Dataset\n",
    "    slas_mod_pivot = slas_mod.pivot(index='Date', columns='RGIId', values=['SLA_mod_m', 'SMB_at_SLA_obs_mwe'])\n",
    "    slas_mod_pivot = slas_mod_pivot.sort_index()\n",
    "    \n",
    "    # Convert to xarray Dataset\n",
    "    slas_mod_xr = xr.Dataset(\n",
    "        {\"SLA_mod_m\": (['time', 'RGIId'], slas_mod_pivot['SLA_mod_m'].values),\n",
    "         \"SMB_at_SLA_obs_mwe\":(['time', 'RGIId'], slas_mod_pivot['SMB_at_SLA_obs_mwe'].values)},\n",
    "        coords={\"time\": slas_mod_pivot.index.values,\n",
    "                \"RGIId\": slas_mod_pivot.columns.levels[1].values}\n",
    "    )\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    slas_mod_xr.to_netcdf(slas_mod_fn)\n",
    "    print('Modeled monthly SLAs and snowline SMB saved to file:', slas_mod_fn)\n",
    "    \n",
    "else:\n",
    "    # Load from file\n",
    "    slas_mod_xr = xr.load_dataset(slas_mod_fn)\n",
    "    print('Modeled monthly SLAs loaded from file.')\n",
    "\n",
    "slas_mod_xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512e6fe",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file\n",
    "slas_merged_fn = os.path.join(out_path, 'monthly_SLAs_observed_modeled.nc')\n",
    "if not os.path.exists(slas_merged_fn):\n",
    "\n",
    "    # Merge modeled and remotely-sensed SLAs and modeled SMB at observed snowline\n",
    "    slas_merged = xr.merge([slas_obs_xr, slas_mod_xr])\n",
    "    \n",
    "    # Remove 2000-2012 (no observed values) and 2023 (no modeled values)\n",
    "    slas_merged.sel(time=slice(\"2013-01-01\",\"2023-01-01\"))\n",
    "    \n",
    "    # Remove observations outside May - November (no observed values)\n",
    "    def filter_month_range(month):\n",
    "        return (month >= 5) & (month <= 10)\n",
    "    slas_merged = slas_merged.sel(time=filter_month_range(slas_merged['time.month']))\n",
    "    \n",
    "    # Save results\n",
    "    slas_merged.to_netcdf(slas_merged_fn)\n",
    "    print('Merged monthly SLAs saved to file:', slas_merged_fn)\n",
    "\n",
    "else:\n",
    "    slas_merged = xr.load_dataset(slas_merged_fn)\n",
    "    print('Merged monthly SLAs loaded from file.')\n",
    "\n",
    "slas_merged['SLA_mod-obs_m'] = slas_merged['SLA_mod_m'] - slas_merged['SLA_obs_m']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.hist(slas_merged['SLA_mod-obs_m'].values.ravel(), bins=50)\n",
    "ax.set_xlabel('SLA$_{mod}$ - SLA$_{obs}$ [m]')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f'Mean diff = {np.nanmean((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'Std. diff = {np.nanstd((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'Median diff = {np.nanmedian((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'MAD diff = {MAD((slas_merged[\"SLA_mod-obs_m\"]).values.ravel(), nan_policy=\"omit\")} m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2ecd9",
   "metadata": {},
   "source": [
    "## 2. ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c401e-1035-42af-a623-2d6d03f8365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_elas_merged_fn = os.path.join(out_path, 'monthly_SLAs_annual_ELAs_observed_modeled.nc')\n",
    "\n",
    "if not os.path.exists(slas_elas_merged_fn):\n",
    "    # Make a copy of the SLAs\n",
    "    slas_elas_merged = slas_merged.copy()\n",
    "    \n",
    "    # Identify maximum annual observed SLAs\n",
    "    slas_elas_merged['ELA_obs_m'] = slas_elas_merged['SLA_obs_m'].groupby(['time.year']).max()\n",
    "    \n",
    "    # Identify maximum annual modeled SLAs \n",
    "    slas_elas_merged['ELA_mod_m'] = slas_elas_merged['SLA_mod_m'].groupby(['time.year']).max()\n",
    "    \n",
    "    # Save to NetCDF file\n",
    "    slas_elas_merged.to_netcdf(slas_elas_merged_fn)\n",
    "    print('Merged SLAs and ELAs saved to file:', slas_elas_merged_fn)\n",
    "\n",
    "else:\n",
    "    slas_elas_merged = xr.load_dataset(slas_elas_merged_fn)\n",
    "    print('Merged SLAs and ELAs loaded from file.')\n",
    "\n",
    "slas_elas_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f90cc",
   "metadata": {},
   "source": [
    "## 3. Degree-day factors of snow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413e507",
   "metadata": {},
   "source": [
    "### Modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if already exists in file\n",
    "fsnow_mod_fn = os.path.join(scm_path, 'analysis', 'fsnow_modeled.csv')\n",
    "if not os.path.exists(fsnow_mod_fn):\n",
    "    print('Compiling modeled melt factors of snow')\n",
    "    # Initialize dataframe\n",
    "    fsnow_mod = pd.DataFrame()\n",
    "    # Iterate over RGI IDs\n",
    "    for rgi_id in tqdm(slas_mod['RGIId'].drop_duplicates().values):\n",
    "        # Load model parameters\n",
    "        modelprm_fn = os.path.join(model_path, 'modelprms', f\"{rgi_id.replace('RGI60-0','')}-modelprms_dict.pkl\")\n",
    "        modelprm = pd.read_pickle(modelprm_fn)\n",
    "        # Take the median of MCMC fsnow results (not much different than the mean)\n",
    "        ddfsnow_mcmc = np.array(modelprm['MCMC']['ddfsnow']['chain_0'])\n",
    "        df = pd.DataFrame({\"RGIId\": [rgi_id],\n",
    "                           \"fsnow_mod_m/C/d\": [np.median(ddfsnow_mcmc)]})\n",
    "        # Concatenate df to full dataframe\n",
    "        fsnow_mod = pd.concat([fsnow_mod, df])\n",
    "    # Save to file\n",
    "    fsnow_mod.reset_index(drop=True, inplace=True)\n",
    "    fsnow_mod.to_csv(fsnow_mod_fn, index=False)\n",
    "    print('Compiled melt factors of snow saved to file:', fsnow_mod_fn)\n",
    "\n",
    "else:\n",
    "    fsnow_mod = pd.read_csv(fsnow_mod_fn)\n",
    "    print('Compiled melt factors of snow loaded from file.')\n",
    "\n",
    "plt.hist(fsnow_mod['fsnow_mod_m/C/d'] * 1e3, bins=100)\n",
    "plt.xlabel('Melt factor of snow [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bb9d4",
   "metadata": {},
   "source": [
    "### Observed\n",
    "\n",
    "Adjust the modeled degree-day factors of snow ($f_{snow}$) using the modeled SMB and cumulative PDDs from ERA5 downscaled to the snowline.\n",
    "\n",
    "$SMB(x,t) = Accumulation - Melt = \\Sigma Snowfall(x,t) - \\sum_{t_{melt}}^t PDD(x,t) \\cdot \\Delta t \\cdot f_{snow}$\n",
    "\n",
    "where $t_{melt}$ is the start of the melt season and $\\Delta t$ is $t-t_{melt}$. \n",
    "\n",
    "At the snowline, SMB = 0. Rearranging:\n",
    "\n",
    "$f_{snow}(x,t) = \\frac{\\Sigma Snowfall(x,t)}{\\sum_{t_{melt}}^t PDD(x,t) \\cdot \\Delta t} $\n",
    "\n",
    "If SMB = 10 m at the snowline on day 100 of the melt season and the cumulative PDD are 100 $^{\\circ}C$, this means that the model underestimated melt by 10 m / (100 $^{\\circ}C \\cdot$ 100 days) = 0.001 m/C/d. If the modeled melt factor of snow is 2 m/C/d, adjust the fsnow to 2.001 m/C/d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsnow_merged_fn = os.path.join(out_path, 'fsnow_observed_modeled.csv')\n",
    "if not os.path.exists(fsnow_merged_fn):\n",
    "\n",
    "    # Intialize results for all sites\n",
    "    fsnow_merged = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        if type(rgi_id) != str:\n",
    "            continue\n",
    "        if 'RGI' not in rgi_id:\n",
    "            continue\n",
    "            \n",
    "        ### Load input data\n",
    "        # Get modeled fsnow\n",
    "        fsnow_mod_site = fsnow_mod.loc[fsnow_mod['RGIId']==rgi_id, 'fsnow_mod_m/C/d'].values[0]\n",
    "        # Load ERA-Land data\n",
    "        era_fn = os.path.join(scm_path, 'study-sites', rgi_id, 'ERA', f\"{rgi_id}_ERA5-Land_daily_means.csv\")\n",
    "        era_df = pd.read_csv(era_fn)\n",
    "        era_df['Date'] = pd.to_datetime(era_df['Date'])\n",
    "        # Load model temperature bias\n",
    "        modelprm_fn = os.path.join(model_path, 'modelprms', f\"{rgi_id.replace('RGI60-0','')}-modelprms_dict.pkl\")\n",
    "        modelprm = pd.read_pickle(modelprm_fn)\n",
    "        tbias = np.median(modelprm['MCMC']['tbias']['chain_0'])\n",
    "        # Load centerline elevation profile\n",
    "        smb_fn = glob.glob(os.path.join(model_path, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "        smb = xr.open_dataset(smb_fn).squeeze()\n",
    "        h = smb.bin_surface_h_initial.values.ravel()\n",
    "        # Grab snowlines for site\n",
    "        slas_site = slas_merged.sel(RGIId=rgi_id)\n",
    "        # Don't include dates after September\n",
    "        def filter_month_range(month):\n",
    "            return month < 9\n",
    "        slas_site = slas_site.sel(time=filter_month_range(slas_site['time.month']))\n",
    "        \n",
    "        ### Downscale air temperatures to glacier surface using lapse rates, calculate PDDs\n",
    "        era_ds = xr.Dataset(\n",
    "            coords={'h': h, 'time': era_df['Date']},\n",
    "            data_vars={'temp_C': (['time'], era_df['mean_temperature_2m_C'].values),\n",
    "                    'lapse_rate': (['time'], era_df['lapse_rate_C/m'])})\n",
    "        # implement temperature bias\n",
    "        era_ds['temp_C'] -= tbias\n",
    "        era_ds['h_diff'] = era_df['ERA5_height_mean_m'].values[0] - era_ds['h']\n",
    "        era_ds['temp_downscaled_C'] = era_ds['temp_C'] - era_ds['lapse_rate'] * era_ds['h_diff']\n",
    "        era_ds['PDD'] = xr.where(era_ds['temp_downscaled_C'] > 0, era_ds['temp_downscaled_C'], 0)\n",
    "        era_ds['PDD_cumsum'] = era_ds['PDD'].groupby('time.year').cumsum()\n",
    "        \n",
    "        ### Identify the melt season start date (first PDD > 0) for each elevation\n",
    "        def find_first_positive(group):\n",
    "            # Mask PDD = 0\n",
    "            mask = group > 0\n",
    "            # Find the first index where PDD > 0\n",
    "            first_index = mask.argmax(dim=\"time\")\n",
    "            # Check if no positive PDD exists for the group\n",
    "            no_positive = ~mask.any(dim=\"time\")\n",
    "            # Grab the corresponding time values\n",
    "            time_values = group[\"time\"].isel(time=first_index)\n",
    "            # Replace invalid times with NaT for no_positive cases\n",
    "            time_values = time_values.where(~no_positive, np.datetime64(\"NaT\"))\n",
    "            return time_values\n",
    "        # Apply the function to each year and elevation group\n",
    "        era_ds['melt_season_start_date'] = (era_ds[\"PDD_cumsum\"]\n",
    "                                            .groupby(\"time.year\")\n",
    "                                            .map(find_first_positive))\n",
    "        \n",
    "        ### Interpolate cumulative PDDs at SLAs\n",
    "        pdds_slas = np.array([float(era_ds.sel(time=date, h=sla, method='nearest')['PDD_cumsum'].values) \n",
    "                              for (date,sla) in list(zip(slas_site.time.values, slas_site['SLA_obs_m'].values))])\n",
    "\n",
    "        ### Interpolate melt season start dates at SLAs\n",
    "        melt_start_dates = np.array([era_ds.sel(year=year, h=sla, method='nearest')['melt_season_start_date'].values\n",
    "                                     for (year,sla) in list(zip(slas_site['time.year'].values, slas_site['SLA_obs_m'].values))])\n",
    "\n",
    "        # Compile in dataframe\n",
    "        df = pd.DataFrame({'Date': slas_site.time.values,\n",
    "                           'Year': slas_site['time.year'].values,\n",
    "                           'SMB_at_SLA_obs_mwe': slas_site['SMB_at_SLA_obs_mwe'].values,\n",
    "                           'PDD_cumsum_at_SLA_C': pdds_slas,\n",
    "                           'melt_season_start_date': melt_start_dates})\n",
    "        df['RGIId'] = rgi_id\n",
    "        df.dropna(inplace=True)\n",
    "        df = df.loc[df['PDD_cumsum_at_SLA_C'] > 0] # remove rows with 0 PDDs (to avoid dividing by 0)\n",
    "        df['days_since_melt_season_start_date'] = ((df['Date'] - df['melt_season_start_date']) / np.timedelta64(1, 'D')).astype(int)\n",
    "        \n",
    "        ### Calculate adjustment for modeled fsnow\n",
    "        df['fsnow_mod_adj'] = (df['SMB_at_SLA_obs_mwe'] \n",
    "                                / (df['PDD_cumsum_at_SLA_C'] \n",
    "                                    * df['days_since_melt_season_start_date']))\n",
    "                \n",
    "        ### Add adjustment to fsnow_mod\n",
    "        df['fsnow_obs'] = fsnow_mod_site + df['fsnow_mod_adj']\n",
    "        \n",
    "        # Remove unrealistic values\n",
    "        df.loc[df['fsnow_obs'] > 0.01, 'fsnow_obs'] = np.nan\n",
    "        df.loc[df['fsnow_obs'] <= 0.0, 'fsnow_obs'] = np.nan\n",
    "        \n",
    "        ### Save the median\n",
    "        fsnow_df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                                 'fsnow_obs_m/C/d': [np.nanmedian(df['fsnow_obs'])],\n",
    "                                 'fsnow_mod_m/C/d': [fsnow_mod_site]})\n",
    "        fsnow_merged = pd.concat([fsnow_merged, fsnow_df], axis=0)\n",
    "        \n",
    "        ### Plot an example\n",
    "        if rgi_id=='RGI60-01.00032':\n",
    "            plt.rcParams.update({'font.size': 12, 'font.sans-serif': 'Arial'})\n",
    "            fig, ax = plt.subplots(3, 1, figsize=(8,10))\n",
    "            ax[0].plot(era_ds.time.data, era_ds['temp_C'], '-k', linewidth=0.5)\n",
    "            ax[0].set_ylabel('Air temperature [$^{\\circ}$C]')\n",
    "            ax[0].grid()\n",
    "            cmap = matplotlib.colors.LinearSegmentedColormap.from_list('my_cmap', ['w', '#fb6a4a', '#67000d']) # white to red\n",
    "            era_ds['PDD_cumsum'].transpose().plot(cmap=cmap, ax=ax[1], \n",
    "                                                cbar_kwargs={'orientation': 'horizontal', \n",
    "                                                            'shrink': 0.5, \n",
    "                                                            'label': 'Cumulative PDD [$^{\\circ}$C]'})\n",
    "            ax[1].set_ylabel('Elevation [m]')\n",
    "            ax[1].plot(slas_site.time.values, slas_site['SLA_obs_m'].values, '*k', label='Snowline altitude')\n",
    "            # melt season start\n",
    "            for year in era_ds.year.data[1:]:\n",
    "                era_ds_year = era_ds.sel(time=slice(f\"{year}-01-01\", f\"{year}-08-01\"))\n",
    "                xmesh, ymesh = np.meshgrid(era_ds_year.time.data, era_ds.h.data)\n",
    "                ax[1].contour(xmesh, ymesh, era_ds_year['PDD_cumsum'].data.transpose(), levels=[0], colors=['gray'])\n",
    "            ax[1].plot(pd.Timestamp('2010-01-01'), 0, '-', color='gray', label='Melt season start')\n",
    "                        \n",
    "            ax[1].legend(loc='upper left')\n",
    "            ax[1].set_xlabel('')\n",
    "            ax[2].plot(df['Date'], df['fsnow_obs'] * 1e3, '.k')\n",
    "            ax[2].axhline(fsnow_merged['fsnow_mod_m/C/d'].values[0] * 1e3, color='k', linestyle='--', label='Modeled')\n",
    "            ax[2].axhline(fsnow_merged['fsnow_obs_m/C/d'].values[0] * 1e3, color='k', label='Observed median')\n",
    "            ax[2].legend(loc='lower left')\n",
    "            ax[2].set_ylabel('$f_{snow}$ [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "            ax[2].grid()\n",
    "            ax[2].set_ylim(2, 5)\n",
    "            labels = ['a', 'b', 'c']\n",
    "            for i, axis in enumerate(ax):\n",
    "                axis.set_xlim(np.datetime64('2013-01-01'), np.datetime64('2023-01-01'))\n",
    "                axis.text(0.97, 0.85, labels[i], transform=axis.transAxes, fontweight='bold', fontsize=16)\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            # save figure\n",
    "            fig_fn = os.path.join(code_path, 'figures', 'figS5_melt_factors_example.png')\n",
    "            fig.savefig(fig_fn)\n",
    "            print('Figure saved to file:', fig_fn)\n",
    "        \n",
    "    # Save results to file\n",
    "    fsnow_merged.to_csv(fsnow_merged_fn, index=False)\n",
    "    print('Observed fsnow saved to file:', fsnow_merged_fn)  \n",
    "    \n",
    "else:\n",
    "    fsnow_merged = pd.read_csv(fsnow_merged_fn)\n",
    "    print('Observed fsnow loaded from file')  \n",
    "\n",
    "plt.hist(fsnow_merged['fsnow_obs_m/C/d'] * 1e3, bins=100)\n",
    "plt.xlabel('Melt factor of snow [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea73e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot differences between modeled and observed fsnow\n",
    "import seaborn as sns\n",
    "fsnow_merged['fsnow_mod-fsnow_obs_m/C/d'] = fsnow_merged['fsnow_mod_m/C/d'] - fsnow_merged['fsnow_obs_m/C/d']\n",
    "fsnow_merged['fsnow_mod-fsnow_obs_mm/C/d'] = fsnow_merged['fsnow_mod-fsnow_obs_m/C/d'] * 1e3\n",
    "\n",
    "clusters_fn = os.path.join(out_path, 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "\n",
    "fsnow_merged = pd.merge(fsnow_merged, clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "\n",
    "sns.boxplot(fsnow_merged, x='fsnow_obs_m/C/d', hue='clustName')\n",
    "plt.show()\n",
    "sns.kdeplot(fsnow_merged, x='fsnow_mod-fsnow_obs_mm/C/d', hue='clustName') #, clip=(-0.5, 0.5))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be1a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gsca_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
