{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54fee1d3-89a0-4332-91af-77fbee8cc9ed",
   "metadata": {},
   "source": [
    "# Calculate median weekly trends in snowlines for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ded5b9-6a41-4e78-aa7d-dbec024f6fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from scipy.stats import iqr\n",
    "from shapely import wkt\n",
    "import seaborn as sns\n",
    "import contextily as ctx\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4325a8-0c88-45c3-b6a7-5fd875a8205f",
   "metadata": {},
   "source": [
    "## Define paths in directory, import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c89c723-245e-4769-8633-a503980f94e0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1d398a-23a0-4431-aee8-5fe622f39b68",
   "metadata": {},
   "source": [
    "## Load compiled glacier boundaries (AOIs) and snowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb97356-a7e0-4c83-af84-ceee07b044fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load AOIs\n",
    "aois_fn = os.path.join(scm_path, 'compiled_data', 'all_aois_climate_cluster.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "print('All glacier boundaries loaded from file.')\n",
    "\n",
    "# -----Load compiled snow cover stats\n",
    "scs_fn = os.path.join(scm_path, 'compiled_data', 'all_snow_cover_stats.csv')\n",
    "scs = pd.read_csv(scs_fn)\n",
    "print('All snow cover stats loaded from file.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dada9ced-080b-493c-bdaf-8ac48a51bfb1",
   "metadata": {},
   "source": [
    "## Calculate weekly median trends for each site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f23ff0d-5260-47ad-86d0-9a053e716cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scs_medians_fn = os.path.join(scm_path, 'results', 'all_snow_cover_stats_weekly_median_trends.csv') \n",
    "if not os.path.exists(scs_medians_fn):\n",
    "    # add week-of-year (WOY) column to snow cover stats\n",
    "    scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "    scs['WOY'] = scs['datetime'].dt.isocalendar().week\n",
    "    # determine columns to calculate weekly stats\n",
    "    columns = ['AAR', 'snowline_elevs_median_m', 'SCA_m2', 'ELA_from_AAR_m']\n",
    "    scs_medians = pd.DataFrame()\n",
    "    for rgi_id in tqdm(scs['RGIId'].drop_duplicates().values):\n",
    "        # subset snow cover stats to site\n",
    "        scs_site = scs.loc[scs['RGIId']==rgi_id]\n",
    "        # calculate weekly quartile trends\n",
    "        q1 = scs_site[['WOY'] + columns].groupby(by='WOY').quantile(0.25)\n",
    "        q1.columns = [x + '_P25' for x in q1.columns]\n",
    "        q2 = scs_site[['WOY'] + columns].groupby(by='WOY').quantile(0.5)\n",
    "        q2.columns = [x + '_P50' for x in q2.columns]\n",
    "        q3 = scs_site[['WOY'] + columns].groupby(by='WOY').quantile(0.75)\n",
    "        q3.columns = [x + '_P75' for x in q3.columns]\n",
    "        qs = pd.merge(q1, pd.merge(q2, q3, on='WOY'), on='WOY')\n",
    "        qs = qs.reindex(sorted(qs.columns), axis=1)\n",
    "        qs['WOY'] = qs.index\n",
    "        qs['RGIId'] = rgi_id\n",
    "        # concatenate to medians dataframe\n",
    "        scs_medians = pd.concat([scs_medians, qs])\n",
    "    # save to file\n",
    "    scs_medians.to_csv(scs_medians_fn, index=False)\n",
    "    print('Median weekly snow trends saved to file: ', scs_medians_fn)\n",
    "        \n",
    "else:\n",
    "    scs_medians = pd.read_csv(scs_medians_fn)\n",
    "    print('Median weekly snow cover trends loaded from file.')\n",
    "    \n",
    "scs_medians\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec25ef-248b-4fff-8a5e-b7ac70f2827e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Compile RGI characteristics and minimum snow cover median statistics\n",
    "min_snow_cover_stats_fn = os.path.join(scm_path, 'results', 'min_snow_cover_stats.csv') \n",
    "# check if exists in directory\n",
    "if not os.path.exists(min_snow_cover_stats_fn):\n",
    "    # initialize dataframe for RGI stats and minimum snow cover statts\n",
    "    min_snow_cover_stats = pd.DataFrame()\n",
    "    \n",
    "    # iterate over site names in median snow cover stats dataframe\n",
    "    for rgi_id in tqdm(sorted(scs_medians['RGIId'].drop_duplicates().values)):\n",
    "        # grab AOI for site\n",
    "        aoi_site = aois.loc[aois['RGIId']==rgi_id, :]\n",
    "        # grab median snowline stats for site\n",
    "        scs_medians_site = scs_medians.loc[scs_medians['RGIId']==rgi_id, :]\n",
    "        # calculate min median stats\n",
    "        median_columns = [x for x in scs_medians.columns if 'P50' in x]\n",
    "        for column in median_columns:\n",
    "            if (column=='ELA_from_AAR_m_P50') or (column=='snowline_elevs_median_m_P50'):\n",
    "                aoi_site[column+'_max'] = scs_medians_site[column].max()\n",
    "            else:\n",
    "                aoi_site[column+'_min'] = scs_medians_site[column].min()\n",
    "        # concatenate to full dataframe\n",
    "        min_snow_cover_stats = pd.concat([min_snow_cover_stats, aoi_site])\n",
    "\n",
    "    # add subregion names and colors\n",
    "    min_snow_cover_stats[['Subregion', 'color']] = '', ''\n",
    "    min_snow_cover_stats[['O1Region', 'O2Region']] = min_snow_cover_stats[['O1Region', 'O2Region']].astype(int)\n",
    "    for o1, o2 in min_snow_cover_stats[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "        min_snow_cover_stats.loc[(min_snow_cover_stats['O1Region']==o1) \n",
    "                                 & (min_snow_cover_stats['O2Region']==o2), ['Subregion', 'color']] = f.determine_subregion_name_color(o1, o2)\n",
    "    # save to file\n",
    "    min_snow_cover_stats.to_csv(min_snow_cover_stats_fn, index=False)\n",
    "    print('Minimum median snow cover stats saved to file: ', min_snow_cover_stats_fn)\n",
    "        \n",
    "else:\n",
    "    # load from file\n",
    "    min_snow_cover_stats = pd.read_csv(min_snow_cover_stats_fn)\n",
    "    min_snow_cover_stats['geometry'] = min_snow_cover_stats['geometry'].apply(wkt.loads)\n",
    "    print('Minimum median snow cover stats loaded from file.')\n",
    "\n",
    "# reformat as GeoDataFrame\n",
    "min_snow_cover_stats = gpd.GeoDataFrame(min_snow_cover_stats, crs='EPSG:4326')\n",
    "min_snow_cover_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f2ecd1-99bf-4f9c-90d7-b8e5138c2aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(10, 12))\n",
    "sns.histplot(data=min_snow_cover_stats, x='AAR_P50_min', multiple='stack', hue='Subregion', \n",
    "             bins=np.arange(0, 1, step=0.1), ax=ax[0])\n",
    "sns.histplot(data=min_snow_cover_stats, x='AAR_P50_min', multiple='stack', hue='clustName', \n",
    "             bins=np.arange(0, 1, step=0.1), ax=ax[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1781acfe-3e6d-46a1-8962-1086332f34d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print stats\n",
    "min_snow_cover_stats.groupby(by=['Subregion', 'clustName'])['AAR_P50_min'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9703d812-c2ad-482a-8518-f5ef4fda44a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
