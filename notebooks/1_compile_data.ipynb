{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87004723-8992-4bbe-8665-aad40b4e00f4",
   "metadata": {},
   "source": [
    "# Compile data: snowlines, ERA, AOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9dd20c4-6f54-476d-8b44-f3510e3d3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d87a4488-354e-43b2-93cd-e581173bf52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of study sites =  195\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['RGI60-01.00032',\n",
       " 'RGI60-01.00033',\n",
       " 'RGI60-01.00037',\n",
       " 'RGI60-01.00038',\n",
       " 'RGI60-01.00046',\n",
       " 'RGI60-01.00312',\n",
       " 'RGI60-01.00566',\n",
       " 'RGI60-01.00570',\n",
       " 'RGI60-01.00576',\n",
       " 'RGI60-01.00675',\n",
       " 'RGI60-01.01104',\n",
       " 'RGI60-01.01151',\n",
       " 'RGI60-01.01390',\n",
       " 'RGI60-01.01524',\n",
       " 'RGI60-01.01733',\n",
       " 'RGI60-01.03594',\n",
       " 'RGI60-01.03622',\n",
       " 'RGI60-01.03861',\n",
       " 'RGI60-01.04375',\n",
       " 'RGI60-01.04624',\n",
       " 'RGI60-01.06268',\n",
       " 'RGI60-01.06722',\n",
       " 'RGI60-01.08155',\n",
       " 'RGI60-01.08174',\n",
       " 'RGI60-01.08246',\n",
       " 'RGI60-01.08248',\n",
       " 'RGI60-01.08262',\n",
       " 'RGI60-01.08288',\n",
       " 'RGI60-01.08296',\n",
       " 'RGI60-01.08302',\n",
       " 'RGI60-01.08336',\n",
       " 'RGI60-01.08353',\n",
       " 'RGI60-01.08389',\n",
       " 'RGI60-01.08395',\n",
       " 'RGI60-01.08403',\n",
       " 'RGI60-01.08412',\n",
       " 'RGI60-01.08427',\n",
       " 'RGI60-01.09148',\n",
       " 'RGI60-01.09162',\n",
       " 'RGI60-01.09216',\n",
       " 'RGI60-01.09411',\n",
       " 'RGI60-01.09639',\n",
       " 'RGI60-01.10196',\n",
       " 'RGI60-01.10555',\n",
       " 'RGI60-01.10689',\n",
       " 'RGI60-01.10778',\n",
       " 'RGI60-01.10851',\n",
       " 'RGI60-01.10857',\n",
       " 'RGI60-01.11616',\n",
       " 'RGI60-01.11654',\n",
       " 'RGI60-01.11788',\n",
       " 'RGI60-01.12347',\n",
       " 'RGI60-01.12355',\n",
       " 'RGI60-01.12370',\n",
       " 'RGI60-01.12425',\n",
       " 'RGI60-01.12635',\n",
       " 'RGI60-01.13696',\n",
       " 'RGI60-01.14391',\n",
       " 'RGI60-01.14420',\n",
       " 'RGI60-01.14443',\n",
       " 'RGI60-01.14523',\n",
       " 'RGI60-01.14883',\n",
       " 'RGI60-01.15719',\n",
       " 'RGI60-01.15769',\n",
       " 'RGI60-01.15788',\n",
       " 'RGI60-01.15995',\n",
       " 'RGI60-01.16262',\n",
       " 'RGI60-01.16342',\n",
       " 'RGI60-01.16348',\n",
       " 'RGI60-01.16467',\n",
       " 'RGI60-01.16816',\n",
       " 'RGI60-01.16853',\n",
       " 'RGI60-01.17183',\n",
       " 'RGI60-01.17348',\n",
       " 'RGI60-01.17359',\n",
       " 'RGI60-01.17423',\n",
       " 'RGI60-01.17464',\n",
       " 'RGI60-01.17478',\n",
       " 'RGI60-01.17623',\n",
       " 'RGI60-01.17761',\n",
       " 'RGI60-01.17774',\n",
       " 'RGI60-01.17803',\n",
       " 'RGI60-01.17807',\n",
       " 'RGI60-01.19460',\n",
       " 'RGI60-01.19592',\n",
       " 'RGI60-01.19599',\n",
       " 'RGI60-01.19682',\n",
       " 'RGI60-01.19725',\n",
       " 'RGI60-01.19773',\n",
       " 'RGI60-01.19783',\n",
       " 'RGI60-01.19790',\n",
       " 'RGI60-01.19814',\n",
       " 'RGI60-01.19825',\n",
       " 'RGI60-01.20180',\n",
       " 'RGI60-01.20181',\n",
       " 'RGI60-01.20186',\n",
       " 'RGI60-01.20196',\n",
       " 'RGI60-01.20272',\n",
       " 'RGI60-01.20274',\n",
       " 'RGI60-01.20279',\n",
       " 'RGI60-01.20286',\n",
       " 'RGI60-01.20302',\n",
       " 'RGI60-01.20303',\n",
       " 'RGI60-01.20309',\n",
       " 'RGI60-01.20324',\n",
       " 'RGI60-01.20653',\n",
       " 'RGI60-01.20708',\n",
       " 'RGI60-01.20796',\n",
       " 'RGI60-01.20824',\n",
       " 'RGI60-01.20897',\n",
       " 'RGI60-01.20998',\n",
       " 'RGI60-01.21014',\n",
       " 'RGI60-01.21080',\n",
       " 'RGI60-01.22193',\n",
       " 'RGI60-01.22204',\n",
       " 'RGI60-01.22207',\n",
       " 'RGI60-01.22699',\n",
       " 'RGI60-01.23094',\n",
       " 'RGI60-01.23199',\n",
       " 'RGI60-01.23597',\n",
       " 'RGI60-01.23635',\n",
       " 'RGI60-01.23645',\n",
       " 'RGI60-01.23649',\n",
       " 'RGI60-01.23664',\n",
       " 'RGI60-01.26738',\n",
       " 'RGI60-01.26743',\n",
       " 'RGI60-01.27103',\n",
       " 'RGI60-02.00556',\n",
       " 'RGI60-02.01346',\n",
       " 'RGI60-02.01843',\n",
       " 'RGI60-02.02364',\n",
       " 'RGI60-02.02606',\n",
       " 'RGI60-02.03578',\n",
       " 'RGI60-02.03586',\n",
       " 'RGI60-02.03769',\n",
       " 'RGI60-02.04305',\n",
       " 'RGI60-02.04363',\n",
       " 'RGI60-02.04403',\n",
       " 'RGI60-02.04410',\n",
       " 'RGI60-02.05157',\n",
       " 'RGI60-02.05169',\n",
       " 'RGI60-02.06145',\n",
       " 'RGI60-02.06149',\n",
       " 'RGI60-02.06152',\n",
       " 'RGI60-02.06488',\n",
       " 'RGI60-02.06859',\n",
       " 'RGI60-02.06862',\n",
       " 'RGI60-02.06868',\n",
       " 'RGI60-02.06929',\n",
       " 'RGI60-02.07002',\n",
       " 'RGI60-02.07301',\n",
       " 'RGI60-02.09116',\n",
       " 'RGI60-02.09254',\n",
       " 'RGI60-02.09720',\n",
       " 'RGI60-02.09758',\n",
       " 'RGI60-02.10175',\n",
       " 'RGI60-02.10389',\n",
       " 'RGI60-02.12428',\n",
       " 'RGI60-02.12433',\n",
       " 'RGI60-02.12435',\n",
       " 'RGI60-02.12437',\n",
       " 'RGI60-02.12441',\n",
       " 'RGI60-02.12468',\n",
       " 'RGI60-02.12483',\n",
       " 'RGI60-02.12684',\n",
       " 'RGI60-02.12702',\n",
       " 'RGI60-02.12721',\n",
       " 'RGI60-02.12722',\n",
       " 'RGI60-02.12947',\n",
       " 'RGI60-02.12969',\n",
       " 'RGI60-02.13130',\n",
       " 'RGI60-02.13254',\n",
       " 'RGI60-02.13284',\n",
       " 'RGI60-02.13285',\n",
       " 'RGI60-02.13287',\n",
       " 'RGI60-02.13290',\n",
       " 'RGI60-02.13685',\n",
       " 'RGI60-02.14009',\n",
       " 'RGI60-02.14017',\n",
       " 'RGI60-02.14256',\n",
       " 'RGI60-02.14297',\n",
       " 'RGI60-02.16664',\n",
       " 'RGI60-02.16674',\n",
       " 'RGI60-02.16722',\n",
       " 'RGI60-02.16808',\n",
       " 'RGI60-02.16912',\n",
       " 'RGI60-02.16923',\n",
       " 'RGI60-02.17023',\n",
       " 'RGI60-02.17025',\n",
       " 'RGI60-02.17029',\n",
       " 'RGI60-02.17736',\n",
       " 'RGI60-02.17738',\n",
       " 'RGI60-02.17739',\n",
       " 'RGI60-02.17741',\n",
       " 'RGI60-02.18778']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -----Path to snow_cover_mapping\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "\n",
    "# -----Path to snow-cover-mapping-application/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "# -----Path to study-sites/\n",
    "study_sites_path = os.path.join(scm_path, 'study-sites')\n",
    "\n",
    "# -----Load study site names\n",
    "rgi_ids = [x for x in sorted(os.listdir(study_sites_path)) if 'RGI' in x]\n",
    "print('Number of study sites = ', len(rgi_ids))\n",
    "rgi_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d30ed4-37a3-4d1d-84ad-42112cc49d6b",
   "metadata": {},
   "source": [
    "## Load and compile snow cover stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b966de1-46d3-4b8e-8c77-9300f1df3c6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All snow cover stats loaded from file.\n",
      "Adding RGI60-01.22207 to snow cover stats...\n",
      "\n",
      "Number of sites with snow cover stats files: 194\n",
      "\n",
      "Sites without snow cover stats files: N=1 \n",
      "['RGI60-01.22207']\n"
     ]
    }
   ],
   "source": [
    "scs_path = os.path.join(scm_path, 'compiled_data')\n",
    "scs_fn = 'all_snow_cover_stats.csv'\n",
    "\n",
    "def load_site_sc_stats(site_name, study_sites_path):\n",
    "    sc_path = os.path.join(study_sites_path, site_name)\n",
    "    sc_fns = glob.glob(os.path.join(sc_path, '*_snow_cover_stats.csv'))\n",
    "    if len(sc_fns) > 0:\n",
    "        sc_fn = sc_fns[0]\n",
    "        sc = pd.read_csv(sc_fn)\n",
    "    else:\n",
    "        sc = 'N/A'\n",
    "    return sc\n",
    "    \n",
    "# check if snow cover stats path exists\n",
    "if not os.path.exists(scs_path):\n",
    "    os.mkdir(scs_path)\n",
    "# check if all snowlines CSV exists\n",
    "if not os.path.exists(os.path.join(scs_path, scs_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    scs = pd.DataFrame()\n",
    "    for rgi_id in tqdm(rgi_ids):\n",
    "        scs_site = load_site_sc_stats(rgi_id, study_sites_path)\n",
    "        if type(scs_site) != str:\n",
    "            scs = pd.concat([scs, scs_site])\n",
    "    scs.reset_index(drop=True, inplace=True)\n",
    "    # reduce memory storage\n",
    "    scs = f.reduce_memory_usage(scs)\n",
    "    scs.to_csv(os.path.join(scs_path, scs_fn), index=False)\n",
    "    print('All snow cover stats saved to file: ', os.path.join(scs_path, scs_fn))\n",
    "\n",
    "else:\n",
    "    # Load from file if it already exists\n",
    "    scs = pd.read_csv(os.path.join(scs_path, scs_fn))\n",
    "    scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "    print('All snow cover stats loaded from file.')\n",
    "    # Check if more sites need to be added to snow cover stats\n",
    "    rgi_ids_no_scs = [x for x in rgi_ids if x not in scs['RGIId'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for rgi_id in rgi_ids_no_scs:\n",
    "        print(f'Adding {rgi_id} to snow cover stats...')\n",
    "        scs_site = load_site_sc_stats(rgi_id, study_sites_path)\n",
    "        if type(scs_site) != str:\n",
    "            scs = pd.concat([scs, scs_site])\n",
    "    scs.reset_index(drop=True, inplace=True)\n",
    "    if updated:\n",
    "        # re-save snowlines to file\n",
    "        scs.to_csv(os.path.join(scs_path, scs_fn), index=False)\n",
    "        print('All snow cover stats saved to file: ', os.path.join(scs_path, scs_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        rgi_ids_no_scs = [x for x in rgi_ids if x not in \n",
    "                          scs['RGIId'].drop_duplicates().values]\n",
    "\n",
    "print('\\nNumber of sites with snow cover stats files:', len(scs['RGIId'].drop_duplicates()))\n",
    "print(f'\\nSites without snow cover stats files: N={len(rgi_ids_no_scs)} \\n{rgi_ids_no_scs}')\n",
    "# scs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf1426-7af0-4488-8a9e-6a15afa9543e",
   "metadata": {},
   "source": [
    "## Load and compile manually picked ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f85180-f966-4cf9-9c53-a77aea5881a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/compiled_data/'\n",
    "elas_fn = 'all_manual_ELA_picks.csv'\n",
    "\n",
    "# Check if compiled ELAs already exist in directory\n",
    "if os.path.exists(os.path.join(elas_path, elas_fn)):\n",
    "    elas = pd.read_csv(os.path.join(elas_path, elas_fn))\n",
    "    print('Manual ELA picks loaded from file.')\n",
    "else:\n",
    "    # Grab site IDs with ELAs\n",
    "    rgi_ids = sorted([x for x in os.listdir(data_path) if os.path.exists(os.path.join(data_path, x))])\n",
    "    print(f'Number of sites with manual ELA picks = {len(rgi_ids)}')\n",
    "    # Iterate over sites\n",
    "    elas = pd.DataFrame()\n",
    "    for rgi_id in tqdm(rgi_ids_elas):\n",
    "        ela_fn = os.path.join(data_path, rgi_id, f'{rgi_id}_ELAs_manual_picks.csv')\n",
    "        ela = pd.read_csv(ela_fn)\n",
    "        elas = pd.concat([elas, ela])\n",
    "    elas.reset_index(drop=True, inplace=True)\n",
    "    # Save to file\n",
    "    elas.to_csv(os.path.join(elas_path, elas_fn), index=False)\n",
    "    print('Manual ELA picks saved to file:', os.path.join(elas_path, elas_fn))\n",
    "    \n",
    "# elas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c9291-40fa-485a-858b-02ceb8870081",
   "metadata": {},
   "source": [
    "## Load and compile glacier boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951a825-d4d5-46f1-82e1-940092489340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aois_path = os.path.join(scm_path, 'all_AOIs')\n",
    "aois_fn = 'all_aois.shp'\n",
    "\n",
    "def load_site_aoi(site_name, study_sites_path):\n",
    "    aoi_path = os.path.join(study_sites_path, site_name, 'AOIs')\n",
    "    aoi_fns = glob.glob(os.path.join(aoi_path, '*RGI*.shp'))\n",
    "    if len(aoi_fns) > 0:\n",
    "        aoi_fn = aoi_fns[0]\n",
    "        aoi = gpd.read_file(aoi_fn)\n",
    "        aoi = aoi.to_crs('EPSG:4326')\n",
    "    else:\n",
    "        aoi = 'N/A'\n",
    "    return aoi\n",
    "    \n",
    "# check if aois path exists\n",
    "if not os.path.exists(aois_path):\n",
    "    os.mkdir(aois_path)\n",
    "# check if all aois shapefile exists\n",
    "if not os.path.exists(os.path.join(aois_path, aois_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    aois = gpd.GeoDataFrame()\n",
    "    for rgi_id in tqdm(rgi_ids):\n",
    "        aoi = load_site_aoi(rgi_id, study_sites_path)\n",
    "        if type(aoi) != str:\n",
    "            aois = pd.concat([aois, aoi])\n",
    "    aois.reset_index(drop=True, inplace=True)\n",
    "    aois.to_file(os.path.join(aois_path, aois_fn), index=False)\n",
    "    print('All glacier boundaries saved to file: ', os.path.join(aois_path, aois_fn))\n",
    "\n",
    "else:\n",
    "    # Load from file if it already exists\n",
    "    aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "    print('All glacier boundaries loaded from file.')\n",
    "    # Check if more sites need to be added to AOIs\n",
    "    rgi_ids_no_aois = [x for x in rgi_ids if x not in aois['RGIId'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for rgi_id in rgi_ids_no_aois:\n",
    "        print(f'Adding {rgi_id} to AOIs...')\n",
    "        aoi = load_site_aoi(rgi_id, study_sites_path)\n",
    "        if type(aoi) != str:\n",
    "            aois = pd.concat([aois, aoi])\n",
    "    if updated:\n",
    "        # re-save AOIs to file\n",
    "        aois.reset_index(drop=True, inplace=True)\n",
    "        aois.to_file(os.path.join(aois_path, aois_fn), index=False)\n",
    "        print('All glacier boundaries saved to file: ', os.path.join(aois_path, aois_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        rgi_ids_no_aois = [x for x in rgi_ids if x not in aois['RGIId'].drop_duplicates().values]\n",
    "\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('Number of sites with glacier boundaries = ', len(aois['RGIId'].drop_duplicates()))\n",
    "print(f'\\nSites without glacier boundaries: N={len(rgi_ids_no_aois)} \\n{rgi_ids_no_aois}')\n",
    "# aois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865080db-941f-45e6-8bb3-33c59bf8e3b8",
   "metadata": {},
   "source": [
    "## Load and compile ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d48c6-7ee5-4d1e-95cc-bb9029cef95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras_path = os.path.join(scm_path, 'all_ERA_data')\n",
    "eras_fn = 'all_era_data.csv'\n",
    "\n",
    "def load_site_era_data(site_name, study_sites_path):\n",
    "    era_path = os.path.join(study_sites_path, site_name, 'ERA')\n",
    "    era_fns = glob.glob(os.path.join(era_path, '*ERA*.csv'))\n",
    "    if len(era_fns) > 0:\n",
    "        era_fn = era_fns[0]\n",
    "        era = pd.read_csv(era_fn)\n",
    "        era['site_name'] = site_name\n",
    "    else:\n",
    "        era = 'N/A'\n",
    "    return era\n",
    "    \n",
    "# Check if ERA path exists\n",
    "if not os.path.exists(eras_path):\n",
    "    os.mkdir(eras_path)\n",
    "# Check if ERA CSV exists\n",
    "if not os.path.exists(os.path.join(eras_path, eras_fn)):\n",
    "    # Compile all ERA data\n",
    "    eras = pd.DataFrame()\n",
    "    rgi_ids = [os.path.basename(x) for x in sorted(glob.glob(os.path.join(scm_path, 'study-sites', 'RGI*')))]\n",
    "    for rgi_id in tqdm(rgi_ids):\n",
    "        era_site = load_site_era_data(rgi_id, study_sites_path)\n",
    "        if type(era_site) != str:\n",
    "            eras = pd.concat([eras, era_site])\n",
    "    eras.reset_index(drop=True, inplace=True)\n",
    "    eras.to_csv(os.path.join(eras_path, eras_fn), index=False)\n",
    "    print('All ERA data saved to file: ', os.path.join(eras_path, eras_fn))\n",
    "    site_names_no_era = [x for x in site_names if x not in eras['site_name'].drop_duplicates().values]\n",
    "\n",
    "else:\n",
    "    # Coad from file if it already exists\n",
    "    eras = pd.read_csv(os.path.join(eras_path, eras_fn))\n",
    "    print('All ERA data loaded from file.')\n",
    "    # Check if more sites need to be added to ERAs\n",
    "    rgi_ids = [os.path.basename(x) for x in sorted(glob.glob(os.path.join(scm_path, 'study-sites', 'RGI*')))]\n",
    "    rgi_ids_no_era = [x for x in rgi_ids if x not in eras['RGIId'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for rgi_id in rgi_ids_no_era:\n",
    "        print(f'Adding {rgi_id} to ERAs...')\n",
    "        era_site = load_site_era_data(rgi_id, study_sites_path)\n",
    "        if type(era_site) != str:\n",
    "            eras = pd.concat([eras, era_site])\n",
    "    if updated:\n",
    "        # re-save ERAs to file\n",
    "        eras.reset_index(drop=True, inplace=True)\n",
    "        eras.to_csv(os.path.join(eras_path, eras_fn), index=False)\n",
    "        print('All ERA data saved to file: ', os.path.join(eras_path, eras_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        site_names_no_era = [x for x in site_names if x not in eras['RGIId'].drop_duplicates().values]\n",
    "\n",
    "print('Number of sites with ERA = ', len(eras['site_name'].drop_duplicates()))\n",
    "print(f'\\nSites without ERA: N={len(site_names_no_era)} \\n{site_names_no_era}')\n",
    "# eras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf702914-6e8b-40e6-b438-048bb638631c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
