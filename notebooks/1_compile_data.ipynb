{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87004723-8992-4bbe-8665-aad40b4e00f4",
   "metadata": {},
   "source": [
    "# Compile data: snowlines, ERA, AOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd20c4-6f54-476d-8b44-f3510e3d3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a4488-354e-43b2-93cd-e581173bf52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Path to data\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "\n",
    "# -----Path to study-sites/\n",
    "study_sites_path = os.path.join(scm_path, 'study-sites')\n",
    "\n",
    "# -----Load study site names\n",
    "site_names = [x for x in sorted(os.listdir(study_sites_path)) if 'RGI' in x]\n",
    "print('Number of study sites = ', len(site_names))\n",
    "site_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d30ed4-37a3-4d1d-84ad-42112cc49d6b",
   "metadata": {},
   "source": [
    "## Load and compile snowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b966de1-46d3-4b8e-8c77-9300f1df3c6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "snowlines_path = os.path.join(scm_path, 'all_snowlines')\n",
    "snowlines_fn = 'all_snowlines.csv'\n",
    "\n",
    "def load_site_snowlines(site_name, study_sites_path):\n",
    "    snowline_path = os.path.join(study_sites_path, site_name)\n",
    "    snowline_fns = glob.glob(os.path.join(snowline_path, '*_snowlines.csv'))\n",
    "    if len(snowline_fns) > 0:\n",
    "        snowline_fn = snowline_fns[0]\n",
    "        snowline = pd.read_csv(snowline_fn)\n",
    "    else:\n",
    "        snowline = 'N/A'\n",
    "    return snowline\n",
    "    \n",
    "# check if snowlines path exists\n",
    "if not os.path.exists(snowlines_path):\n",
    "    os.mkdir(snowlines_path)\n",
    "# check if all snowlines CSV exists\n",
    "if not os.path.exists(os.path.join(snowlines_path, snowlines_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    snowlines = pd.DataFrame()\n",
    "    for site_name in tqdm(site_names):\n",
    "        snowlines_site = load_site_snowlines(site_name, study_sites_path)\n",
    "        if type(snowlines_site) != str:\n",
    "            snowlines = pd.concat([snowlines, snowline])\n",
    "    snowlines.reset_index(drop=True, inplace=True)\n",
    "    snowlines.to_csv(os.path.join(snowlines_path, snowlines_fn), index=False)\n",
    "    print('All snowlines saved to file: ', os.path.join(snowlines_path, snowlines_fn))\n",
    "\n",
    "else:\n",
    "    # Load from file if it already exists\n",
    "    snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "    snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "    print('All snowlines loaded from file.')\n",
    "    # Check if more sites need to be added to snowlines\n",
    "    site_names_no_snowlines = [x for x in site_names if x not in snowlines['site_name'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for site_name in site_names_no_snowlines:\n",
    "        print(f'Adding {site_name} to snowlines...')\n",
    "        snowlines_site = load_site_snowlines(site_name, study_sites_path)\n",
    "        if type(snowlines_site) != str:\n",
    "            snowlines = pd.concat([snowlines, snowline])\n",
    "    snowlines.reset_index(drop=True, inplace=True)\n",
    "    if updated:\n",
    "        # re-save snowlines to file\n",
    "        snowlines.to_csv(os.path.join(snowlines_path, snowlines_fn), index=False)\n",
    "        print('All snowlines saved to file: ', os.path.join(snowlines_path, snowlines_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        site_names_no_snowlines = [x for x in site_names if x not in \n",
    "                                   snowlines['site_name'].drop_duplicates().values]\n",
    "\n",
    "print('\\nNumber of sites with snowlines files:', len(snowlines['site_name'].drop_duplicates()))\n",
    "print(f'\\nSites without snowline files: N={len(site_names_no_snowlines)} \\n{site_names_no_snowlines}')\n",
    "# snowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c9291-40fa-485a-858b-02ceb8870081",
   "metadata": {},
   "source": [
    "## Load and compile glacier boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951a825-d4d5-46f1-82e1-940092489340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aois_path = os.path.join(scm_path, 'all_AOIs')\n",
    "aois_fn = 'all_aois.shp'\n",
    "\n",
    "def load_site_aoi(site_name, study_sites_path):\n",
    "    aoi_path = os.path.join(study_sites_path, site_name, 'AOIs')\n",
    "    aoi_fns = glob.glob(os.path.join(aoi_path, '*RGI*.shp'))\n",
    "    if len(aoi_fns) > 0:\n",
    "        aoi_fn = aoi_fns[0]\n",
    "        aoi = gpd.read_file(aoi_fn)\n",
    "        aoi = aoi.to_crs('EPSG:4326')\n",
    "    else:\n",
    "        aoi = 'N/A'\n",
    "    return aoi\n",
    "    \n",
    "# check if aois path exists\n",
    "if not os.path.exists(aois_path):\n",
    "    os.mkdir(aois_path)\n",
    "# check if all aois shapefile exists\n",
    "if not os.path.exists(os.path.join(aois_path, aois_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    aois = gpd.GeoDataFrame()\n",
    "    for site_name in tqdm(site_names):\n",
    "        aoi = load_site_aoi(site_name, study_sites_path)\n",
    "        if type(aoi) != str:\n",
    "            aois = pd.concat([aois, aoi])\n",
    "    aois.reset_index(drop=True, inplace=True)\n",
    "    aois.to_file(os.path.join(aois_path, aois_fn), index=False)\n",
    "    print('All glacier boundaries saved to file: ', os.path.join(aois_path, aois_fn))\n",
    "\n",
    "else:\n",
    "    # Load from file if it already exists\n",
    "    aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "    print('All glacier boundaries loaded from file.')\n",
    "    # Check if more sites need to be added to AOIs\n",
    "    site_names_no_aois = [x for x in site_names if x not in aois['RGIId'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for site_name in site_names_no_aois:\n",
    "        print(f'Adding {site_name} to AOIs...')\n",
    "        aoi = load_site_aoi(site_name, study_sites_path)\n",
    "        if type(aoi) != str:\n",
    "            aois = pd.concat([aois, aoi])\n",
    "    if updated:\n",
    "        # re-save AOIs to file\n",
    "        aois.reset_index(drop=True, inplace=True)\n",
    "        aois.to_file(os.path.join(aois_path, aois_fn), index=False)\n",
    "        print('All glacier boundaries saved to file: ', os.path.join(aois_path, aois_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        site_names_no_aois = [x for x in site_names if x not in aois['RGIId'].drop_duplicates().values]\n",
    "\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('Number of sites with glacier boundaries = ', len(aois['RGIId'].drop_duplicates()))\n",
    "print(f'\\nSites without glacier boundaries: N={len(site_names_no_aois)} \\n{site_names_no_aois}')\n",
    "# aois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865080db-941f-45e6-8bb3-33c59bf8e3b8",
   "metadata": {},
   "source": [
    "## Load and compile ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d48c6-7ee5-4d1e-95cc-bb9029cef95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras_path = os.path.join(scm_path, 'all_ERA_data')\n",
    "eras_fn = 'all_era_data.csv'\n",
    "\n",
    "def load_site_era_data(site_name, study_sites_path):\n",
    "    era_path = os.path.join(study_sites_path, site_name, 'ERA')\n",
    "    era_fns = glob.glob(os.path.join(era_path, '*ERA*.csv'))\n",
    "    if len(era_fns) > 0:\n",
    "        era_fn = era_fns[0]\n",
    "        era = pd.read_csv(era_fn)\n",
    "        era['site_name'] = site_name\n",
    "    else:\n",
    "        era = 'N/A'\n",
    "    return era\n",
    "    \n",
    "# Check if ERA path exists\n",
    "if not os.path.exists(eras_path):\n",
    "    os.mkdir(eras_path)\n",
    "# Check if ERA CSV exists\n",
    "if not os.path.exists(os.path.join(eras_path, eras_fn)):\n",
    "    # Compile all ERA data\n",
    "    eras = pd.DataFrame()\n",
    "    site_names = [os.path.basename(x) for x in sorted(glob.glob(os.path.join(scm_path, 'study-sites', 'RGI*')))]\n",
    "    for site_name in tqdm(site_names):\n",
    "        era_site = load_site_era_data(site_name, study_sites_path)\n",
    "        if type(era_site) != str:\n",
    "            eras = pd.concat([eras, era_site])\n",
    "    eras.reset_index(drop=True, inplace=True)\n",
    "    eras.to_csv(os.path.join(eras_path, eras_fn), index=False)\n",
    "    print('All ERA data saved to file: ', os.path.join(eras_path, eras_fn))\n",
    "    site_names_no_era = [x for x in site_names if x not in eras['site_name'].drop_duplicates().values]\n",
    "\n",
    "else:\n",
    "    # Coad from file if it already exists\n",
    "    eras = pd.read_csv(os.path.join(eras_path, eras_fn))\n",
    "    print('All ERA data loaded from file.')\n",
    "    # Check if more sites need to be added to ERAs\n",
    "    site_names_no_era = [x for x in site_names if x not in eras['site_name'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for site_name in site_names_no_era[1:]:\n",
    "        print(f'Adding {site_name} to ERAs...')\n",
    "        era_site = load_site_era_data(site_name, study_sites_path)\n",
    "        if type(era_site) != str:\n",
    "            eras = pd.concat([eras, era_site])\n",
    "    if updated:\n",
    "        # re-save ERAs to file\n",
    "        eras.reset_index(drop=True, inplace=True)\n",
    "        eras.to_csv(os.path.join(eras_path, eras_fn), index=False)\n",
    "        print('All ERA data saved to file: ', os.path.join(eras_path, eras_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        site_names_no_era = [x for x in site_names if x not in eras['site_name'].drop_duplicates().values]\n",
    "\n",
    "print('Number of sites with ERA = ', len(eras['site_name'].drop_duplicates()))\n",
    "print(f'\\nSites without ERA: N={len(site_names_no_era)} \\n{site_names_no_era}')\n",
    "# eras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf702914-6e8b-40e6-b438-048bb638631c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
