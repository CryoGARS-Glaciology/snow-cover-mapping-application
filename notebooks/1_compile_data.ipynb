{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87004723-8992-4bbe-8665-aad40b4e00f4",
   "metadata": {},
   "source": [
    "# Compile data: snowlines, ERA, AOIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd20c4-6f54-476d-8b44-f3510e3d3914",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87a4488-354e-43b2-93cd-e581173bf52f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Path to snow_cover_mapping\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "\n",
    "# -----Path to snow-cover-mapping-application/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "# -----Path to study-sites/\n",
    "study_sites_path = os.path.join(scm_path, 'study-sites')\n",
    "\n",
    "# -----Load study site names\n",
    "rgi_ids = [x for x in sorted(os.listdir(study_sites_path)) if 'RGI' in x]\n",
    "print('Number of study sites = ', len(rgi_ids))\n",
    "rgi_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d30ed4-37a3-4d1d-84ad-42112cc49d6b",
   "metadata": {},
   "source": [
    "## Load and compile snow cover stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b966de1-46d3-4b8e-8c77-9300f1df3c6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scs_path = os.path.join(scm_path, 'compiled_data')\n",
    "scs_fn = 'all_snow_cover_stats.csv'\n",
    "\n",
    "def load_site_sc_stats(site_name, study_sites_path):\n",
    "    sc_path = os.path.join(study_sites_path, site_name)\n",
    "    sc_fns = glob.glob(os.path.join(sc_path, '*_snow_cover_stats.csv'))\n",
    "    if len(sc_fns) > 0:\n",
    "        sc_fn = sc_fns[0]\n",
    "        sc = pd.read_csv(sc_fn)\n",
    "    else:\n",
    "        sc = 'N/A'\n",
    "    return sc\n",
    "    \n",
    "# check if snow cover stats path exists\n",
    "if not os.path.exists(scs_path):\n",
    "    os.mkdir(scs_path)\n",
    "# check if all snowlines CSV exists\n",
    "if not os.path.exists(os.path.join(scs_path, scs_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    scs = pd.DataFrame()\n",
    "    for rgi_id in tqdm(rgi_ids):\n",
    "        scs_site = load_site_sc_stats(rgi_id, study_sites_path)\n",
    "        if type(scs_site) != str:\n",
    "            scs = pd.concat([scs, scs_site])\n",
    "    scs.reset_index(drop=True, inplace=True)\n",
    "    # reduce memory storage\n",
    "    scs = f.reduce_memory_usage(scs)\n",
    "    scs.to_csv(os.path.join(scs_path, scs_fn), index=False)\n",
    "    print('All snow cover stats saved to file: ', os.path.join(scs_path, scs_fn))\n",
    "\n",
    "else:\n",
    "    # Load from file if it already exists\n",
    "    scs = pd.read_csv(os.path.join(scs_path, scs_fn))\n",
    "    scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "    print('All snow cover stats loaded from file.')\n",
    "    # Check if more sites need to be added to snow cover stats\n",
    "    rgi_ids_no_scs = [x for x in rgi_ids if x not in scs['RGIId'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for rgi_id in rgi_ids_no_scs:\n",
    "        print(f'Adding {rgi_id} to snow cover stats...')\n",
    "        scs_site = load_site_sc_stats(rgi_id, study_sites_path)\n",
    "        if type(scs_site) != str:\n",
    "            scs = pd.concat([scs, scs_site])\n",
    "    scs.reset_index(drop=True, inplace=True)\n",
    "    if updated:\n",
    "        # re-save snowlines to file\n",
    "        scs.to_csv(os.path.join(scs_path, scs_fn), index=False)\n",
    "        print('All snow cover stats saved to file: ', os.path.join(scs_path, scs_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        rgi_ids_no_scs = [x for x in rgi_ids if x not in \n",
    "                          scs['RGIId'].drop_duplicates().values]\n",
    "\n",
    "print('\\nNumber of sites with snow cover stats files:', len(scs['RGIId'].drop_duplicates()))\n",
    "print(f'\\nSites without snow cover stats files: N={len(rgi_ids_no_scs)} \\n{rgi_ids_no_scs}')\n",
    "# scs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cf1426-7af0-4488-8a9e-6a15afa9543e",
   "metadata": {},
   "source": [
    "## Load and compile manually picked ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f85180-f966-4cf9-9c53-a77aea5881a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/compiled_data/'\n",
    "elas_fn = 'all_manual_ELA_picks.csv'\n",
    "\n",
    "# Check if compiled ELAs already exist in directory\n",
    "if os.path.exists(os.path.join(elas_path, elas_fn)):\n",
    "    elas = pd.read_csv(os.path.join(elas_path, elas_fn))\n",
    "    print('Manual ELA picks loaded from file.')\n",
    "else:\n",
    "    # Grab site IDs with ELAs\n",
    "    rgi_ids = sorted([x for x in os.listdir(data_path) if os.path.exists(os.path.join(data_path, x))])\n",
    "    print(f'Number of sites with manual ELA picks = {len(rgi_ids)}')\n",
    "    # Iterate over sites\n",
    "    elas = pd.DataFrame()\n",
    "    for rgi_id in tqdm(rgi_ids_elas):\n",
    "        ela_fn = os.path.join(data_path, rgi_id, f'{rgi_id}_ELAs_manual_picks.csv')\n",
    "        ela = pd.read_csv(ela_fn)\n",
    "        elas = pd.concat([elas, ela])\n",
    "    elas.reset_index(drop=True, inplace=True)\n",
    "    # Save to file\n",
    "    elas.to_csv(os.path.join(elas_path, elas_fn), index=False)\n",
    "    print('Manual ELA picks saved to file:', os.path.join(elas_path, elas_fn))\n",
    "    \n",
    "# elas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44c9291-40fa-485a-858b-02ceb8870081",
   "metadata": {},
   "source": [
    "## Load and compile glacier boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951a825-d4d5-46f1-82e1-940092489340",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "aois_path = os.path.join(scm_path, 'all_AOIs')\n",
    "aois_fn = 'all_aois.shp'\n",
    "\n",
    "def load_site_aoi(site_name, study_sites_path):\n",
    "    aoi_path = os.path.join(study_sites_path, site_name, 'AOIs')\n",
    "    aoi_fns = glob.glob(os.path.join(aoi_path, '*RGI*.shp'))\n",
    "    if len(aoi_fns) > 0:\n",
    "        aoi_fn = aoi_fns[0]\n",
    "        aoi = gpd.read_file(aoi_fn)\n",
    "        aoi = aoi.to_crs('EPSG:4326')\n",
    "    else:\n",
    "        aoi = 'N/A'\n",
    "    return aoi\n",
    "    \n",
    "# check if aois path exists\n",
    "if not os.path.exists(aois_path):\n",
    "    os.mkdir(aois_path)\n",
    "# check if all aois shapefile exists\n",
    "if not os.path.exists(os.path.join(aois_path, aois_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    aois = gpd.GeoDataFrame()\n",
    "    for rgi_id in tqdm(rgi_ids):\n",
    "        aoi = load_site_aoi(rgi_id, study_sites_path)\n",
    "        if type(aoi) != str:\n",
    "            aois = pd.concat([aois, aoi])\n",
    "    aois.reset_index(drop=True, inplace=True)\n",
    "    aois.to_file(os.path.join(aois_path, aois_fn), index=False)\n",
    "    print('All glacier boundaries saved to file: ', os.path.join(aois_path, aois_fn))\n",
    "\n",
    "else:\n",
    "    # Load from file if it already exists\n",
    "    aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "    print('All glacier boundaries loaded from file.')\n",
    "    # Check if more sites need to be added to AOIs\n",
    "    rgi_ids_no_aois = [x for x in rgi_ids if x not in aois['RGIId'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for rgi_id in rgi_ids_no_aois:\n",
    "        print(f'Adding {rgi_id} to AOIs...')\n",
    "        aoi = load_site_aoi(rgi_id, study_sites_path)\n",
    "        if type(aoi) != str:\n",
    "            aois = pd.concat([aois, aoi])\n",
    "    if updated:\n",
    "        # re-save AOIs to file\n",
    "        aois.reset_index(drop=True, inplace=True)\n",
    "        aois.to_file(os.path.join(aois_path, aois_fn), index=False)\n",
    "        print('All glacier boundaries saved to file: ', os.path.join(aois_path, aois_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        rgi_ids_no_aois = [x for x in rgi_ids if x not in aois['RGIId'].drop_duplicates().values]\n",
    "\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('Number of sites with glacier boundaries = ', len(aois['RGIId'].drop_duplicates()))\n",
    "print(f'\\nSites without glacier boundaries: N={len(rgi_ids_no_aois)} \\n{rgi_ids_no_aois}')\n",
    "# aois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865080db-941f-45e6-8bb3-33c59bf8e3b8",
   "metadata": {},
   "source": [
    "## Load and compile ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d48c6-7ee5-4d1e-95cc-bb9029cef95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eras_path = os.path.join(scm_path, 'all_ERA_data')\n",
    "eras_fn = 'all_era_data.csv'\n",
    "\n",
    "def load_site_era_data(site_name, study_sites_path):\n",
    "    era_path = os.path.join(study_sites_path, site_name, 'ERA')\n",
    "    era_fns = glob.glob(os.path.join(era_path, '*ERA*.csv'))\n",
    "    if len(era_fns) > 0:\n",
    "        era_fn = era_fns[0]\n",
    "        era = pd.read_csv(era_fn)\n",
    "        era['site_name'] = site_name\n",
    "    else:\n",
    "        era = 'N/A'\n",
    "    return era\n",
    "    \n",
    "# Check if ERA path exists\n",
    "if not os.path.exists(eras_path):\n",
    "    os.mkdir(eras_path)\n",
    "# Check if ERA CSV exists\n",
    "if not os.path.exists(os.path.join(eras_path, eras_fn)):\n",
    "    # Compile all ERA data\n",
    "    eras = pd.DataFrame()\n",
    "    rgi_ids = [os.path.basename(x) for x in sorted(glob.glob(os.path.join(scm_path, 'study-sites', 'RGI*')))]\n",
    "    for rgi_id in tqdm(rgi_ids):\n",
    "        era_site = load_site_era_data(rgi_id, study_sites_path)\n",
    "        if type(era_site) != str:\n",
    "            eras = pd.concat([eras, era_site])\n",
    "    eras.reset_index(drop=True, inplace=True)\n",
    "    eras.to_csv(os.path.join(eras_path, eras_fn), index=False)\n",
    "    print('All ERA data saved to file: ', os.path.join(eras_path, eras_fn))\n",
    "    site_names_no_era = [x for x in site_names if x not in eras['site_name'].drop_duplicates().values]\n",
    "\n",
    "else:\n",
    "    # Coad from file if it already exists\n",
    "    eras = pd.read_csv(os.path.join(eras_path, eras_fn))\n",
    "    print('All ERA data loaded from file.')\n",
    "    # Check if more sites need to be added to ERAs\n",
    "    rgi_ids = [os.path.basename(x) for x in sorted(glob.glob(os.path.join(scm_path, 'study-sites', 'RGI*')))]\n",
    "    rgi_ids_no_era = [x for x in rgi_ids if x not in eras['RGIId'].drop_duplicates().values]\n",
    "    updated = False\n",
    "    for rgi_id in rgi_ids_no_era:\n",
    "        print(f'Adding {rgi_id} to ERAs...')\n",
    "        era_site = load_site_era_data(rgi_id, study_sites_path)\n",
    "        if type(era_site) != str:\n",
    "            eras = pd.concat([eras, era_site])\n",
    "    if updated:\n",
    "        # re-save ERAs to file\n",
    "        eras.reset_index(drop=True, inplace=True)\n",
    "        eras.to_csv(os.path.join(eras_path, eras_fn), index=False)\n",
    "        print('All ERA data saved to file: ', os.path.join(eras_path, eras_fn))\n",
    "        # re-define site names with no snowlines\n",
    "        site_names_no_era = [x for x in site_names if x not in eras['RGIId'].drop_duplicates().values]\n",
    "\n",
    "print('Number of sites with ERA = ', len(eras['site_name'].drop_duplicates()))\n",
    "print(f'\\nSites without ERA: N={len(site_names_no_era)} \\n{site_names_no_era}')\n",
    "# eras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf702914-6e8b-40e6-b438-048bb638631c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
