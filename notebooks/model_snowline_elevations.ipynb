{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba288d3b",
   "metadata": {
    "id": "ba288d3b"
   },
   "source": [
    "# Test machine learning models for predicting median snowline elevations using terrain parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "174e5e9d-8380-4f15-8465-79c83e47932d",
   "metadata": {
    "id": "174e5e9d-8380-4f15-8465-79c83e47932d"
   },
   "source": [
    "Helpful links!\n",
    "\n",
    "- https://scikit-learn.org/stable/tutorial/index.html\n",
    "- https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "- https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "- https://medium.com/analytics-vidhya/how-to-use-google-colab-with-github-via-google-drive-68efb23a42d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47564b97",
   "metadata": {
    "id": "47564b97",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sys\n",
    "import joblib\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4ef34e",
   "metadata": {
    "id": "cc4ef34e"
   },
   "outputs": [],
   "source": [
    "# !pip install geopandas #Installation format for packages not included with Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a67472",
   "metadata": {
    "id": "98a67472",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If using Google Colab, mount Google Drive so you can access the files in this folder\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae962ad8",
   "metadata": {
    "id": "ae962ad8"
   },
   "outputs": [],
   "source": [
    "# Uncomment the line below to locate the snow-cover-mapping-application folder in your Drive using os.listdir()\n",
    "#os.listdir('drive/MyDrive/Research/PhD/snow_cover_mapping/snow_cover_mapping_application/snow-cover-mapping-application/')\n",
    "# os.listdir('/content/drive/MyDrive/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/terrain-parameters/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zHAWHwurQEw0",
   "metadata": {
    "id": "zHAWHwurQEw0"
   },
   "outputs": [],
   "source": [
    "# -----Define paths in directory\n",
    "# ALEXANDRA'S PATH TO SNOW-COVER-MAPPING-APPLICATION\n",
    "# base_path = '/content/drive/MyDrive/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/snow-cover-mapping-application'\n",
    "# RAINEY'S PATH TO SNOW-COVER-MAPPING-APPLICATION\n",
    "base_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/snow-cover-mapping-application/'\n",
    "\n",
    "# ALEXANDRA'S PATH TO DATA:\n",
    "# path_to_folder = ('/content/drive/MyDrive/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/terrain-parameters/')\n",
    "# Remote Sensing Project Path to Data:\n",
    "#path_to_folder = ('/content/drive/Shareddrives/GEOG361_Final_Project/')\n",
    "\n",
    "# RAINEY'S PATH TO DATA:\n",
    "path_to_folder = os.path.join(base_path, 'inputs-outputs')\n",
    "\n",
    "# -----Add path to functions and load\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc7842f-d90d-49ca-9115-9beb9ee811f6",
   "metadata": {
    "id": "5cc7842f-d90d-49ca-9115-9beb9ee811f6"
   },
   "source": [
    "## Load training data from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af71dbd",
   "metadata": {
    "id": "2af71dbd"
   },
   "outputs": [],
   "source": [
    "#fn = 'Totalsite_terrainparameters.csv'  # file name\n",
    "fn = ('ELA_training_data.csv')\n",
    "# fn = ('Totalsite_terrainparameters.csv') # <--This file doesn't have snow cover observations, just terrain parameters\n",
    "df = pd.read_csv(os.path.join(path_to_folder, fn))\n",
    "\n",
    "# format 'datetime' column as pandas.datetimes (no datetime column in terrain parameters CSVs)\n",
    "df['datetime'] = pd.to_datetime(df['datetime'], format='mixed')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e86209",
   "metadata": {
    "id": "87e86209"
   },
   "source": [
    "## Define which columns to use as X (predictive features) and y (output labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65f4bcf",
   "metadata": {
    "id": "a65f4bcf"
   },
   "outputs": [],
   "source": [
    "# Feel free to adjust and play around with this\n",
    "training_columns = ['Area', 'Zmed', 'Slope', 'Aspect', 'Lmax']\n",
    "training_columns_display = ['Area', 'Z$_{med}$', 'Slope', 'Aspect', 'L$_{max}$']\n",
    "label = ['AAR']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db3940d",
   "metadata": {
    "id": "5db3940d"
   },
   "source": [
    "## Define supervised machine learning models to test\n",
    "\n",
    "\n",
    "Feel free to use more or less! See the [SciKitLearn Classifier comparison page](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) for more models, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67152922-0bd3-4cdb-a30f-0e73be646c82",
   "metadata": {
    "id": "67152922-0bd3-4cdb-a30f-0e73be646c82"
   },
   "outputs": [],
   "source": [
    "# Classifier names\n",
    "model_names = [\n",
    "  \"Linear Regression\",\n",
    "    \"Random Forest Regression\",\n",
    "    \"Decision Tree Regression\",\n",
    "    \"Support Vector Regression\",\n",
    "    \"Gradient Boosting Regression\",\n",
    "    \"Ridge Regression\"\n",
    "\n",
    "]\n",
    "\n",
    "# models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    RandomForestRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    SVR(),\n",
    "    GradientBoostingRegressor(),\n",
    "    Ridge()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7c86b9-13f0-4168-b2da-101bfcaf310a",
   "metadata": {
    "id": "6f7c86b9-13f0-4168-b2da-101bfcaf310a"
   },
   "source": [
    "## Determine best (most accurate) model for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa263b4-be0b-461d-9dbd-fc685587abff",
   "metadata": {
    "id": "baa263b4-be0b-461d-9dbd-fc685587abff",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "out_path = os.path.join(base_path, 'inputs-outputs')\n",
    "best_model_fn = 'best_model_ELAs.joblib'\n",
    "save_performances = True\n",
    "performances_fn = 'ELA_model_performances.csv'\n",
    "best_model_retrained, X, y = f.determine_best_model(df, models, model_names, training_columns,\n",
    "                                                    label, out_path, best_model_fn, save_performances, performances_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7333dc-b741-46e1-b52a-25211b703e80",
   "metadata": {
    "id": "cd7333dc-b741-46e1-b52a-25211b703e80"
   },
   "source": [
    "## Assess perturbation feature importances for best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68ac85e2-8efe-4e01-8f48-e7f7c2af280e",
   "metadata": {
    "id": "68ac85e2-8efe-4e01-8f48-e7f7c2af280e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "figure_out_path = os.path.join(base_path, 'figures')\n",
    "figure_fn = 'best_model_ELAs_feature_importances.png'\n",
    "importances_fn = 'best_model_ELAs_feature_importances.csv'\n",
    "feature_importances = f.assess_model_feature_importances(best_model_retrained, X, y, training_columns, training_columns_display,\n",
    "                                                         out_path, importances_fn, figure_out_path, figure_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UV7_JpauBUAN",
   "metadata": {
    "id": "UV7_JpauBUAN"
   },
   "outputs": [],
   "source": [
    "\n",
    "# models = [\n",
    "#     LinearRegression(),\n",
    "#     RandomForestRegressor(),\n",
    "#     DecisionTreeRegressor(),\n",
    "#     SVR(),\n",
    "#     GradientBoostingRegressor(),\n",
    "#     Ridge()\n",
    "# ]\n",
    "\n",
    "# model_names = [\n",
    "#     \"Linear Regression\",\n",
    "#     \"Random Forest Regression\",\n",
    "#     \"Decision Tree Regression\",\n",
    "#     \"Support Vector Regression\",\n",
    "#     \"Gradient Boosting Regression\",\n",
    "#     \"Ridge Regression\"\n",
    "# ]\n",
    "\n",
    "# for name, model in zip(model_names, models):\n",
    "#     scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "#     mean_score = -scores.mean()\n",
    "#     print(f\"{name}: Mean Squared Error - {mean_score}\")\n",
    "\n",
    "# param_grid = [\n",
    "#     {},\n",
    "#     {'n_estimators': [50, 100, 200], 'max_depth': [None, 5, 10]},\n",
    "#     {'max_depth': [None, 5, 10]},\n",
    "#     {'kernel': ['linear', 'rbf'], 'C': [1, 10, 100]},\n",
    "#     {'n_estimators': [50, 100, 200], 'learning_rate': [0.1, 0.05, 0.01]},\n",
    "#     {'alpha': [0.1, 1.0, 10.0]}\n",
    "# ]\n",
    "\n",
    "# best_models = []\n",
    "# for name, model, params in zip(model_names, models, param_grid):\n",
    "#     grid_search = GridSearchCV(model, params, cv=5, scoring='neg_mean_squared_error')\n",
    "#     grid_search.fit(X, y)\n",
    "#     best_model = grid_search.best_estimator_\n",
    "#     best_models.append(best_model)\n",
    "#     print(f\"Best parameters for {name}: {grid_search.best_params_}\")\n",
    "\n",
    "# best_model_scores = []\n",
    "# for name, best_model in zip(model_names, best_models):\n",
    "#     scores = cross_val_score(best_model, X, y, cv=5, scoring='neg_mean_squared_error')\n",
    "#     mean_score = -scores.mean()\n",
    "#     best_model_scores.append(mean_score)\n",
    "#     print(f\"{name} with best parameters: Mean Squared Error - {mean_score}\")\n",
    "\n",
    "# best_model_index = best_model_scores.index(min(best_model_scores))\n",
    "# best_model_name = model_names[best_model_index]\n",
    "# print(f\"The best model is: {best_model_name} with MSE of {best_model_scores[best_model_index]}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zL0qHNtphPCC",
   "metadata": {
    "id": "zL0qHNtphPCC"
   },
   "outputs": [],
   "source": [
    "# -----Determine unique subregions and number of sites in each subregion\n",
    "df[['O1Region', 'O2Region']] = df[['O1Region', 'O2Region']].astype(float)\n",
    "df = df.sort_values(by=['O1Region', 'O2Region']).reset_index(drop=True)\n",
    "# grab unique site names with their O1 and O2 regions\n",
    "unique_sites = df[['site_name', 'O1Region', 'O2Region']].drop_duplicates()\n",
    "# count number of sites in each unique O1 and O2 region combination\n",
    "unique_subregion_counts = unique_sites[['O1Region', 'O2Region']].value_counts().reset_index(name='count')\n",
    "# sort by subregion number\n",
    "unique_subregion_counts = unique_subregion_counts.sort_values(by=['O1Region', 'O2Region'])\n",
    "# save just the unique subregion values\n",
    "unique_subregions = unique_subregion_counts[['O1Region', 'O2Region']].values\n",
    "# print unique subregions and site counts\n",
    "unique_subregion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ejL0kwcdvK0F",
   "metadata": {
    "id": "ejL0kwcdvK0F"
   },
   "outputs": [],
   "source": [
    "# -----Define a function for determining subregion name and color for plotting\n",
    "def determine_subregion_name_color(o1, o2):\n",
    "    if (o1 == 1.0) and (o2 == 1.0):\n",
    "        subregion_name, color = 'Brooks Range', 'c'\n",
    "    elif (o1 == 1.0) and (o2 == 2.0):\n",
    "        subregion_name, color = 'Alaska Range', '#1f78b4'\n",
    "    elif (o1 == 1.0) and (o2 == 3.0):\n",
    "        subregion_name, color = 'Aleutians', '#6d9c43'\n",
    "    elif (o1 == 1.0) and (o2 == 4.0):\n",
    "        subregion_name, color = 'W. Chugach Mtns.', '#264708'\n",
    "    elif (o1 == 1.0) and (o2 == 5.0):\n",
    "        subregion_name, color = 'St. Elias Mtns.', '#fb9a99'\n",
    "    elif (o1 == 1.0) and (o2 == 6.0):\n",
    "        subregion_name, color = 'N. Coast Ranges', '#e31a1c'\n",
    "    elif (o1 == 2.0) and (o2 == 1.0):\n",
    "        subregion_name, color = 'N. Rockies', '#cab2d6'\n",
    "    elif (o1 == 2.0) and (o2 == 2.0):\n",
    "        subregion_name, color = 'N. Cascades', '#fdbf6f'\n",
    "    elif (o1 == 2.0) and (o2 == 3.0):\n",
    "        subregion_name, color = 'C. Rockies', '#9657d9'\n",
    "    elif (o1 == 2.0) and (o2 == 4.0):\n",
    "        subregion_name, color = 'S. Cascades', '#ff7f00'\n",
    "    elif (o1 == 2.0) and (o2 == 5.0):\n",
    "        subregion_name, color = 'S. Rockies', '#6a3d9a'\n",
    "    else:\n",
    "        subregion_name = 'O1:' + o1 + ' O2:' + o2\n",
    "        color = 'k'\n",
    "\n",
    "    return subregion_name, color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VE8OLTbHgqlY",
   "metadata": {
    "id": "VE8OLTbHgqlY"
   },
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "training_columns_only_Zmed = ['Area','Zmed', 'Slope', 'Aspect']\n",
    "# -----Iterate over unique subregions (O1 and O2 region combinations)\n",
    "# initialize data table of feature importances\n",
    "feature_importances_df_full = pd.DataFrame()\n",
    "i = 0 # loop counter\n",
    "for o1, o2 in unique_subregions:\n",
    "\n",
    "    # grab subregion name and color for plotting\n",
    "    subregion_name, color = determine_subregion_name_color(o1, o2)\n",
    "    print(subregion_name)\n",
    "\n",
    "    # subset training data for subregion\n",
    "    df_subregion = df.loc[(df['O1Region']==o1) & (df['O2Region']==o2)]\n",
    "    # define X and y\n",
    "    X_subregion = df_subregion[training_columns_only_Zmed]\n",
    "    y_subregion = df_subregion[label]\n",
    "    # convert y to 1D row vector\n",
    "    y_subregion = np.transpose(y_subregion.values)[0]\n",
    "\n",
    "    # initialize empty array for storing mean absolute errors\n",
    "    abs_err_mean = np.zeros(len(names))\n",
    "\n",
    "    # iterate over ML classifiers\n",
    "    for j, name, classifier in list(zip(np.arange(0,len(names)), names, classifiers)):\n",
    "\n",
    "        # train model and estimate feature importances using permutation\n",
    "        classifier.fit(X_subregion, y_subregion)\n",
    "        perm = permutation_importance(classifier, X_subregion, y_subregion,\n",
    "                                      scoring='neg_mean_squared_error')\n",
    "        # get importances\n",
    "        feature_importances = perm.importances_mean\n",
    "        # normalize importances so that they sum to about 1\n",
    "        feature_importances = feature_importances / np.sum(feature_importances)\n",
    "\n",
    "        feature_importances_df = pd.DataFrame({'subregion': [subregion_name],\n",
    "                                               'color': [color],\n",
    "                                               'model': [name]\n",
    "                                               })\n",
    "        for column, importance in list(zip(training_columns_only_Zmed, feature_importances)):\n",
    "            feature_importances_df[column] = [importance]\n",
    "        # concatenate to full feature importances df\n",
    "        feature_importances_df_full = pd.concat([feature_importances_df_full, feature_importances_df])\n",
    "\n",
    "    i += 1 # increase loop counter\n",
    "\n",
    "feature_importances_df_full.reset_index(drop=True, inplace=True)\n",
    "feature_importances_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oaPESnj-sPJM",
   "metadata": {
    "id": "oaPESnj-sPJM"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# -----Plot results by column\n",
    "# construct dictionary of colors for each subregion\n",
    "color_dict = dict(feature_importances_df_full[['subregion', 'color']].drop_duplicates().values)\n",
    "fig, ax = plt.subplots(2, 2, figsize=(14, 8))\n",
    "ax = ax.flatten()\n",
    "for i, column in enumerate(training_columns_only_Zmed):\n",
    "    sns.boxplot(data=feature_importances_df_full, x='subregion', y=column,\n",
    "                hue='subregion', ax=ax[i], palette=color_dict, width=9)\n",
    "    ax[i].set_title(column)\n",
    "    ax[i].set_xlabel('')\n",
    "    ax[i].set_xticks([])\n",
    "    ax[i].set_xticklabels([])\n",
    "    ax[i].set_xlim(-5,13)\n",
    "    ax[i].set_ylim(-0.1,1.1)\n",
    "    if i!=3:\n",
    "        ax[i].get_legend().remove()\n",
    "    else:\n",
    "        ax[i].legend(loc='center right', bbox_to_anchor=[1.2, 1.0, 0.2, 0.2])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TfP4A4sKAhQ7",
   "metadata": {
    "id": "TfP4A4sKAhQ7"
   },
   "outputs": [],
   "source": [
    "# -----Plot results by subregion\n",
    "# fig, ax = plt.subplots(3, 3, figsize=(14, 8))\n",
    "# ax = ax.flatten()\n",
    "# for i, column in enumerate(unique_subregions):\n",
    "\n",
    "#     sns.boxplot(data=feature_importances_df_full, x='subregion', y=column,\n",
    "#                 hue='subregion', ax=ax[i], palette=color_dict, width=9)\n",
    "#     ax[i].set_title(column)\n",
    "#     ax[i].set_xlabel('')\n",
    "#     ax[i].set_xticks([])\n",
    "#     ax[i].set_xticklabels([])\n",
    "#     ax[i].set_xlim(-5,13)\n",
    "#     ax[i].set_ylim(-0.1,1.1)\n",
    "#     if i!=3:\n",
    "#         ax[i].get_legend().remove()\n",
    "#     else:\n",
    "#         ax[i].legend(loc='center right', bbox_to_anchor=[1.2, 1.0, 0.2, 0.2])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yNBFq2v64dm7",
   "metadata": {
    "id": "yNBFq2v64dm7"
   },
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770732ce",
   "metadata": {
    "id": "770732ce"
   },
   "outputs": [],
   "source": [
    "#How this works:\n",
    "\n",
    "#this code performs K-fold cross-validation for multiple classifiers, trains and evaluates the models on each fold, calculates the mean absolute error for each classifier, and displays the performance results.\n",
    "\n",
    "\n",
    "#abs_err = np.zeros(len(names)): This line creates a NumPy array of zeros with the length equal to the number of classifiers. This array will store the mean absolute error for each classifier.\n",
    "\n",
    "\n",
    "#for i, (name, clf) in enumerate(zip(names, classifiers)):: This loop iterates over the names and classifiers, which are provided in the names and classifiers variables. The enumerate function is used to get both the index (i) and the corresponding name and classifier.\n",
    "\n",
    "#num_folds = 10: This line sets the number of folds to 10.\n",
    "#kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1): This line creates an instance of the KFold class with the specified number of splits, enabling shuffling of the data, and setting a random state for reproducibility.\n",
    "#abs_err_folds = np.zeros(num_folds): This line creates a NumPy array of zeros with the length equal to the number of folds. This array will store the mean absolute error for each fold.\n",
    "#j = 0: This line initializes a counter variable j to keep track of the fold number.\n",
    "\n",
    "\n",
    "#for train_ix, test_ix in kfold.split(X):: This loop iterates over the indices generated by the KFold split on the feature data X. The train_ix and test_ix contain the indices for the training and testing data for each fold, respectively.\n",
    "#X_train, X_test = X.loc[train_ix], X.loc[test_ix]: This line splits the feature data X into training and testing sets based on the fold indices.\n",
    "#y_train, y_test = y[train_ix], y[test_ix]: This line splits the target variable y into training and testing sets based on the fold indices.\n",
    "#Fitting the model and predicting:\n",
    "\n",
    "#clf.fit(X_train, y_train): This line fits (trains) the classifier model (clf) using the training data.\n",
    "#y_pred = clf.predict(X_test): This line predicts the outputs for the testing data (X_test) using the trained model.\n",
    "\n",
    "#abs_err_folds[j] = np.nanmean(np.abs(y_test - y_pred)): This line calculates the mean absolute error for the current fold by taking the absolute difference between the true target values (y_test) and the predicted values (y_pred), and then computing the mean. The result is stored in the abs_err_folds array for the corresponding fold.\n",
    "#j += 1: This line increments the fold counter.\n",
    "\n",
    "#abs_err[i] = np.nanmean(abs_err_folds): This line computes the mean of the absolute errors across all folds and assigns it to the abs_err array at the corresponding index (i).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69994580",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 250
    },
    "executionInfo": {
     "elapsed": 1224,
     "status": "error",
     "timestamp": 1701899595794,
     "user": {
      "displayName": "Alexandra Friel",
      "userId": "09130180654457172534"
     },
     "user_tz": 420
    },
    "id": "69994580",
    "outputId": "bbc80e3e-cc32-4668-a993-30f3109dad7e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "training_columns = ['Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect']\n",
    "labels = 'AAR' #If you're using the training_data file, add an 's' after snowline in this line.\n",
    "\n",
    "X = df[training_columns]\n",
    "y = df[labels]\n",
    "\n",
    "clf = RandomForestRegressor()\n",
    "clf.fit(X, y)\n",
    "\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "sorted_indices = sorted(range(len(feature_importances)), key=lambda i: feature_importances[i], reverse=True)\n",
    "sorted_feature_importances = [feature_importances[i] for i in sorted_indices]\n",
    "sorted_feature_names = [training_columns[i] for i in sorted_indices]\n",
    "\n",
    "plt.bar(range(len(sorted_feature_importances)), sorted_feature_importances, color=['maroon', 'teal', 'orange', 'pink', 'cyan', 'purple'],edgecolor = 'black')\n",
    "plt.xticks(range(len(sorted_feature_importances)), sorted_feature_names, rotation='vertical')\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Importance')\n",
    "plt.title('Random Forest Regressor for Study Sites')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94162ea5",
   "metadata": {
    "id": "94162ea5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "training_columns = ['Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect']\n",
    "labels = 'AAR'\n",
    "\n",
    "X = df[training_columns]\n",
    "y = df[labels]\n",
    "\n",
    "clf = DecisionTreeRegressor()\n",
    "clf.fit(X, y)\n",
    "\n",
    "feature_importances = clf.feature_importances_\n",
    "\n",
    "sorted_indices = sorted(range(len(feature_importances)), key=lambda i: feature_importances[i], reverse=True)\n",
    "sorted_feature_importances = [feature_importances[i] for i in sorted_indices]\n",
    "sorted_feature_names = [training_columns[i] for i in sorted_indices]\n",
    "\n",
    "plt.bar(range(len(sorted_feature_importances)), sorted_feature_importances, color=['maroon', 'teal', 'orange', 'pink', 'cyan', 'purple'], edgecolor = 'black')\n",
    "plt.xticks(range(len(sorted_feature_importances)), sorted_feature_names, rotation='vertical')\n",
    "\n",
    "\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('importance')\n",
    "plt.title('Decision Tree Regressor for Study Sites')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71765d9",
   "metadata": {
    "id": "f71765d9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "labels = 'AAR'\n",
    "\n",
    "X = df[training_columns]\n",
    "y = df[labels]\n",
    "\n",
    "clf = Ridge()\n",
    "clf.fit(X, y)\n",
    "\n",
    "coefficients = np.abs(clf.coef_)\n",
    "\n",
    "sorted_indices = np.argsort(coefficients)[::-1]\n",
    "sorted_coefficients = coefficients[sorted_indices]\n",
    "sorted_feature_names = np.array(training_columns)[sorted_indices]\n",
    "\n",
    "plt.bar(range(len(sorted_coefficients)), sorted_coefficients, color=['maroon', 'teal', 'orange', 'pink', 'cyan', 'purple'], edgecolor = 'black')\n",
    "plt.xticks(range(len(sorted_coefficients)), sorted_feature_names, rotation='vertical')\n",
    "\n",
    "plt.xlabel('Terrain Parameters')\n",
    "plt.ylabel('Absolute values')\n",
    "plt.title('Ridge Regression for selected study sites')\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HPBWXd4miYQS",
   "metadata": {
    "id": "HPBWXd4miYQS"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thjL57f1U4Gh",
   "metadata": {
    "id": "thjL57f1U4Gh"
   },
   "outputs": [],
   "source": [
    "# Cross validation is the best way to assess accuracy:\n",
    "  # - Reduces overfitting: Machine may learn data too well, unable to generalize new data\n",
    "  # - Cross validation will split the data into multiple folds. Then,  then the avg accuracy of the model across all folds is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85542880",
   "metadata": {
    "id": "85542880"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "\n",
    "training_columns = ['Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect']\n",
    "labels = 'AAR'\n",
    "X = df[training_columns]\n",
    "y = df[labels]\n",
    "\n",
    "random_forest = RandomForestRegressor()\n",
    "decision_tree = DecisionTreeRegressor()\n",
    "ridge_regression = Ridge()\n",
    "\n",
    "random_forest.fit(X, y)\n",
    "decision_tree.fit(X, y)\n",
    "ridge_regression.fit(X, y)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "colors = {\n",
    "    'Area': 'maroon',\n",
    "    'Zmin': 'pink',\n",
    "    'Zmax': 'violet',\n",
    "    'Zmed': 'blue',\n",
    "    'Slope': 'red',\n",
    "    'Aspect': 'purple'\n",
    "}\n",
    "\n",
    "rf_feature_importances = random_forest.feature_importances_\n",
    "dt_feature_importances = decision_tree.feature_importances_\n",
    "ridge_feature_importances = np.abs(ridge_regression.coef_)\n",
    "\n",
    "sorted_rf_indices = np.argsort(rf_feature_importances)[::-1]\n",
    "sorted_dt_indices = np.argsort(dt_feature_importances)[::-1]\n",
    "sorted_ridge_indices = np.argsort(ridge_feature_importances)[::-1]\n",
    "\n",
    "sorted_rf_feature_importances = rf_feature_importances[sorted_rf_indices]\n",
    "sorted_dt_feature_importances = dt_feature_importances[sorted_dt_indices]\n",
    "sorted_ridge_feature_importances = ridge_feature_importances[sorted_ridge_indices]\n",
    "\n",
    "sorted_training_columns = np.array(training_columns)[sorted_rf_indices]\n",
    "\n",
    "axs[0].bar(range(len(sorted_rf_feature_importances)), sorted_rf_feature_importances, color=[colors[column] for column in sorted_training_columns], edgecolor = 'black')\n",
    "axs[0].set_xticks(range(len(sorted_rf_feature_importances)))\n",
    "axs[0].set_xticklabels(sorted_training_columns, rotation='vertical')\n",
    "axs[0].set_xlabel('Features')\n",
    "axs[0].set_ylabel('Importance')\n",
    "axs[0].set_title('Random Forest Regressor')\n",
    "\n",
    "axs[1].bar(range(len(sorted_dt_feature_importances)), sorted_dt_feature_importances, color=[colors[column] for column in sorted_training_columns], edgecolor = 'black')\n",
    "axs[1].set_xticks(range(len(sorted_dt_feature_importances)))\n",
    "axs[1].set_xticklabels(sorted_training_columns, rotation='vertical')\n",
    "axs[1].set_xlabel('Features')\n",
    "axs[1].set_ylabel('Importance')\n",
    "axs[1].set_title('Decision Tree Regressor')\n",
    "\n",
    "axs[2].bar(range(len(sorted_ridge_feature_importances)), sorted_ridge_feature_importances, color=[colors[column] for column in sorted_training_columns],edgecolor = 'black')\n",
    "axs[2].set_xticks(range(len(sorted_ridge_feature_importances)))\n",
    "axs[2].set_xticklabels(sorted_training_columns, rotation='vertical')\n",
    "axs[2].set_xlabel('Features')\n",
    "axs[2].set_ylabel('Importance')\n",
    "axs[2].set_title('Ridge Regression')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LMrqbrApjOqW",
   "metadata": {
    "id": "LMrqbrApjOqW"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182d170d",
   "metadata": {
    "id": "182d170d"
   },
   "outputs": [],
   "source": [
    "#the feature importances are sorted in descending order using np.argsort() with the [::-1] indexing to reverse the order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "875a48ab",
   "metadata": {
    "id": "875a48ab"
   },
   "outputs": [],
   "source": [
    "#Make sure that it sorts the name of the variables correctly. Assign colors to each feature in the data set, not just random ones.\n",
    "\n",
    "#note to self: 2nd subplot is different than figure plotted above, need to fix. (Decision tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae711206",
   "metadata": {
    "id": "ae711206"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import pandas as pd\n",
    "\n",
    "fn = 'O1_1_terrain_parameters - Sheet1.csv'  # file name\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable,\n",
    "\n",
    "]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42\n",
    "    )\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    # plot data\n",
    "\n",
    "\n",
    "# COLOR SCHEME BELOW:\n",
    "    cm = plt.cm.GnBu\n",
    "    cm_bright = ListedColormap([\"#16ab2f\", \"#0000FF\"])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "\n",
    "    # Plot training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=(cm_bright), edgecolors=\"k\")\n",
    "\n",
    "    # Plot testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    "    )\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "        )\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # Plot the testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(\n",
    "            x_max - 0.3,\n",
    "            y_min + 0.3,\n",
    "            (\"%.2f\" % score).lstrip(\"0\"),\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "legend_colors = [\"g\", \"b\"]\n",
    "legend_labels = [\"True\", \"False\"]\n",
    "handles = [plt.Line2D([], [], marker='o', color=color, label=label) for color, label in zip(legend_colors, legend_labels)]\n",
    "plt.legend(handles=handles, loc=\"center\", bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ijautopkCy7d",
   "metadata": {
    "id": "ijautopkCy7d"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.inspection import DecisionBoundaryDisplay\n",
    "import pandas as pd\n",
    "\n",
    "fn = ('ELA_training_data.csv')\n",
    "\n",
    "X = data[['Area', 'Zmin']].values\n",
    "y = data['target'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, random_state=42)\n",
    "\n",
    "X, y = make_classification(\n",
    "    n_features=2, n_redundant=0, n_informative=2, random_state=1, n_clusters_per_class=1\n",
    ")\n",
    "rng = np.random.RandomState(2)\n",
    "X += 2 * rng.uniform(size=X.shape)\n",
    "linearly_separable = (X, y)\n",
    "\n",
    "datasets = [\n",
    "    make_moons(noise=0.3, random_state=0),\n",
    "    make_circles(noise=0.2, factor=0.5, random_state=1),\n",
    "    linearly_separable,\n",
    "\n",
    "]\n",
    "\n",
    "figure = plt.figure(figsize=(27, 9))\n",
    "i = 1\n",
    "# iterate over datasets\n",
    "for ds_cnt, ds in enumerate(datasets):\n",
    "    # preprocess dataset, split into training and test part\n",
    "    X, y = ds\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.4, random_state=42\n",
    "    )\n",
    "\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "    # plot data\n",
    "\n",
    "\n",
    "# COLOR SCHEME BELOW:\n",
    "    cm = plt.cm.GnBu\n",
    "    cm_bright = ListedColormap([\"#16ab2f\", \"#0000FF\"])\n",
    "    ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "    if ds_cnt == 0:\n",
    "        ax.set_title(\"Input data\")\n",
    "\n",
    "    # Plot training points\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=(cm_bright), edgecolors=\"k\")\n",
    "\n",
    "    # Plot testing points\n",
    "    ax.scatter(\n",
    "        X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\"\n",
    "    )\n",
    "    ax.set_xlim(x_min, x_max)\n",
    "    ax.set_ylim(y_min, y_max)\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    i += 1\n",
    "\n",
    "    # iterate over classifiers\n",
    "    for name, clf in zip(names, classifiers):\n",
    "        ax = plt.subplot(len(datasets), len(classifiers) + 1, i)\n",
    "\n",
    "        clf = make_pipeline(StandardScaler(), clf)\n",
    "        clf.fit(X_train, y_train)\n",
    "        score = clf.score(X_test, y_test)\n",
    "        DecisionBoundaryDisplay.from_estimator(\n",
    "            clf, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "        )\n",
    "\n",
    "        # Plot the training points\n",
    "        ax.scatter(\n",
    "            X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors=\"k\"\n",
    "        )\n",
    "        # Plot the testing points\n",
    "        ax.scatter(\n",
    "            X_test[:, 0],\n",
    "            X_test[:, 1],\n",
    "            c=y_test,\n",
    "            cmap=cm_bright,\n",
    "            edgecolors=\"k\",\n",
    "            alpha=0.8,\n",
    "        )\n",
    "\n",
    "        ax.set_xlim(x_min, x_max)\n",
    "        ax.set_ylim(y_min, y_max)\n",
    "        ax.set_xticks(())\n",
    "        ax.set_yticks(())\n",
    "        if ds_cnt == 0:\n",
    "            ax.set_title(name)\n",
    "        ax.text(\n",
    "            x_max - 0.3,\n",
    "            y_min + 0.3,\n",
    "            (\"%.2f\" % score).lstrip(\"0\"),\n",
    "            size=15,\n",
    "            horizontalalignment=\"right\",\n",
    "        )\n",
    "        i += 1\n",
    "\n",
    "legend_colors = [\"g\", \"b\"]\n",
    "legend_labels = [\"True\", \"False\"]\n",
    "handles = [plt.Line2D([], [], marker='o', color=color, label=label) for color, label in zip(legend_colors, legend_labels)]\n",
    "plt.legend(handles=handles, loc=\"center\", bbox_to_anchor=(1.2, 0.5))\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zb6fXxmtrtV2",
   "metadata": {
    "id": "zb6fXxmtrtV2"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Extract features and labels\n",
    "X = df[['Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect']]\n",
    "y = df['Zmin']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define classification pipelines\n",
    "pipelines = [\n",
    "    make_pipeline(StandardScaler(), MLPClassifier()),\n",
    "    make_pipeline(StandardScaler(), KNeighborsClassifier()),\n",
    "    make_pipeline(StandardScaler(), SVC()),\n",
    "    make_pipeline(StandardScaler(), GaussianProcessClassifier(kernel=RBF())),\n",
    "    make_pipeline(StandardScaler(), DecisionTreeClassifier()),\n",
    "    make_pipeline(StandardScaler(), RandomForestClassifier()),\n",
    "    make_pipeline(StandardScaler(), AdaBoostClassifier()),\n",
    "    make_pipeline(StandardScaler(), GaussianNB()),\n",
    "    make_pipeline(StandardScaler(), QuadraticDiscriminantAnalysis()),\n",
    "]\n",
    "\n",
    "\n",
    "# Train and evaluate classifiers\n",
    "scores = []\n",
    "for pipeline in pipelines:\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    score = pipeline.score(X_test, y_test)\n",
    "    scores.append(score)\n",
    "    print(f\"Classifier: {pipeline[-1].__class__.__name__}\")\n",
    "    print(f\"Score: {score}\")\n",
    "\n",
    "\n",
    "# Plot decision boundaries\n",
    "for i, pipeline in enumerate(pipelines):\n",
    "    ax = plt.subplot(len(pipelines), 2, i + 1)\n",
    "    DecisionBoundaryDisplay.from_estimator(\n",
    "        pipeline, X, cmap=cm, alpha=0.8, ax=ax, eps=0.5\n",
    "    )\n",
    "    ax.set_title(pipeline[-1].__class__.__name__)\n",
    "# Visualize the results\n",
    "x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "\n",
    "\n",
    "# Plot data and decision boundaries\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=(cm_bright), edgecolors=\"k\")\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6, edgecolors=\"k\")\n",
    "ax.set_xlim(x_min, x_max)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "NOLMejJWrXoN",
   "metadata": {
    "id": "NOLMejJWrXoN"
   },
   "outputs": [],
   "source": [
    "#Test the reliability! Run allll cells first.\n",
    "\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    SVC(),\n",
    "    GradientBoostingClassifier(),\n",
    "    RidgeClassifier()\n",
    "]\n",
    "\n",
    "names = [\n",
    "    \"Logistic Regression\",\n",
    "    \"Random Forest Classifier\",\n",
    "    \"Decision Tree Classifier\",\n",
    "    \"Support Vector Classifier\",\n",
    "    \"Gradient Boosting Classifier\",\n",
    "    \"Ridge Classifier\"\n",
    "]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "cross_val_accuracies = []\n",
    "\n",
    "for classifier in classifiers:\n",
    "    cross_val_accuracy = cross_val_score(classifier, X_train, y_train, cv=5, scoring='accuracy').mean()*100\n",
    "    cross_val_accuracies.append(cross_val_accuracy)\n",
    "\n",
    "    #Test size = percentage of each fold\n",
    "\n",
    "print(\"Cross-validated accuracy scores of each classifier: \")\n",
    "print(\" \")\n",
    "for i in range(len(classifiers)):\n",
    "    print(f\"{names[i]}: {cross_val_accuracies[i]}\", \"%\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dd080b",
   "metadata": {
    "id": "e9dd080b"
   },
   "outputs": [],
   "source": [
    "#Output is weird, need to change it!! SOS\n",
    "#Run all cells first!\n",
    "\n",
    "\n",
    "# Modified the for loop. (# Iterate over classifiers: for i, (name, clf) in enumerate(zip(names, classifiers))\n",
    "\n",
    "#this code performs K-fold cross-validation for multiple classifiers, trains and evaluates the models on each fold, calculates the mean absolute error for each classifier, and displays the performance results.\n",
    "\n",
    "#k fold will split the data set into equal(ish) parts, then it will be trained and evaluated a certain amount of times.\n",
    "\n",
    "# Initialize performance metrics\n",
    "abs_err = np.zeros(len(names)) # absolute error [m]\n",
    "\n",
    "# Iterate over classifiers\n",
    "for i, (name, clf) in enumerate(zip(names, classifiers)):\n",
    "    print(name)\n",
    "\n",
    "    # Conduct K-Fold cross-validation\n",
    "    num_folds = 10\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "    abs_err_folds = np.zeros(num_folds) # absolute error for all folds\n",
    "    j = 0 # fold counter\n",
    "\n",
    "    # loop through fold indices\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        # split data into training and testing using kfold indices\n",
    "        X_train, X_test = X[train_ix], X[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # fit model to X_train and y_train\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # predict outputs for X_test values\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # calculate performance metrics\n",
    "        abs_err_folds[j] = np.nanmean(np.abs(y_test - y_pred))\n",
    "        j += 1\n",
    "\n",
    "    # take average performance metrics for all folds\n",
    "    abs_err[i] = np.nanmean(abs_err_folds)\n",
    "\n",
    "    # display performance results\n",
    "    print('    Mean absolute error = ' + str(np.round(abs_err[i])) + ' m')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MQqnWgaOaVwq",
   "metadata": {
    "id": "MQqnWgaOaVwq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6210eb0b",
   "metadata": {
    "id": "6210eb0b"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8ce7b5",
   "metadata": {
    "id": "3b8ce7b5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200096f",
   "metadata": {
    "id": "e200096f"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
