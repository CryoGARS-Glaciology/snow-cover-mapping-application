{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fa19aea-f65a-4694-aa1b-1c1c24efbf58",
   "metadata": {},
   "source": [
    "# Estimate differences in modeled and remotely-sensed SMB\n",
    "\n",
    "1. Monthly snowline altitudes (SLAs)\n",
    "2. Equilibrium line altitudes (ELAs)\n",
    "3. Modeled surface mass balance (SMB) at the remotely-sensed snowline\n",
    "4. Degree-day factors of snow ($f_{snow}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd10b4fc-0d6f-45af-b2ac-9c92269f5ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "from scipy.stats import median_abs_deviation as MAD\n",
    "from tqdm.auto import tqdm\n",
    "import glob\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bde12fe-197d-41a7-8ec0-5b0d34956a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths for inputs and outputs\n",
    "scm_dir = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "model_dir = os.path.join(scm_dir, 'Rounce_et_al_2023')\n",
    "# Load glacier boundaries for RGI IDs\n",
    "aois_fn = os.path.join(scm_dir, 'analysis', 'all_aois.shp')\n",
    "aois = gpd.read_file(aois_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6cc2430",
   "metadata": {},
   "source": [
    "## 1. Monthly snowline altitudes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214d98cc",
   "metadata": {},
   "source": [
    "### Remotely-sensed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c287520",
   "metadata": {},
   "outputs": [],
   "source": [
    "slas_obs_fn = os.path.join(scm_dir, 'analysis', 'monthly_SLAs_observed.csv')\n",
    "if not os.path.exists(slas_obs_fn):\n",
    "    # iterate over RGI IDs\n",
    "    slas_obs = pd.DataFrame()\n",
    "    for rgi_id in tqdm(sorted(aois['RGIId'].drop_duplicates().values)):\n",
    "        scs_fn = os.path.join(scm_dir, 'study-sites', rgi_id, f'{rgi_id}_snow_cover_stats.csv')\n",
    "        scs = pd.read_csv(scs_fn)\n",
    "        scs['datetime'] = pd.to_datetime(scs['datetime'], format='mixed')\n",
    "        scs['Year'] = scs['datetime'].dt.year\n",
    "        scs['Month'] = scs['datetime'].dt.month\n",
    "        scs['Day'] = scs['datetime'].dt.day\n",
    "        # Filter data to within one week of the first of each month\n",
    "        scs_filtered = scs[(scs['Day'] >= 25) | (scs['Day'] <= 7)]\n",
    "        # Grab monthly snowline\n",
    "        Imonths = []\n",
    "        dates = []\n",
    "        for year, month in scs_filtered[['Year', 'Month']].drop_duplicates().values:\n",
    "            first_of_month = pd.Timestamp(year=year, month=month, day=1)\n",
    "            # identify closest observation to this date\n",
    "            scs_filtered.loc[:, 'diff'] = np.abs(scs_filtered.loc[:, 'datetime'] - first_of_month)\n",
    "            Imonths.append(scs_filtered['diff'].idxmin())\n",
    "            # save date \n",
    "            dates.append(pd.Timestamp(f\"{year}-{month}-01\"))\n",
    "        scs_monthly = scs.iloc[Imonths].reset_index(drop=True)\n",
    "\n",
    "        # add date column that is first of month\n",
    "        scs_monthly['Date'] = dates\n",
    "\n",
    "        # concatenate to full dataframe\n",
    "        slas_obs = pd.concat([slas_obs, scs_monthly])\n",
    "\n",
    "    # select relevant columns\n",
    "    slas_obs.rename(columns={'ELA_from_AAR_m': 'SLA_obs_m'}, inplace=True)\n",
    "    slas_obs = slas_obs[['RGIId', 'Date', 'SLA_obs_m']]\n",
    "    slas_obs.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    # save to file\n",
    "    slas_obs.to_csv(slas_obs_fn, index=False)\n",
    "    print('Remotely-sensed monthly SLAs saved to file:', slas_obs_fn)\n",
    "\n",
    "else:  \n",
    "    slas_obs = pd.read_csv(slas_obs_fn)\n",
    "    slas_obs['Date'] = pd.to_datetime(slas_obs['Date'])\n",
    "    print('Remotely-sensed monthly SLAs loaded from file.')\n",
    "\n",
    "slas_obs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c2c326-e946-4ec3-b453-2149a351f311",
   "metadata": {},
   "source": [
    "### Modeled SLAs and SMB at observed SLAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7383829",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Grab minimum glacier elevations for standardizing ###\n",
    "# Define output file name\n",
    "min_elevs_obs_fn = os.path.join(scm_dir, 'analysis', 'minimum_glacier_elevations_observed.csv')\n",
    "if not os.path.exists(min_elevs_obs_fn):\n",
    "    # load AOIs\n",
    "    aois = gpd.read_file(aois_fn)\n",
    "    min_elevs_obs = pd.DataFrame()\n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load DEM\n",
    "        dem_fn = glob.glob(os.path.join(scm_dir, 'study-sites', rgi_id, 'DEMs', '*.tif'))[0]\n",
    "        dem = xr.open_dataset(dem_fn).squeeze()\n",
    "        # Remove any wonky values\n",
    "        dem = xr.where((dem < -1e3) | (dem > 1e4) | (dem==0), np.nan, dem)\n",
    "        # Get minimum snowline altitude\n",
    "        min_elev = np.nanmin(dem.band_data.data)\n",
    "        # Add to dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id], 'min_elevation_m': [min_elev]})\n",
    "        min_elevs_obs = pd.concat([min_elevs_obs, df], axis=0)\n",
    "\n",
    "    # Save to file\n",
    "    min_elevs_obs.reset_index(drop=True, inplace=True)\n",
    "    min_elevs_obs.to_csv(min_elevs_obs_fn, index=False)\n",
    "    print('Minimum remotely-sensed glacier elevations saved to file:', min_elevs_obs_fn)\n",
    "    \n",
    "else:\n",
    "    min_elevs_obs = pd.read_csv(min_elevs_obs_fn)\n",
    "    print('Minimum remotely-sensed glacier elevations loaded.')\n",
    "\n",
    "plt.hist(min_elevs_obs['min_elevation_m'], bins=50)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66b26c4-b13f-4f05-b233-bd6e773bfdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if file already exists\n",
    "slas_mod_fn = os.path.join(scm_dir, 'analysis', 'monthly_SLAs_modeled.csv')\n",
    "if not os.path.exists(slas_mod_fn):\n",
    "    \n",
    "    # Initialize dataframe for results\n",
    "    slas_mod = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(aois['RGIId'].drop_duplicates().values):\n",
    "        # Load modeled monthly SMB\n",
    "        smb_fn = glob.glob(os.path.join(model_dir, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "        smb = xr.open_dataset(smb_fn)\n",
    "        # calculate cumulative SMB\n",
    "        def water_year(date):\n",
    "            if date.month >= 10:\n",
    "                return date.year\n",
    "            else:\n",
    "                return date.year - 1\n",
    "        smb = smb.assign_coords({'water_year': (['time'], [water_year(t) for t in smb.time.values])})\n",
    "        smb['bin_massbalclim_monthly_cumsum'] = smb['bin_massbalclim_monthly'].groupby('water_year').cumsum()\n",
    "        smb['time'] = smb.time.values.astype('datetime64[D]')\n",
    "        h = smb['bin_surface_h_initial'].data.ravel()\n",
    "        \n",
    "        # Interpolate modeled SLA as where SMB = 0 and SMB at the observed SLA\n",
    "        slas = np.nan * np.zeros(len(smb.time.data))\n",
    "        smb_at_slas = np.nan * np.zeros(len(smb.time.data))\n",
    "        for j, t in enumerate(smb.time.data):\n",
    "            smb_time = smb.sel(time=t)['bin_massbalclim_monthly_cumsum'].data[0]\n",
    "            # when SMB <= 0 everywhere, set SLA to maximum glacier elevation\n",
    "            if np.all(smb_time <= 0):\n",
    "                slas[j] = np.max(h)\n",
    "            # when SMB >= 0 everywhere, set SLA to minimum glacier elevation\n",
    "            elif np.all(smb_time >= 0):\n",
    "                slas[j] = np.min(h)\n",
    "            # otherwise, linearly interpolate SLA\n",
    "            else:\n",
    "                sorted_indices = np.argsort(h)\n",
    "                slas[j] = np.interp(0, smb_time[sorted_indices], h[sorted_indices])\n",
    "            # interpolate the modeled SMB at the observed SLA\n",
    "            sla_obs = slas_obs.loc[(slas_obs['RGIId']==rgi_id) & (slas_obs['Date']==t), 'SLA_obs_m']\n",
    "            if len(sla_obs) > 0:\n",
    "                smb_at_slas[j] = np.interp(sla_obs.values[0], h, smb_time)\n",
    "\n",
    "        # Save results in dataframe\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id]*len(smb.time.data),\n",
    "                            'Date': smb.time.data,\n",
    "                            'SLA_mod_m': slas,\n",
    "                            'SMB_at_obs_SLA_mwe': smb_at_slas})\n",
    "        # concatenate to full dataframe\n",
    "        slas_mod = pd.concat([slas_mod, df], axis=0)\n",
    "        \n",
    "    # Save to file\n",
    "    slas_mod.reset_index(drop=True, inplace=True)\n",
    "    slas_mod.to_csv(slas_mod_fn, index=False)\n",
    "    print('Modeled monthly SLAs saved to file:', slas_mod_fn)\n",
    "    \n",
    "else:\n",
    "    slas_mod = pd.read_csv(slas_mod_fn)\n",
    "    slas_mod['Date'] = pd.to_datetime(slas_mod['Date'])\n",
    "    print('Modeled monthly SLAs loaded from file.')\n",
    "\n",
    "slas_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6512e6fe",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a006b91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file\n",
    "slas_merged_fn = os.path.join(scm_dir, 'analysis', 'monthly_SLAs_modeled_observed_merged.csv')\n",
    "if not os.path.exists(slas_merged_fn):\n",
    "\n",
    "    # Merge modeled and remotely-sensed ELAs\n",
    "    slas_merged = slas_mod[['RGIId', 'Date', 'SLA_mod_m']].merge(slas_obs[['RGIId', 'Date', 'SLA_obs_m']],\n",
    "                                                                 on=['RGIId', 'Date'])\n",
    "    # Remove 2023 values (no modeled data in 2023)\n",
    "    slas_merged = slas_merged.loc[pd.DatetimeIndex(slas_merged['Date']).year < 2023]\n",
    "    \n",
    "    # Remove observations outside May - September\n",
    "    slas_merged = slas_merged.loc[(pd.DatetimeIndex(slas_merged['Date']).month >=5) \n",
    "                                  & (pd.DatetimeIndex(slas_merged['Date']).month <=9)]\n",
    "\n",
    "    # Subtract the minimum snowline altitudes to mitigate datum issues, s.t. SLAs are w.r.t. 0 m. \n",
    "    # for rgi_id in slas_merged['RGIId'].drop_duplicates().values:\n",
    "    #     min_sla_obs = min_slas_obs.loc[min_slas_obs['RGIId']==rgi_id, 'SLA_obs_m_min'].values[0]\n",
    "    #     slas_merged.loc[slas_merged['RGIId']==rgi_id, 'SLA_obs_m'] -= min_sla_obs\n",
    "    #     min_sla_mod = slas_mod.loc[slas_mod['RGIId']==rgi_id, 'SLA_mod_m'].min()\n",
    "    #     slas_merged.loc[slas_merged['RGIId']==rgi_id, 'SLA_mod_m'] -= min_sla_mod\n",
    "\n",
    "    # Save results\n",
    "    slas_merged.to_csv(slas_merged_fn, index=False)\n",
    "    print('Merged monthly SLAs saved to file:', slas_merged_fn)\n",
    "\n",
    "else:\n",
    "    slas_merged = pd.read_csv(slas_merged_fn)\n",
    "    print('Merged monthly SLAs loaded.')\n",
    "\n",
    "\n",
    "slas_merged['SLA_mod-obs_m'] = slas_merged['SLA_mod_m'] - slas_merged['SLA_obs_m']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(6,5))\n",
    "ax.hist(slas_merged['SLA_mod-obs_m'], bins=50)\n",
    "ax.set_xlabel('SLA$_{mod}$ - SLA$_{obs}$ [m]')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f'Mean diff = {np.nanmean((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'Std. diff = {np.nanstd((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'Median diff = {np.nanmedian((slas_merged[\"SLA_mod-obs_m\"]).values)} m')\n",
    "print(f'MAD diff = {MAD((slas_merged[\"SLA_mod-obs_m\"]).values, nan_policy=\"omit\")} m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de2ecd9",
   "metadata": {},
   "source": [
    "## 2. ELAs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad5f0a-21a1-4cc9-9ae5-0d82dc296576",
   "metadata": {},
   "source": [
    "### Modeled ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263c401e-1035-42af-a623-2d6d03f8365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_mod_fn = os.path.join(scm_dir, 'analysis', 'annual_ELAs_modeled.csv')\n",
    "if not os.path.exists(elas_mod_fn):\n",
    "    # Add Year column\n",
    "    slas_mod['Year'] = pd.DatetimeIndex(slas_mod['Date']).year\n",
    "    slas_mod = slas_mod.loc[slas_mod['Year'] < 2023].reset_index(drop=True) # remove 2023 observations\n",
    "    # Identify the row of maximum ELA for each site and each year\n",
    "    Imax = slas_mod.groupby(by=['RGIId', 'Year'])['SLA_mod_m'].idxmax().values\n",
    "    elas_mod = slas_mod.iloc[Imax].reset_index(drop=True)\n",
    "    elas_mod.rename(columns={'SLA_mod_m': 'ELA_mod_m'}, inplace=True)\n",
    "    # Reorder columns\n",
    "    elas_mod = elas_mod[['RGIId', 'Date', 'Year', 'ELA_mod_m']]\n",
    "    # Save to file\n",
    "    elas_mod.to_csv(elas_mod_fn, index=False)\n",
    "    print('Modeled annual ELAs saved to file:', elas_mod_fn)\n",
    "else:\n",
    "    elas_mod = pd.read_csv(elas_mod_fn)\n",
    "    elas_mod['Date'] = pd.to_datetime(elas_mod['Date'])\n",
    "    print('Modeled annual ELAs loaded from file.')\n",
    "    \n",
    "elas_mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33a61b4",
   "metadata": {},
   "source": [
    "### Remotely-sensed ELAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4f78fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "elas_obs_fn = os.path.join(scm_dir, 'analysis', 'annual_ELAs_observed.csv')\n",
    "if not os.path.exists(elas_obs_fn):\n",
    "    # iterate over sites\n",
    "    elas_obs = pd.DataFrame()\n",
    "    for rgi_id in tqdm(slas_obs['RGIId'].drop_duplicates().values):\n",
    "        # Subset to site\n",
    "        slas_obs_site = slas_obs.loc[slas_obs['RGIId']==rgi_id].reset_index(drop=True)\n",
    "        # Subset to 2016–2023\n",
    "        slas_obs_site = slas_obs_site.loc[slas_obs_site['Date'].dt.year >= 2016].reset_index(drop=True)\n",
    "        # identify maximum annual SLA\n",
    "        imax = slas_obs_site.groupby(slas_obs_site['Date'].dt.year)['SLA_obs_m'].idxmax().values\n",
    "        df = slas_obs_site.iloc[imax]\n",
    "        # concatenate to full dataframe\n",
    "        elas_obs = pd.concat([elas_obs, df])\n",
    "    elas_obs.reset_index(drop=True, inplace=True)\n",
    "    elas_obs.rename(columns={'SLA_obs_m': 'ELA_obs_m'}, inplace=True)\n",
    "\n",
    "    # save to file\n",
    "    elas_obs.to_csv(elas_obs_fn, index=False)\n",
    "    print('Remotely-sensed ELAs saved to file:', elas_obs_fn)\n",
    "\n",
    "else:\n",
    "    elas_obs = pd.read_csv(elas_obs_fn)\n",
    "    elas_obs['Date'] = pd.to_datetime(elas_obs['Date'])\n",
    "    print('Remotely-sensed ELAs loaded from file.')\n",
    "\n",
    "elas_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6739099a-00f6-428a-96ed-63a9a7fd655c",
   "metadata": {},
   "source": [
    "### Merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84bb9a5-a22f-41a1-9f5c-31f002300a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file name\n",
    "elas_merged_fn = os.path.join(scm_dir, 'analysis', 'annual_ELAs_modeled_observed_merged.csv')\n",
    "if not os.path.exists(elas_merged_fn):\n",
    "\n",
    "    # Merge modeled and remotely-sensed modeled ELAs\n",
    "    elas_obs['Year'] = elas_obs['Date'].dt.year\n",
    "    elas_merged = elas_obs[['RGIId', 'Year', 'ELA_obs_m']].merge(elas_mod[['RGIId', 'Year', 'ELA_mod_m']],\n",
    "                                                                 on=['RGIId', 'Year'])\n",
    "    \n",
    "    # Subset to 2016–2022 (no modeled data in 2023)\n",
    "    elas_merged = elas_merged.loc[(elas_merged['Year'] >= 2016) \n",
    "                                  & (elas_merged['Year'] < 2023)]\n",
    "        \n",
    "    # Save results\n",
    "    elas_merged.to_csv(elas_merged_fn, index=False)\n",
    "    print('Merged annual ELAs saved to file:', elas_merged_fn)\n",
    "\n",
    "else:\n",
    "    elas_merged = pd.read_csv(elas_merged_fn)\n",
    "    print('Merged annual ELAs loaded.')\n",
    "    \n",
    "# Calculate difference\n",
    "elas_merged['ELA_mod-obs_m'] = elas_merged['ELA_mod_m'] - elas_merged['ELA_obs_m']\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(5,5))\n",
    "ax.hist(elas_merged['ELA_mod-obs_m'], bins=50)\n",
    "ax.set_xlabel('ELA$_{mod}$ - ELA$_{obs}$ [m]')\n",
    "ax.set_ylabel('Counts')\n",
    "plt.show()\n",
    "\n",
    "print('\\nDifference stats:')\n",
    "print(f\"Mean diff = {np.nanmean(elas_merged['ELA_mod-obs_m'])} m\")\n",
    "print(f\"Std. diff = {np.nanstd(elas_merged['ELA_mod-obs_m'])} m\")\n",
    "print(f\"Median diff = {np.nanmedian(elas_merged['ELA_mod-obs_m'])} m\")\n",
    "print(f\"MAD diff = {MAD(elas_merged['ELA_mod-obs_m'], nan_policy='omit')} m\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f90cc",
   "metadata": {},
   "source": [
    "## 3. Degree-day factors of snow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7413e507",
   "metadata": {},
   "source": [
    "### Modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad13de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if already exists in file\n",
    "fsnow_mod_fn = os.path.join(scm_dir, 'analysis', 'fsnow_modeled.csv')\n",
    "if not os.path.exists(fsnow_mod_fn):\n",
    "    print('Compiling modeled melt factors of snow')\n",
    "    modelprm_dir = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/Rounce_et_al_2023/modelprms'\n",
    "    # Initialize dataframe\n",
    "    fsnow_mod = pd.DataFrame()\n",
    "    # Iterate over RGI IDs\n",
    "    for rgi_id in tqdm(slas_mod['RGIId'].drop_duplicates().values):\n",
    "        # Load model parameters\n",
    "        modelprm_fn = os.path.join(modelprm_dir, f\"{rgi_id.replace('RGI60-0','')}-modelprms_dict.pkl\")\n",
    "        modelprm = pd.read_pickle(modelprm_fn)\n",
    "        # Take the median of MCMC fsnow results (not much different than the mean)\n",
    "        ddfsnow_mcmc = np.array(modelprm['MCMC']['ddfsnow']['chain_0'])\n",
    "        df = pd.DataFrame({\"RGIId\": [rgi_id],\n",
    "                           \"fsnow_mod_m/C/d\": [np.median(ddfsnow_mcmc)]})\n",
    "        # Concatenate df to full dataframe\n",
    "        fsnow_mod = pd.concat([fsnow_mod, df])\n",
    "    # Save to file\n",
    "    fsnow_mod.reset_index(drop=True, inplace=True)\n",
    "    fsnow_mod.to_csv(fsnow_mod_fn, index=False)\n",
    "    print('Compiled melt factors of snow saved to file:', fsnow_mod_fn)\n",
    "\n",
    "else:\n",
    "    fsnow_mod = pd.read_csv(fsnow_mod_fn)\n",
    "    print('Compiled melt factors of snow loaded from file.')\n",
    "\n",
    "plt.hist(fsnow_mod['fsnow_mod_m/C/d'] * 1e3, bins=100)\n",
    "plt.xlabel('Melt factor of snow [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9bb9d4",
   "metadata": {},
   "source": [
    "### Observed\n",
    "\n",
    "Adjust the modeled degree-day factors of snow ($f_{snow}$) using the modeled SMB and cumulative PDDs from ERA5 downscaled to the snowline.\n",
    "\n",
    "$SMB(x,t) = Accumulation - Melt = \\Sigma Snowfall(x,t) - \\sum_{t_{melt}}^t PDD(x,t) \\cdot \\Delta t \\cdot f_{snow}$\n",
    "\n",
    "where $t_{melt}$ is the start of the melt season and $\\Delta t$ is $t-t_{melt}$. \n",
    "\n",
    "At the snowline, SMB = 0. Rearranging:\n",
    "\n",
    "$f_{snow}(x,t) = \\frac{\\Sigma Snowfall(x,t)}{\\sum_{t_{melt}}^t PDD(x,t) \\cdot \\Delta t} $\n",
    "\n",
    "If SMB = 10 m at the snowline on day 100 of the melt season and the cumulative PDD are 100 $^{\\circ}C$, this means that the model underestimated melt by 10 m / (100 $^{\\circ}C \\cdot$ 100 days) = 0.001 m/C/d. If the modeled melt factor of snow is 2 m/C/d, adjust the fsnow to 2.001 m/C/d. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b35d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fsnow_obs_fn = os.path.join(scm_dir, 'analysis', 'fsnow_observed.csv')\n",
    "if 1==1: #not os.path.exists(fsnow_obs_fn):\n",
    "    # Intialize results for all sites\n",
    "    fsnow_obs = pd.DataFrame()\n",
    "    \n",
    "    # Iterate over sites\n",
    "    for rgi_id in tqdm(slas_obs['RGIId'].drop_duplicates().values):\n",
    "        if type(rgi_id) != str:\n",
    "            continue\n",
    "        if 'RGI' not in rgi_id:\n",
    "            continue\n",
    "        \n",
    "        ### Load input data\n",
    "        # Get modeled fsnow\n",
    "        fsnow_mod_site = fsnow_mod.loc[fsnow_mod['RGIId']==rgi_id, 'fsnow_mod_m/C/d'].values[0]\n",
    "        # Load ERA-Land data\n",
    "        era_fn = os.path.join(scm_dir, 'study-sites', rgi_id, 'ERA', f\"{rgi_id}_ERA5-Land_daily_means.csv\")\n",
    "        era_df = pd.read_csv(era_fn)\n",
    "        era_df['Date'] = pd.to_datetime(era_df['Date'])\n",
    "        # Load centerline elevation profile\n",
    "        smb_fn = glob.glob(os.path.join(model_dir, 'glac_SMB_binned', f\"{rgi_id.split('RGI60-0')[1]}*.nc\"))[0]\n",
    "        smb = xr.open_dataset(smb_fn).squeeze()\n",
    "        h = smb.bin_surface_h_initial.values.ravel()\n",
    "        # Grab observed snowlines\n",
    "        slas_obs_site = slas_obs.loc[slas_obs['RGIId']==rgi_id]\n",
    "        slas_obs_site['Year'] = slas_obs_site['Date'].dt.year # add year column\n",
    "        # Grab modeled SMB at observed snowlines\n",
    "        slas_obs_site = pd.merge(slas_obs_site, slas_mod[['RGIId', 'Date', 'SMB_at_obs_SLA_mwe']], on=['RGIId', 'Date'])\n",
    "        # Don't include dates after September\n",
    "        slas_obs_site = slas_obs_site.loc[slas_obs_site['Date'].dt.month < 9]\n",
    "        \n",
    "        ### Downscale air temperatures to glacier surface using lapse rates, calculate PDDs\n",
    "        era_ds = xr.Dataset(\n",
    "            coords={'h': h, 'time': era_df['Date']},\n",
    "            data_vars={'temp_C': (['time'], era_df['mean_temperature_2m_C'].values),\n",
    "                    'lapse_rate': (['time'], era_df['lapse_rate_C/m'])})\n",
    "        era_ds['h_diff'] = era_df['ERA5_height_mean_m'].values[0] - era_ds['h']\n",
    "        era_ds['temp_downscaled_C'] = era_ds['temp_C'] - era_ds['lapse_rate'] * era_ds['h_diff']\n",
    "        era_ds['PDD'] = xr.where(era_ds['temp_downscaled_C'] > 0, era_ds['temp_downscaled_C'], 0)\n",
    "        era_ds['PDD_cumsum'] = era_ds['PDD'].groupby('time.year').cumsum()\n",
    "        \n",
    "        ### Identify the melt season start date (first PDD > 0) for each elevation\n",
    "        def find_first_positive(group):\n",
    "            # Mask PDD = 0\n",
    "            mask = group > 0\n",
    "            # Find the first index where PDD > 0\n",
    "            first_index = mask.argmax(dim=\"time\")\n",
    "            # Check if no positive PDD exists for the group\n",
    "            no_positive = ~mask.any(dim=\"time\")\n",
    "            # Grab the corresponding time values\n",
    "            time_values = group[\"time\"].isel(time=first_index)\n",
    "            # Replace invalid times with NaT for no_positive cases\n",
    "            time_values = time_values.where(~no_positive, np.datetime64(\"NaT\"))\n",
    "            return time_values\n",
    "        # Apply the function to each year and elevation group\n",
    "        era_ds['melt_season_start_date'] = (era_ds[\"PDD_cumsum\"]\n",
    "                                            .groupby(\"time.year\")\n",
    "                                            .map(find_first_positive))\n",
    "        \n",
    "        ### Interpolate cumulative PDDs at SLAs\n",
    "        pdds_slas = np.array([float(era_ds.sel(time=date, h=sla, method='nearest')['PDD_cumsum'].values) \n",
    "                              for date,sla in slas_obs_site[['Date', 'SLA_obs_m']].values])\n",
    "\n",
    "        ### Interpolate melt season start dates at SLAs\n",
    "        melt_start_dates = np.array([era_ds.sel(year=year, h=sla, method='nearest')['melt_season_start_date'].values\n",
    "                                     for year, sla in slas_obs_site[['Year', 'SLA_obs_m']].values])\n",
    "\n",
    "        # Compile in dataframe\n",
    "        slas_obs_site['Year'] = slas_obs_site['Date'].dt.year\n",
    "        slas_obs_site['PDD_cumsum_at_SLA_C'] = pdds_slas\n",
    "        slas_obs_site['melt_season_start_date'] = melt_start_dates\n",
    "        slas_obs_site = slas_obs_site.loc[slas_obs_site['PDD_cumsum_at_SLA_C'] > 0] # remove rows with 0 PDDs (to avoid dividing by 0)\n",
    "        slas_obs_site['days_since_melt_season_start_date'] = ((slas_obs_site['Date'] - slas_obs_site['melt_season_start_date']) / np.timedelta64(1, 'D')).astype(int)\n",
    "        \n",
    "        ### Calculate adjustment for modeled fsnow\n",
    "        slas_obs_site['fsnow_mod_adj'] = (slas_obs_site['SMB_at_obs_SLA_mwe'] \n",
    "                                          / (slas_obs_site['PDD_cumsum_at_SLA_C'] \n",
    "                                             * slas_obs_site['days_since_melt_season_start_date']))\n",
    "                \n",
    "        ### Add adjustment to fsnow_mod\n",
    "        slas_obs_site['fsnow_obs'] = fsnow_mod_site + slas_obs_site['fsnow_mod_adj']\n",
    "        \n",
    "        # Remove unrealistic values\n",
    "        slas_obs_site.loc[slas_obs_site['fsnow_obs'] > 0.01, 'fsnow_obs'] = np.nan\n",
    "        slas_obs_site.loc[slas_obs_site['fsnow_obs'] <= 0.0, 'fsnow_obs'] = np.nan\n",
    "        \n",
    "        ### Save the median\n",
    "        df = pd.DataFrame({'RGIId': [rgi_id],\n",
    "                           'fsnow_obs_m/C/d': [np.nanmedian(slas_obs_site['fsnow_obs'])]})\n",
    "        fsnow_obs = pd.concat([fsnow_obs, df], axis=0)\n",
    "        \n",
    "        ### Plot an example\n",
    "        if rgi_id=='RGI60-01.00032':\n",
    "            plt.rcParams.update({'font.size': 12, 'font.sans-serif': 'Arial'})\n",
    "            fig, ax = plt.subplots(3, 1, figsize=(8,10))\n",
    "            ax[0].plot(era_ds.time.data, era_ds['temp_C'], '-k', linewidth=0.5)\n",
    "            ax[0].set_ylabel('Air temperature [$^{\\circ}$C]')\n",
    "            ax[0].grid()\n",
    "            cmap = matplotlib.colors.LinearSegmentedColormap.from_list('my_cmap', ['w', '#fb6a4a', '#67000d']) # white to red\n",
    "            era_ds['PDD_cumsum'].transpose().plot(cmap=cmap, ax=ax[1], \n",
    "                                                cbar_kwargs={'orientation': 'horizontal', \n",
    "                                                            'shrink': 0.5, \n",
    "                                                            'label': 'Cumulative PDD [$^{\\circ}$C]'})\n",
    "            ax[1].set_ylabel('Elevation [m]')\n",
    "            ax[1].plot(slas_obs_site['Date'], slas_obs_site['SLA_obs_m'], '*k', label='Snowline altitude')\n",
    "            # melt season start\n",
    "            for year in era_ds.year.data[1:]:\n",
    "                era_ds_year = era_ds.sel(time=slice(f\"{year}-01-01\", f\"{year}-08-01\"))\n",
    "                xmesh, ymesh = np.meshgrid(era_ds_year.time.data, era_ds.h.data)\n",
    "                ax[1].contour(xmesh, ymesh, era_ds_year['PDD_cumsum'].data.transpose(), levels=[0], colors=['gray'])\n",
    "            ax[1].plot(pd.Timestamp('2010-01-01'), 0, '-', color='gray', label='Melt season start')\n",
    "                        \n",
    "            ax[1].legend(loc='upper left')\n",
    "            ax[1].set_xlabel('')\n",
    "            ax[2].plot(slas_obs_site['Date'], slas_obs_site['fsnow_obs'] * 1e3, '.k')\n",
    "            ax[2].axhline(fsnow_mod_site * 1e3, color='k', linestyle='--', label='Modeled')\n",
    "            ax[2].axhline(slas_obs_site['fsnow_obs'].median() * 1e3, color='k', label='Observed median')\n",
    "            ax[2].legend(loc='lower left')\n",
    "            ax[2].set_ylabel('$f_{snow}$ [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "            ax[2].grid()\n",
    "            ax[2].set_ylim(2, 5)\n",
    "            labels = ['a', 'b', 'c']\n",
    "            for i, axis in enumerate(ax):\n",
    "                axis.set_xlim(np.datetime64('2013-01-01'), np.datetime64('2023-01-01'))\n",
    "                axis.text(0.97, 0.85, labels[i], transform=axis.transAxes, fontweight='bold', fontsize=16)\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            # save figure\n",
    "            fig_fn = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/glacier-snow-cover-analysis/figures/figS4_melt_factors_example.png'\n",
    "            fig.savefig(fig_fn)\n",
    "            print('Figure saved to file:', fig_fn)\n",
    "        \n",
    "    # Save results to file\n",
    "    fsnow_obs.to_csv(fsnow_obs_fn, index=False)\n",
    "    print('Observed fsnow saved to file:', fsnow_obs_fn)  \n",
    "    \n",
    "else:\n",
    "    fsnow_obs = pd.read_csv(fsnow_obs_fn)\n",
    "    print('Observed fsnow loaded from file')  \n",
    "\n",
    "plt.hist(fsnow_obs['fsnow_obs_m/C/d'] * 1e3, bins=100)\n",
    "plt.xlabel('Melt factor of snow [mm $^{\\circ}$C$^{-1}$ d$^{-1}$]')\n",
    "plt.ylabel('Counts')\n",
    "plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea73e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot differences between modeled and observed fsnow\n",
    "import seaborn as sns\n",
    "fsnow_merged = pd.merge(fsnow_mod, fsnow_obs, on='RGIId')\n",
    "fsnow_merged['fsnow_mod-fsnow_obs_m/C/d'] = fsnow_merged['fsnow_mod_m/C/d'] - fsnow_merged['fsnow_obs_m/C/d']\n",
    "fsnow_merged['fsnow_mod-fsnow_obs_mm/C/d'] = fsnow_merged['fsnow_mod-fsnow_obs_m/C/d'] * 1e3\n",
    "\n",
    "clusters_fn = os.path.join(scm_dir, 'analysis', 'climate_clusters.csv')\n",
    "clusters = pd.read_csv(clusters_fn)\n",
    "\n",
    "fsnow_merged = pd.merge(fsnow_merged, clusters[['RGIId', 'clustName']], on='RGIId')\n",
    "\n",
    "sns.boxplot(fsnow_merged, x='fsnow_obs_m/C/d', hue='clustName')\n",
    "plt.show()\n",
    "sns.kdeplot(fsnow_merged, x='fsnow_mod-fsnow_obs_mm/C/d', hue='clustName') #, clip=(-0.5, 0.5))\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be1a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "glacier-snow-cover-mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
