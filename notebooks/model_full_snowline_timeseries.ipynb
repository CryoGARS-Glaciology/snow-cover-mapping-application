{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773296c3",
   "metadata": {},
   "source": [
    "# Analyze controls on AARs/ELAs using ERA-derived climate conditions and machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f345891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67fd8fd",
   "metadata": {},
   "source": [
    "### Define paths in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e49fd",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define path to study-sites/\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "\n",
    "# define path to snow-cover-mapping-application/\n",
    "base_path = os.path.join(study_sites_path, '..', 'snow-cover-mapping-application')\n",
    "\n",
    "# path to save output figures\n",
    "figures_out_path = os.path.join(study_sites_path, '..', 'snow-cover-mapping-application', 'figures')\n",
    "\n",
    "# Load necessary functions\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57b202",
   "metadata": {},
   "source": [
    "### Construct and/or update training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87e2efd-5e54-4551-ad41-d27855adfd55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define path and file name of training data\n",
    "training_data_path = os.path.join(study_sites_path, '..', 'snow-cover-mapping-application', 'inputs-outputs')\n",
    "training_data_fn = 'snowline_timeseries_full_training_data.csv'\n",
    "\n",
    "# load training data from file\n",
    "training_data_df = f.construct_update_training_data(study_sites_path, training_data_path, training_data_fn)\n",
    "\n",
    "# remove NaNs and reset index\n",
    "training_data_df.dropna(inplace=True)\n",
    "training_data_df.reset_index(drop=True, inplace=True)\n",
    "training_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd3e4fe-8f59-47d8-b9df-81e17d0ba818",
   "metadata": {},
   "source": [
    "## Subset the training data: sample conditions for median week of minimum AAR in each subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d978327-9807-46d1-8938-84e4d3e52beb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define path and file name for training data subset\n",
    "training_data_subset_path = os.path.join(study_sites_path, '..', 'snow-cover-mapping-application', 'inputs-outputs')\n",
    "training_data_subset_fn = 'snowline_timeseries_subset_training_data.csv'\n",
    "\n",
    "# load training data subset from file\n",
    "training_data_subset_df = f.subset_training_data(training_data_df, training_data_subset_path, training_data_subset_fn)\n",
    "\n",
    "# remove NaNs and reset index\n",
    "training_data_subset_df.dropna(inplace=True)  \n",
    "training_data_subset_df.reset_index(drop=True, inplace=True)\n",
    "training_data_subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424223f-4428-4d81-80c3-3291a9782ca3",
   "metadata": {},
   "source": [
    "## Plot pairplot of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b20ae84-370b-4d86-a6d8-ad32b5b86b47",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plt.rcParams.update({'font.size':14, 'font.sans-serif':'Arial'})\n",
    "# df = training_data_df.copy()\n",
    "# # change column names for plotting\n",
    "# df['$\\Sigma$PDDs'] = df['Cumulative_Positive_Degree_Days']\n",
    "# df['$\\Sigma$Snowfall'] = df['Cumulative_Snowfall_mwe']\n",
    "# df['Hyps. Index'] = df['Hypsometric_Index']\n",
    "# feature_cols = ['AAR', '$\\Sigma$PDDs', '$\\Sigma$Snowfall', \n",
    "#                 'Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect', 'Hyps. Index']\n",
    "# fig = sns.pairplot(df, vars=feature_cols, corner=True, diag_kind='kde', hue='AAR')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save figure\n",
    "# if 'Area' in feature_cols:\n",
    "#     fig_fn = os.path.join(figures_out_path, 'training_data_pairplot.png')\n",
    "# else:\n",
    "#     fig_fn = os.path.join(figures_out_path, 'training_data_climate_vars_pairplot.png')\n",
    "# fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "# print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb796f3",
   "metadata": {},
   "source": [
    "## Define feature columns and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to use for prediction in training data\n",
    "feature_columns = [\n",
    "                   # 'Cumulative_Positive_Degree_Days', \n",
    "                   # 'Cumulative_Snowfall_mwe', \n",
    "                   # 'PA_Ratio',\n",
    "                   'Hypsometric_Index',\n",
    "                   'Area', \n",
    "                   # 'Zmin', \n",
    "                   # 'Zmax', \n",
    "                   'Zmed', \n",
    "                   'Slope', \n",
    "                   'Aspect']\n",
    "# how to display each feature column in plots, etc.\n",
    "feature_columns_display = [\n",
    "                   # '$\\Sigma$PDDs', \n",
    "                   # '$\\Sigma$Snowfall [m.w.e.]', \n",
    "                   # 'P / \\sqrt(A)',\n",
    "                   'Hypsometric Index',\n",
    "                   'Area', \n",
    "                   # 'Zmin', \n",
    "                   # 'Zmax', \n",
    "                   'Z$_{med}$', \n",
    "                   'Slope', \n",
    "                   'Aspect']\n",
    "# variable to predict\n",
    "labels = ['AAR']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d686b03",
   "metadata": {},
   "source": [
    "## Define supervised machine learning models to test\n",
    "\n",
    "\n",
    "See the [SciKitLearn Classifier comparison page](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) for more models, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    RandomForestRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    SVR(),\n",
    "    GradientBoostingRegressor(),\n",
    "    Ridge()\n",
    "]\n",
    "\n",
    "# Model names (used for plotting, etc.)\n",
    "model_names = [\n",
    "    \"Linear Regression\",\n",
    "    \"Random Forest Regression\",\n",
    "    \"Decision Tree Regression\",\n",
    "    \"Support Vector Regression\",\n",
    "    \"Gradient Boosting Regression\",\n",
    "    \"Ridge Regression\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398d23b",
   "metadata": {},
   "source": [
    "## Train and test machine learning models using K-folds cross-validation, conduct permutation feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf530ca-9103-40d8-a1fe-ec19f8021668",
   "metadata": {},
   "source": [
    "### One model for all subregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f09fab8-7c1f-4f87-9ff2-855dd7f3012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236d54b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Determine best model\n",
    "out_path = os.path.join(base_path, 'inputs-outputs')\n",
    "best_model_fn = 'best_model_all_regions.json'\n",
    "best_model, X, y = f.determine_best_model(training_data_subset_df, models, model_names, \n",
    "                                          feature_columns, labels, out_path, best_model_fn)\n",
    "\n",
    "# -----Assess model feature importances\n",
    "importances_fn = 'best_model_all_regions_feature_importances.csv'\n",
    "figure_fn = 'best_model_all_regions_feature_importances.png'\n",
    "feature_importances = f.assess_model_feature_importances(best_model, X, y, feature_columns, feature_columns_display, \n",
    "                                                         out_path, importances_fn, figures_out_path, figure_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dd6074-04fa-4e9c-b5c3-3451e82f0ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf861d82-d8ba-4677-a2d4-9ea923374f3c",
   "metadata": {},
   "source": [
    "### Separate model for each subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d291e5b-99e6-4786-a38f-80f2e18b4c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Grab list of all unique regions and subregions in dataset\n",
    "unique_subregion_counts = training_data_subset_df[['O1Region', 'O2Region']].value_counts().reset_index(name='count')\n",
    "unique_subregion_counts = unique_subregion_counts.sort_values(by=['O1Region', 'O2Region']).reset_index(drop=True)\n",
    "unique_subregions = unique_subregion_counts[['O1Region', 'O2Region']].values\n",
    "unique_subregion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2973b6dd-70e8-484f-8a97-576bb2502f54",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Iterate over unique subregions\n",
    "for o1region, o2region in unique_subregions:\n",
    "    \n",
    "    # subset training data to subregion\n",
    "    training_subregion_df = training_data_subset_df.loc[(training_data_subset_df['O1Region']==o1region) \n",
    "                                                        & (training_data_subset_df['O2Region']==o2region)]\n",
    "    # grab subregion name and color for plotting\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    subregion_name_save = subregion_name.replace('.','').replace(' ','') # remove periods and spaces for saving\n",
    "\n",
    "    print('----------')\n",
    "    print(subregion_name)\n",
    "    print('----------')\n",
    "\n",
    "    # -----Determine best model\n",
    "    out_path = os.path.join(base_path, 'inputs-outputs')\n",
    "    best_model_fn = 'best_model_' + subregion_name_save + '.json'\n",
    "    best_model, X, y = f.determine_best_model(training_subregion_df, models, model_names, \n",
    "                                              feature_columns, labels, out_path, best_model_fn)\n",
    "    \n",
    "    # -----Assess model feature importances\n",
    "    importances_fn = 'best_model_' + subregion_name_save + '_feature_importances.csv'\n",
    "    figure_fn = 'best_model_' + subregion_name_save + '_feature_importances.png'\n",
    "    feature_importances = f.assess_model_feature_importances(best_model, X, y, feature_columns, feature_columns_display, \n",
    "                                                             out_path, importances_fn, figures_out_path, figure_fn)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa042f06-ad39-465c-a57b-1bd15cdd28ce",
   "metadata": {},
   "source": [
    "## Conduct simulations for assessing regional AAR response to changes in terrain parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763fc685-c820-4378-b201-999c32b73beb",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # -----Test for all regions model\n",
    "# # Construct input data\n",
    "# input_df = pd.DataFrame()\n",
    "# for column in feature_columns:\n",
    "#     input_df[column] = [training_data_df[column].drop_duplicates().mean()]\n",
    "# # predict AAR using input data\n",
    "# clf_fn = os.path.join(base_path, 'inputs-outputs', 'best_classifier_all_regions.joblib')\n",
    "# clf = load(clf_fn)\n",
    "# AAR_mean = clf.predict(input_df)[0]\n",
    "\n",
    "# # Set up figure\n",
    "# plt.rcParams.update({'font.size':14, 'font.sans-serif': 'Arial'})\n",
    "# fig, ax = plt.subplots(2, 3, figsize=(16, 8))\n",
    "# ax = ax.flatten()\n",
    "# # define settings for each column\n",
    "# columns = ['Area', 'Zmed', 'Hypsometric_Index', 'Aspect', 'Slope']\n",
    "# ranges = [np.arange(0.5, 1000, step=1),    # Areas\n",
    "#           np.arange(500, 3000, step=100),  # Zmeds\n",
    "#           np.arange(-3, 3, step=0.2),     # HIs\n",
    "#           np.arange(0, 360, step=20),      # Aspects\n",
    "#           np.arange(5, 30, step=1)]        # Slopes\n",
    "# units = ['km$^2$', 'm.a.s.l.', 'm/m', 'degrees', 'degrees']\n",
    "# colors = ['k', 'g', 'c', 'r', 'm', 'b']\n",
    "# # iterate over columns\n",
    "# i=0\n",
    "# for column, range, unit, color in list(zip(columns, ranges, units, colors)):\n",
    "#     # initialize AARs\n",
    "#     aars = np.zeros(len(range))\n",
    "#     # predict AAR for each value in range\n",
    "#     for j, value in enumerate(range):\n",
    "#         input_adj_df = input_df.copy()\n",
    "#         input_adj_df[column] = value\n",
    "#         aars[j] = clf.predict(input_adj_df)[0]\n",
    "#     # plot results   \n",
    "#     ax[i].plot(range, aars, '-', color=color, linewidth=3)\n",
    "#     ax[i].set_ylim(0.4, 0.9)\n",
    "#     ax[i].grid()\n",
    "#     ax[i].set_title(column.replace('_', ' '))\n",
    "#     ax[i].set_xlabel('[' + unit + ']')\n",
    "#     ax[i].set_ylabel('AAR')\n",
    "\n",
    "#     i+=1\n",
    "\n",
    "# fig.delaxes(ax[-1])\n",
    "# fig.tight_layout()\n",
    "# plt.show()\n",
    "\n",
    "# # Save figure\n",
    "# fig_fn = os.path.join(figures_out_path, 'model_sensitivities_terrain_parameters.png')\n",
    "# fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "# print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260d16bd-e143-4e94-b11a-dc9d709b7afc",
   "metadata": {},
   "source": [
    "## Conduct simulations for assessing regional sensitivity to climate perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c9dea-b259-4fc3-8d73-e7b2fd5d5025",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # -----Iterate over unique subregions\n",
    "# perturb_df = pd.DataFrame()\n",
    "# for o1region, o2region in unique_subregions:\n",
    "    \n",
    "#     # Subset training data to subregion\n",
    "#     snowlines_subregion = training_data_df.loc[(training_data_df['O1Region']==o1region) & (training_data_df['O2Region']==o2region)]\n",
    "#     subregion_name, color = f.determine_subregion_name_color(float(o1region), float(o2region))\n",
    "#     print(subregion_name)\n",
    "\n",
    "#     # Construct input data\n",
    "#     input_df = pd.DataFrame()\n",
    "#     for column in feature_columns:\n",
    "#         input_df[column] = [snowlines_subregion[column].drop_duplicates().mean()]\n",
    "#     # predict AAR using input data\n",
    "#     clf_fn = os.path.join(base_path, 'inputs-outputs', 'best_classifier_' + subregion_name + '.joblib')\n",
    "#     clf = load(clf_fn)\n",
    "#     AAR_mean = clf.predict(input_df)[0]\n",
    "    \n",
    "#     # predict AAR if 150 additional PDDs\n",
    "#     input_adj_df = input_df.copy()\n",
    "#     input_adj_df['Cumulative_Positive_Degree_Days'] = input_adj_df['Cumulative_Positive_Degree_Days'] + 150\n",
    "#     AAR_PDD_perturb = clf.predict(input_adj_df)[0]\n",
    "\n",
    "#     # predict AAR if - 10% cumulative snow\n",
    "#     input_adj_df = input_df.copy()\n",
    "#     input_adj_df['Cumulative_Snowfall_mwe'] = input_adj_df['Cumulative_Snowfall_mwe'] * 0.9\n",
    "#     AAR_snowfall_perturb = clf.predict(input_adj_df)[0]\n",
    "\n",
    "#     # compile in dataframe\n",
    "#     df = pd.DataFrame({'Subregion Name': [subregion_name],\n",
    "#                        'Mean conditions AAR': [AAR_mean],\n",
    "#                        '+150 PPDs AAR': [AAR_PDD_perturb],\n",
    "#                        '+150 PDDs AAR % change': [(AAR_mean-AAR_PDD_perturb)/AAR_mean * 100],\n",
    "#                        '-10% snowfall AAR': [AAR_snowfall_perturb],\n",
    "#                        '-10% snowfall AAR % change': [(AAR_mean-AAR_snowfall_perturb)/AAR_mean * 100]\n",
    "#                       })\n",
    "#     perturb_df = pd.concat([perturb_df, df])\n",
    "\n",
    "# perturb_df.reset_index(drop=True, inplace=True)\n",
    "# perturb_df['+150 PDDs AAR % change'] = perturb_df['+150 PDDs AAR % change'].apply(np.round)\n",
    "# perturb_df['-10% snowfall AAR % change'] = perturb_df['-10% snowfall AAR % change'].apply(np.round)\n",
    "\n",
    "# perturb_df[['Subregion Name', '+150 PDDs AAR % change', '-10% snowfall AAR % change']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710f5b89-9cfb-458c-ad60-9f1c7fc70f64",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Plot PDDs and AAR for one site as an example\n",
    "# site_name = 'SouthCascade'\n",
    "# training_site = training_data_df.loc[training_data_df['site_name']==site_name]\n",
    "# training_site = training_site.sort_values(by='datetime')\n",
    "\n",
    "# # load ERA data\n",
    "# ERA_fn = glob.glob(os.path.join(study_sites_path, site_name, 'ERA', '*.csv'))[0]\n",
    "# ERA = pd.read_csv(ERA_fn)\n",
    "# ERA['Date'] = pd.to_datetime(ERA['Date'])\n",
    "\n",
    "# plt.rcParams.update({'font.size':16, 'font.sans-serif':'Arial'})\n",
    "# fig, ax = plt.subplots(2, 1, figsize=(12,8))\n",
    "# # AAR\n",
    "# ax[0].plot(training_site['datetime'], training_site['AAR'], '.k')\n",
    "# ax[0].grid()\n",
    "# ax[0].set_xlim(np.datetime64('2016-01-01'), np.datetime64('2023-01-01'))\n",
    "# ax[0].set_ylabel('AAR')\n",
    "# ax[0].set_title('South Cascade Glacier')\n",
    "\n",
    "# # PDDs and snowfall\n",
    "# ax[1].bar(ERA['Date'], ERA['Cumulative_Snowfall_mwe'], color='#4eb3d3', width=1)\n",
    "# ax[1].set_ylabel('$\\Sigma$ Snowfall [m.w.e.]', color='#4eb3d3')\n",
    "# ax[1].set_xlim(np.datetime64('2016-01-01'), np.datetime64('2023-01-01'))\n",
    "# ax[1].grid()\n",
    "# ax[1].tick_params(axis='y', colors='#4eb3d3')\n",
    "# ax2 = ax[1].twinx()\n",
    "# ax2.plot(ERA['Date'], ERA['Cumulative_Positive_Degree_Days'], '.m')\n",
    "# ax2.set_ylabel('$\\Sigma$ Positive degree days', color='m')\n",
    "# ax2.tick_params(axis='y', colors='m')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save figure\n",
    "# fig_fn = os.path.join(figures_out_path, 'example_time_series_SouthCascadeGlacier.png')\n",
    "# fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "# print('figure saved to file: ' + fig_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
