{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4573b7c6",
   "metadata": {},
   "source": [
    "# Analyze controls on AARs/ELAs using ERA-derived climate conditions and machine learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cd6bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a980f01e",
   "metadata": {},
   "source": [
    "### Define paths in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2407b68e",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define path to study-sites/\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "\n",
    "# define path to snow-cover-mapping-application/\n",
    "base_path = os.path.join(study_sites_path, '..', 'snow-cover-mapping-application')\n",
    "\n",
    "# path to save output figures\n",
    "figures_out_path = os.path.join(study_sites_path, '..', 'snow-cover-mapping-application', 'figures')\n",
    "\n",
    "# Load necessary functions\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f3aa7",
   "metadata": {},
   "source": [
    "### Construct and/or update training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e907b2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define path and file name of training data\n",
    "training_data_path = os.path.join(study_sites_path, '..', 'snow-cover-mapping-application', 'inputs-outputs')\n",
    "training_data_fn = 'snowline_timeseries_full_training_data.csv'\n",
    "\n",
    "# load training data from file\n",
    "training_data_df = f.construct_update_training_data(study_sites_path, training_data_path, training_data_fn)\n",
    "\n",
    "# remove NaNs and reset index\n",
    "training_data_df.dropna(inplace=True)\n",
    "training_data_df.reset_index(drop=True, inplace=True)\n",
    "training_data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1d7133",
   "metadata": {},
   "source": [
    "## Subset the training data: sample conditions for median week of minimum AAR in each subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b45c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# define path and file name for training data subset\n",
    "training_data_subset_path = os.path.join(study_sites_path, '..', 'snow-cover-mapping-application', 'inputs-outputs')\n",
    "training_data_subset_fn = 'snowline_timeseries_subset_training_data.csv'\n",
    "\n",
    "# load training data subset from file\n",
    "training_data_subset_df = f.subset_training_data(training_data_df, training_data_subset_path, training_data_subset_fn)\n",
    "\n",
    "# remove NaNs and reset index\n",
    "training_data_subset_df.dropna(inplace=True)  \n",
    "training_data_subset_df.reset_index(drop=True, inplace=True)\n",
    "training_data_subset_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3765d6",
   "metadata": {},
   "source": [
    "## Plot pairplot of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc421af4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':14, 'font.sans-serif':'Arial'})\n",
    "df = training_data_df.copy()\n",
    "# change column names for plotting\n",
    "df['$\\Sigma$PDDs'] = df['Cumulative_Positive_Degree_Days']\n",
    "df['$\\Sigma$Snowfall'] = df['Cumulative_Snowfall_mwe']\n",
    "df['Hyps. Index'] = df['Hypsometric_Index']\n",
    "df['ELA [m]'] = df['ELA_from_AAR_m']\n",
    "feature_cols = ['AAR', 'ELA [m]', '$\\Sigma$PDDs', '$\\Sigma$Snowfall', \n",
    "                'Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect', 'Hyps. Index', 'Mean_Annual_Air_Temp_Range', 'Mean_Annual_Precipitation_Max']\n",
    "fig = sns.pairplot(df, vars=feature_cols, corner=True, diag_kind='kde', hue='AAR')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "if 'Area' in feature_cols:\n",
    "    fig_fn = os.path.join(figures_out_path, 'training_data_pairplot.png')\n",
    "else:\n",
    "    fig_fn = os.path.join(figures_out_path, 'training_data_climate_vars_pairplot.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6036c6d8",
   "metadata": {},
   "source": [
    "## Define feature columns and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3408fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to use for prediction in training data\n",
    "feature_columns = [\n",
    "                   'Cumulative_Positive_Degree_Days', \n",
    "                   'Cumulative_Snowfall_mwe', \n",
    "                   # 'PA_Ratio',\n",
    "                   'Hypsometric_Index',\n",
    "                   'Area', \n",
    "                   # 'Zmin', \n",
    "                   # 'Zmax', \n",
    "                   'Zmed', \n",
    "                   'Slope', \n",
    "                   'Aspect',\n",
    "                   'Mean_Annual_Air_Temp_Range',\n",
    "                   'Mean_Annual_Precipitation_Max']\n",
    "# how to display each feature column in plots, etc.\n",
    "feature_columns_display = [\n",
    "                   '$\\Sigma$PDDs', \n",
    "                   '$\\Sigma$Snowfall [m.w.e.]', \n",
    "                   # 'P / \\sqrt(A)',\n",
    "                   'Hypsometric Index',\n",
    "                   'Area', \n",
    "                   # 'Z$_{min}$ [m]', \n",
    "                   # 'Z$_{max}$ [m]', \n",
    "                   'Z$_{med}$', \n",
    "                   'Slope', \n",
    "                   'Aspect',\n",
    "                   'Mean annual air temperature range [$^{\\circ}$C]',\n",
    "                   'Mean annual $\\Sigma$Precipitation [m.w.e.]']\n",
    "# variable to predict\n",
    "labels = ['AAR']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a21a3db",
   "metadata": {},
   "source": [
    "## Define supervised machine learning models to test\n",
    "\n",
    "\n",
    "See the [SciKitLearn Classifier comparison page](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) for more models, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a149a94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    RandomForestRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    SVR(),\n",
    "    GradientBoostingRegressor(),\n",
    "    Ridge()\n",
    "]\n",
    "\n",
    "# Model names (used for plotting, etc.)\n",
    "model_names = [\n",
    "    \"Linear Regression\",\n",
    "    \"Random Forest Regression\",\n",
    "    \"Decision Tree Regression\",\n",
    "    \"Support Vector Regression\",\n",
    "    \"Gradient Boosting Regression\",\n",
    "    \"Ridge Regression\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d781c02",
   "metadata": {},
   "source": [
    "# Train and test machine learning models using K-folds cross-validation, conduct permutation feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f66c623",
   "metadata": {},
   "source": [
    "## One model for all subregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8172c9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Determine best model\n",
    "out_path = os.path.join(base_path, 'inputs-outputs')\n",
    "best_model_fn = 'best_model_all_regions.joblib'\n",
    "best_model, X, y = f.determine_best_model(training_data_subset_df, models, model_names, \n",
    "                                          feature_columns, labels, out_path, best_model_fn)\n",
    "\n",
    "# -----Assess model feature importances\n",
    "importances_fn = 'best_model_all_regions_feature_importances.csv'\n",
    "figure_fn = 'best_model_all_regions_feature_importances.png'\n",
    "feature_importances = f.assess_model_feature_importances(best_model, X, y, feature_columns, feature_columns_display, \n",
    "                                                         out_path, importances_fn, figures_out_path, figure_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aafbc1cd",
   "metadata": {},
   "source": [
    "### Tune the hyperparameters for the Random Forest Regression model\n",
    "\n",
    "Adapted from [Will Koehrsen's blog post](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74) and code package and [here](https://pyimagesearch.com/2021/05/17/introduction-to-hyperparameter-tuning-with-scikit-learn-and-python/). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f417a51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "import time \n",
    "\n",
    "# -----Define function for conducting the grid search\n",
    "def grid_search(X_train, X_test, y_train, y_test, parameters):\n",
    "    # start timer\n",
    "    t1 = time.time()\n",
    "    \n",
    "    # Generate and evaluate the BASE model with no hyperparameter tuning\n",
    "    rf = RandomForestRegressor(random_state = 42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    base_score = rf.score(X_test, y_test)\n",
    "    \n",
    "    # Generate the grid search model\n",
    "    # rf = RandomForestRegressor(random_state = 42)\n",
    "    grid_search = GridSearchCV(estimator=rf, param_grid=parameters, \n",
    "                               cv=3, n_jobs=-1, verbose=1, return_train_score=True)\n",
    "    # fit the model\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # print best parameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(best_params)\n",
    "\n",
    "    # evaluate best estimator\n",
    "    best_model = grid_search.best_estimator_\n",
    "    score = best_model.score(X_test, y_test)\n",
    "    print('Score of best estimator = ', score)\n",
    "    print('Score improvement = ', score - base_score)\n",
    "\n",
    "    # stop timer\n",
    "    t2 = time.time()\n",
    "    print('Time elapsed = ', np.round((t2-t1) / 60, 2), ' min')\n",
    "\n",
    "    return best_params, best_model\n",
    "\n",
    "# -----Conduct a Grid Search of parameter values, determine the best ones\n",
    "# Split X and y into training and testing \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "# -----ITERATION 1\n",
    "# Define ranges for hyperparameters to test\n",
    "parameters = {'n_estimators': [int(x) for x in np.linspace(1, 500, num = 3)],  # Number of trees\n",
    "              'max_depth': [int(x) for x in np.linspace(0, 100, num = 3)],  # Maximum number of levels in tree\n",
    "              'min_samples_split': [2, 5, 10],  # Minimum number of samples required to split a node\n",
    "              'min_samples_leaf': [1, 2, 4],  # Minimum number of samples required at each leaf node\n",
    "              'bootstrap': [True, False]  # Method of selecting samples for training each tree\n",
    "             }\n",
    "best_params, best_model_tuned = grid_search(X_train, X_test, y_train, y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d363fb3-3bb0-4e0e-8318-c8375479dfbe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----ITERATION 2\n",
    "# Define ranges for hyperparameters to test\n",
    "parameters = {'n_estimators': [int(x) for x in np.linspace(100, 300, num = 5)],  # Number of trees\n",
    "              'max_depth': [int(x) for x in np.linspace(30, 70, num = 5)],  # Maximum number of levels in tree\n",
    "              'min_samples_split': [2, 3, 4],  # Minimum number of samples required to split a node\n",
    "              'min_samples_leaf': [1],  # Minimum number of samples required at each leaf node\n",
    "              'bootstrap': [True]  # Method of selecting samples for training each tree\n",
    "             }\n",
    "best_params, best_model_tuned = grid_search(X_train, X_test, y_train, y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21bdfe1f-19e4-440c-95eb-401f9de2ce9d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----ITERATION 3\n",
    "# Define ranges for hyperparameters to test\n",
    "parameters = {'n_estimators': [int(x) for x in np.linspace(100, 200, num = 11)],  # Number of trees\n",
    "              'max_depth': [int(x) for x in np.linspace(20, 40, num = 11)],  # Maximum number of levels in tree\n",
    "              'min_samples_split': [3],  # Minimum number of samples required to split a node\n",
    "              'min_samples_leaf': [1],  # Minimum number of samples required at each leaf node\n",
    "              'bootstrap': [True]  # Method of selecting samples for training each tree\n",
    "             }\n",
    "best_params, best_model_tuned = grid_search(X_train, X_test, y_train, y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532626a-ab77-4d23-9cfc-3c229b89bde7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----ITERATION 4\n",
    "# Define ranges for hyperparameters to test\n",
    "parameters = {'n_estimators': [int(x) for x in np.linspace(140, 160, num = 11)],  # Number of trees\n",
    "              'max_depth': [int(x) for x in np.linspace(20, 24, num = 5)],  # Maximum number of levels in tree\n",
    "              'min_samples_split': [3],  # Minimum number of samples required to split a node\n",
    "              'min_samples_leaf': [1],  # Minimum number of samples required at each leaf node\n",
    "              'bootstrap': [True]  # Method of selecting samples for training each tree\n",
    "             }\n",
    "best_params, best_model_tuned = grid_search(X_train, X_test, y_train, y_test, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef46183-3ac9-444c-8dbe-a1e4fa2da53a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----ITERATION 5\n",
    "# Define ranges for hyperparameters to test\n",
    "parameters = {'n_estimators': [int(x) for x in np.linspace(146, 150, num = 5)],  # Number of trees\n",
    "              'max_depth': [22],  # Maximum number of levels in tree\n",
    "              'min_samples_split': [3],  # Minimum number of samples required to split a node\n",
    "              'min_samples_leaf': [1],  # Minimum number of samples required at each leaf node\n",
    "              'bootstrap': [True]  # Method of selecting samples for training each tree\n",
    "             }\n",
    "best_params, best_model_tuned = grid_search(X_train, X_test, y_train, y_test, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bac379-4573-46a4-a3f3-ff25a0e06bfc",
   "metadata": {},
   "source": [
    "### Save the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0560ae4f-f796-4771-a36d-ea75311f2e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_tuned_fn = os.path.join(base_path, 'inputs-outputs', 'best_model_all_regions_tuned.joblib')\n",
    "dump(best_model_tuned, best_model_tuned_fn)\n",
    "print('Best model saved to file:', best_model_tuned_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfca9f3b-8480-4713-b746-f5d5dbab3541",
   "metadata": {},
   "source": [
    "### Plot model predictions for each pair of features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfd0e2b-4503-449e-978f-9b5102968ca0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load best model from file\n",
    "best_model_tuned_fn = os.path.join(base_path, 'inputs-outputs', 'best_model_all_regions_tuned.joblib')\n",
    "best_model_tuned = load(best_model_tuned_fn)\n",
    "\n",
    "# Determine how many subplots and rows are needed based on the number of feature columns\n",
    "n_columns = len(feature_columns)-2\n",
    "n_rows = len(feature_columns)-2\n",
    "\n",
    "# Set up figure\n",
    "plt.rcParams.update({'font.size':14, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(n_rows, n_columns, figsize=(4*n_columns, 4*n_rows))\n",
    "cmp = 'viridis'\n",
    "\n",
    "# Use mean of all feature columns for baseline\n",
    "input_df = pd.DataFrame()\n",
    "for feature_column in feature_columns:\n",
    "    input_df[feature_column] = [X[feature_column].mean()]\n",
    "\n",
    "# Iterate over each combination of feature columns\n",
    "for i, column1 in enumerate(feature_columns[0:-1]):\n",
    "    # define range of values to test for column 1\n",
    "    column1_range = list(np.linspace(X[column1].min(), X[column1].max(), num=10))\n",
    "\n",
    "    columns2 = feature_columns[i+1:-1]\n",
    "    columns2.reverse()\n",
    "    for j, column2 in enumerate(columns2):\n",
    "        # define range of values to test for column 2\n",
    "        column2_range = list(np.linspace(X[column2].min(), X[column2].max(), num=10))\n",
    "\n",
    "        # grab model predictions for each combination of parameters in the ranges\n",
    "        predictions = np.zeros((len(column1_range), len(column2_range)))\n",
    "        for k in range(0,len(column1_range)):\n",
    "            for l in range(0, len(column2_range)):\n",
    "                # change feature column values\n",
    "                input_df_adj = input_df.copy()\n",
    "                input_df_adj[column1] = column1_range[k]\n",
    "                input_df_adj[column2] = column2_range[l]\n",
    "                # make model prediction\n",
    "                predictions[k,l] = best_model_tuned.predict(input_df_adj[feature_columns])\n",
    "                # ROWS = column1 = y\n",
    "                # COLUMNS = column2 = x\n",
    "\n",
    "        # plot results\n",
    "        ax[n_rows-1-i,j].imshow(np.flipud(predictions), cmap=cmp, clim=(0.4, 0.8), aspect=np.max(column2_range)/np.max(column1_range),\n",
    "                                extent=(np.min(column2_range), np.max(column2_range), \n",
    "                                        np.min(column1_range), np.max(column1_range)))\n",
    "        ax[n_rows-1-i,j].scatter(X[column2], X[column1], c=y.values, cmap=cmp, vmin=0.4, vmax=0.8, edgecolors='k')\n",
    "        if j==0:\n",
    "            ax[n_rows-1-i,j].set_ylabel(column1)\n",
    "        if i==n_rows-1:\n",
    "            ax[n_rows-1-i,j].set_xlabel(column2)\n",
    "        ax[n_rows-1-i,j].grid()\n",
    "\n",
    "# add xlabels on bottom row\n",
    "# feature_columns_labels = feature_columns.copy()\n",
    "# feature_columns_labels.reverse()\n",
    "# for j, label in enumerate(feature_columns_labels):\n",
    "#     ax[n_rows-1, j].set_xlabel(label)\n",
    "\n",
    "# remove empty axes\n",
    "num_del = 1\n",
    "for i in range(0, n_rows-1):\n",
    "    for j in range(num_del, n_columns):\n",
    "        fig.delaxes(ax[i,j])\n",
    "    num_del += 1      \n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017e5ff7",
   "metadata": {},
   "source": [
    "## Separate model for each subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64f1b910",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# -----Grab list of all unique regions and subregions in dataset\n",
    "unique_subregion_counts = training_data_subset_df[['O1Region', 'O2Region']].value_counts().reset_index(name='count')\n",
    "unique_subregion_counts = unique_subregion_counts.sort_values(by=['O1Region', 'O2Region']).reset_index(drop=True)\n",
    "unique_subregions = unique_subregion_counts[['O1Region', 'O2Region']].values\n",
    "unique_subregion_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af8e42",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Iterate over unique subregions\n",
    "for o1region, o2region in unique_subregions:\n",
    "    \n",
    "    # subset training data to subregion\n",
    "    training_subregion_df = training_data_subset_df.loc[(training_data_subset_df['O1Region']==o1region) \n",
    "                                                        & (training_data_subset_df['O2Region']==o2region)]\n",
    "    # grab subregion name and color for plotting\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    subregion_name_save = subregion_name.replace('.','').replace(' ','') # remove periods and spaces for saving\n",
    "\n",
    "    print('----------')\n",
    "    print(subregion_name)\n",
    "    print('----------')\n",
    "\n",
    "    # -----Determine best model\n",
    "    out_path = os.path.join(base_path, 'inputs-outputs')\n",
    "    best_model_fn = 'best_model_' + subregion_name_save + '.joblib'\n",
    "    best_model, X, y = f.determine_best_model(training_subregion_df, models, model_names, \n",
    "                                              feature_columns, labels, out_path, best_model_fn)\n",
    "    \n",
    "    # -----Assess model feature importances\n",
    "    importances_fn = 'best_model_' + subregion_name_save + '_feature_importances.csv'\n",
    "    figure_fn = 'best_model_' + subregion_name_save + '_feature_importances.png'\n",
    "    feature_importances = f.assess_model_feature_importances(best_model, X, y, feature_columns, feature_columns_display, \n",
    "                                                             out_path, importances_fn, figures_out_path, figure_fn)\n",
    "    print(' ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a404e67",
   "metadata": {},
   "source": [
    "## Plot all feature importances on one figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa020daa",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "plt.rcParams.update({'font.size':16, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(6, 2, figsize=(18, 18))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# grab all feature importance file names\n",
    "feature_importances_fns = sorted(glob.glob(os.path.join(base_path, 'inputs-outputs', \n",
    "                                                        'best_model_*feature_importances.csv')))\n",
    "# iterate over file names\n",
    "for feature_importances_fn in feature_importances_fns:\n",
    "\n",
    "    # load feature importances from file\n",
    "    feature_importances = pd.read_csv(feature_importances_fn)\n",
    "    subregion_name = os.path.basename(feature_importances_fn).split('best_model_')[1].split('_feature')[0]\n",
    "    subregion_name = (subregion_name.replace('NR', 'N. R').replace('NC', 'N. C').replace('SR', 'S. R').replace('SC', 'S. C')\n",
    "                      .replace('CR', 'C. R').replace('StEliasMtns', 'St. Elias Mtns.').replace('CoastR', 'Coast R')\n",
    "                      .replace('WChugachMtns', 'W. Chugach Mtns.').replace('AlaskaR', 'Alaska R'))\n",
    "    \n",
    "    # determine which axis to plot on\n",
    "    if subregion_name=='Aleutians':\n",
    "        j = 0\n",
    "        color = '#6d9c43'\n",
    "    elif subregion_name=='Alaska Range':\n",
    "        j = 3\n",
    "        color = '#1f78b4'\n",
    "    elif subregion_name=='W. Chugach Mtns.':\n",
    "        j = 2\n",
    "        color = '#264708'\n",
    "    elif subregion_name=='St. Elias Mtns.':\n",
    "        j = 4\n",
    "        color = '#fb9a99'\n",
    "    elif subregion_name=='N. Coast Ranges':\n",
    "        j = 6\n",
    "        color = '#e31a1c'\n",
    "    elif subregion_name=='N. Rockies':\n",
    "        j = 7\n",
    "        color = '#cab2d6'\n",
    "    elif subregion_name=='N. Cascades':\n",
    "        j = 8\n",
    "        color = '#fdbf6f'\n",
    "    elif subregion_name=='C. Rockies':\n",
    "        j = 9\n",
    "        color = '#9657d9'\n",
    "    elif subregion_name=='S. Cascades':\n",
    "        j = 10\n",
    "        color = '#ff7f00'\n",
    "    elif subregion_name=='S. Rockies':\n",
    "        j = 11\n",
    "        color = '#6a3d9a'\n",
    "    elif subregion_name=='all_regions':\n",
    "        j = 1\n",
    "        color = 'k'\n",
    "        subregion_name = 'All regions'\n",
    "\n",
    "    # plot\n",
    "    boxprops = dict(color='k',linewidth=1)\n",
    "    medianprops = dict(color=\"#d9d9d9\",linewidth=2)\n",
    "    whiskerprops=dict(color='k', linewidth=2)\n",
    "    capprops=dict(color='k', linewidth=2)\n",
    "    bplot = ax[j].boxplot(feature_importances, showfliers=False, \n",
    "                          boxprops=boxprops, medianprops=medianprops, \n",
    "                          whiskerprops=whiskerprops, capprops=capprops, patch_artist=True)\n",
    "    for patch in bplot['boxes']:\n",
    "        patch.set_facecolor(color)          \n",
    "    if j >= 10:\n",
    "        ax[j].set_xticklabels(feature_columns_display)\n",
    "    else:\n",
    "        ax[j].set_xticklabels([])\n",
    "    ax[j].set_ylim(-0.01, np.nanmax(feature_importances) * 1.1)\n",
    "    ax[j].text(5.4, \n",
    "              (ax[j].get_ylim()[1] - ax[j].get_ylim()[0])*0.85 + ax[j].get_ylim()[0],\n",
    "               subregion_name, horizontalalignment='right',\n",
    "               bbox=dict(facecolor='white', edgecolor='black', pad=3))\n",
    "    ax[j].grid()\n",
    "\n",
    "fig.delaxes(ax[5])\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'feature_importances.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45065bb",
   "metadata": {},
   "source": [
    "## Conduct simulations for assessing regional AAR response to changes in terrain parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f930ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Determine which columns to perturb\n",
    "columns = ['Area', 'Zmed', 'Hypsometric_Index', 'Slope', 'Aspect']\n",
    "columns_display = ['Area [km$^2$]', 'Z$_{med}$ [m]', 'Hypsometric Index', 'Slope [degrees]', 'Aspect [degrees]']\n",
    "\n",
    "# -----Grab model names\n",
    "# model_fns = sorted(glob.glob(os.path.join(base_path, 'inputs-outputs', 'best_model*.joblib')))\n",
    "model_fns = glob.glob(os.path.join(base_path, 'inputs-outputs', 'best_model_all_regions_tuned.joblib'))\n",
    "\n",
    "# -----Add subregion and color column to training data\n",
    "training_data_df[['Subregion', 'Color']] = '', ''\n",
    "for o1region, o2region in training_data_subset_df[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    training_data_subset_df.loc[(training_data_subset_df['O1Region']==o1region) \n",
    "                                 & (training_data_subset_df['O2Region']==o2region), 'Subregion'] = subregion_name\n",
    "    training_data_subset_df.loc[(training_data_subset_df['O1Region']==o1region) \n",
    "                                 & (training_data_subset_df['O2Region']==o2region), 'Color'] = color\n",
    "\n",
    "# -----Set up figure\n",
    "plt.rcParams.update({'font.size':14, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(2, 3, figsize=(14,10))\n",
    "ax = ax.flatten()\n",
    "\n",
    "# -----Iterate over models\n",
    "for model_fn in model_fns:\n",
    "    # determine subregion name\n",
    "    subregion_name = os.path.basename(model_fn).split('best_model_')[1].split('.joblib')[0]\n",
    "    subregion_name = (subregion_name.replace('NR', 'N. R').replace('NC', 'N. C').replace('SR', 'S. R').replace('SC', 'S. C')\n",
    "                      .replace('CR', 'C. R').replace('StEliasMtns', 'St. Elias Mtns.').replace('CoastR', 'Coast R')\n",
    "                      .replace('WChugachMtns', 'W. Chugach Mtns.').replace('AlaskaR', 'Alaska R'))\n",
    "        \n",
    "    # load model\n",
    "    model = load(model_fn)\n",
    "\n",
    "    # create dataframe with mean conditions for subregion\n",
    "    if 'all_regions' in subregion_name:\n",
    "        training_data_subregion_df = training_data_subset_df.copy()\n",
    "        color = 'k'\n",
    "    else:\n",
    "        training_data_subregion_df = training_data_subset_df.loc[training_data_subset_df['Subregion']==subregion_name]\n",
    "        color = training_data_subregion_df['Color'].values[0]\n",
    "    input_df = pd.DataFrame()\n",
    "    for column in feature_columns:\n",
    "        input_df[column] = [training_data_subregion_df[column].mean()]\n",
    "    \n",
    "    # iterate over columns to perturb\n",
    "    for i, column in enumerate(columns):\n",
    "        # determine range of column values to test\n",
    "        column_range = sorted(training_data_subset_df[column].drop_duplicates().values)\n",
    "        # iterate over column range\n",
    "        aars = np.zeros(len(column_range))\n",
    "        for j, value in enumerate(column_range):\n",
    "            # set column to value in range\n",
    "            input_df[column] = value\n",
    "            # predict AAR with model\n",
    "            aars[j] = model.predict(input_df[feature_columns])\n",
    "\n",
    "        # plot results\n",
    "        if 'all_regions' in subregion_name:\n",
    "            linewidth=1\n",
    "            label='All regions'\n",
    "            markerstyle='o'\n",
    "            linestyle='--'\n",
    "        # else:\n",
    "        #     linewidth=2\n",
    "        #     label=subregion_name\n",
    "        #     markerstyle='o'\n",
    "        #     linestyle='--'\n",
    "        ax[i].plot(column_range, aars, marker=markerstyle, color=color, markersize=5,\n",
    "                   linestyle=linestyle, linewidth=linewidth, label=label)\n",
    "\n",
    "# add axes labels\n",
    "for axis, column_display in zip(ax, columns_display):\n",
    "    axis.set_xlabel(column_display)\n",
    "    axis.set_ylim(0.4, 0.8)\n",
    "    axis.set_yticks([0.4, 0.5, 0.6, 0.7, 0.8])\n",
    "    axis.grid()\n",
    "ax[0].set_ylabel('AAR')\n",
    "ax[3].set_ylabel('AAR')\n",
    "# ax[2].legend(loc='center right', bbox_to_anchor=[0.7, -0.8, 0.2, 0.2])\n",
    "fig.delaxes(ax[-1])\n",
    "plt.show()\n",
    "    \n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'model_AAR_predictions_feature_column_perturbations.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5d723c",
   "metadata": {},
   "source": [
    "## Conduct simulations for assessing regional sensitivity to climate perturbations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9f15c9",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # -----Iterate over unique subregions\n",
    "# perturb_df = pd.DataFrame()\n",
    "# for o1region, o2region in unique_subregions:\n",
    "    \n",
    "#     # Subset training data to subregion\n",
    "#     snowlines_subregion = training_data_df.loc[(training_data_df['O1Region']==o1region) & (training_data_df['O2Region']==o2region)]\n",
    "#     subregion_name, color = f.determine_subregion_name_color(float(o1region), float(o2region))\n",
    "#     print(subregion_name)\n",
    "\n",
    "#     # Construct input data\n",
    "#     input_df = pd.DataFrame()\n",
    "#     for column in feature_columns:\n",
    "#         input_df[column] = [snowlines_subregion[column].drop_duplicates().mean()]\n",
    "#     # predict AAR using input data\n",
    "#     clf_fn = os.path.join(base_path, 'inputs-outputs', 'best_classifier_' + subregion_name + '.joblib')\n",
    "#     clf = load(clf_fn)\n",
    "#     AAR_mean = clf.predict(input_df)[0]\n",
    "    \n",
    "#     # predict AAR if 150 additional PDDs\n",
    "#     input_adj_df = input_df.copy()\n",
    "#     input_adj_df['Cumulative_Positive_Degree_Days'] = input_adj_df['Cumulative_Positive_Degree_Days'] + 150\n",
    "#     AAR_PDD_perturb = clf.predict(input_adj_df)[0]\n",
    "\n",
    "#     # predict AAR if - 10% cumulative snow\n",
    "#     input_adj_df = input_df.copy()\n",
    "#     input_adj_df['Cumulative_Snowfall_mwe'] = input_adj_df['Cumulative_Snowfall_mwe'] * 0.9\n",
    "#     AAR_snowfall_perturb = clf.predict(input_adj_df)[0]\n",
    "\n",
    "#     # compile in dataframe\n",
    "#     df = pd.DataFrame({'Subregion Name': [subregion_name],\n",
    "#                        'Mean conditions AAR': [AAR_mean],\n",
    "#                        '+150 PPDs AAR': [AAR_PDD_perturb],\n",
    "#                        '+150 PDDs AAR % change': [(AAR_mean-AAR_PDD_perturb)/AAR_mean * 100],\n",
    "#                        '-10% snowfall AAR': [AAR_snowfall_perturb],\n",
    "#                        '-10% snowfall AAR % change': [(AAR_mean-AAR_snowfall_perturb)/AAR_mean * 100]\n",
    "#                       })\n",
    "#     perturb_df = pd.concat([perturb_df, df])\n",
    "\n",
    "# perturb_df.reset_index(drop=True, inplace=True)\n",
    "# perturb_df['+150 PDDs AAR % change'] = perturb_df['+150 PDDs AAR % change'].apply(np.round)\n",
    "# perturb_df['-10% snowfall AAR % change'] = perturb_df['-10% snowfall AAR % change'].apply(np.round)\n",
    "\n",
    "# perturb_df[['Subregion Name', '+150 PDDs AAR % change', '-10% snowfall AAR % change']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47583ff",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# # Plot PDDs and AAR for one site as an example\n",
    "# site_name = 'SouthCascade'\n",
    "# training_site = training_data_df.loc[training_data_df['site_name']==site_name]\n",
    "# training_site = training_site.sort_values(by='datetime')\n",
    "\n",
    "# # load ERA data\n",
    "# ERA_fn = glob.glob(os.path.join(study_sites_path, site_name, 'ERA', '*.csv'))[0]\n",
    "# ERA = pd.read_csv(ERA_fn)\n",
    "# ERA['Date'] = pd.to_datetime(ERA['Date'])\n",
    "\n",
    "# plt.rcParams.update({'font.size':16, 'font.sans-serif':'Arial'})\n",
    "# fig, ax = plt.subplots(2, 1, figsize=(12,8))\n",
    "# # AAR\n",
    "# ax[0].plot(training_site['datetime'], training_site['AAR'], '.k')\n",
    "# ax[0].grid()\n",
    "# ax[0].set_xlim(np.datetime64('2016-01-01'), np.datetime64('2023-01-01'))\n",
    "# ax[0].set_ylabel('AAR')\n",
    "# ax[0].set_title('South Cascade Glacier')\n",
    "\n",
    "# # PDDs and snowfall\n",
    "# ax[1].bar(ERA['Date'], ERA['Cumulative_Snowfall_mwe'], color='#4eb3d3', width=1)\n",
    "# ax[1].set_ylabel('$\\Sigma$ Snowfall [m.w.e.]', color='#4eb3d3')\n",
    "# ax[1].set_xlim(np.datetime64('2016-01-01'), np.datetime64('2023-01-01'))\n",
    "# ax[1].grid()\n",
    "# ax[1].tick_params(axis='y', colors='#4eb3d3')\n",
    "# ax2 = ax[1].twinx()\n",
    "# ax2.plot(ERA['Date'], ERA['Cumulative_Positive_Degree_Days'], '.m')\n",
    "# ax2.set_ylabel('$\\Sigma$ Positive degree days', color='m')\n",
    "# ax2.tick_params(axis='y', colors='m')\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # save figure\n",
    "# fig_fn = os.path.join(figures_out_path, 'example_time_series_SouthCascadeGlacier.png')\n",
    "# fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "# print('figure saved to file: ' + fig_fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
