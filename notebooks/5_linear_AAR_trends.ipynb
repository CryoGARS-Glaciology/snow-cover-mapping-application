{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8440394-6f37-4aec-a623-f1cd0c7e908d",
   "metadata": {},
   "source": [
    "# Fit linear trendline to AARs, PDDs, and snowfall for all sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1284a7b5-8635-4e3f-971e-17cbd235b4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.stats import iqr\n",
    "# Suppress warnings to prevent kernel crashing (future warning from pandas)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04adc2af-5496-415d-9138-68664f71c590",
   "metadata": {},
   "source": [
    "## Define paths in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c81e5fa5-1c99-4707-9d90-dc0f2bceb007",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f\n",
    "\n",
    "# scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "figures_out_path = os.path.join(base_path, 'figures')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c733ea5f-506f-40f4-82d3-876427738009",
   "metadata": {},
   "source": [
    "## Load compiled glacier boundaries and snowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748974c3-8627-4b76-a5ea-afff7c5ed7cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load glacier boundaries with climate clusters\n",
    "aois_fn = os.path.join(scm_path, 'compiled_data', 'all_aois_climate_cluster.shp')\n",
    "aois = gpd.read_file(aois_fn)\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All AOIs with climate clusters loaded from file.')\n",
    "\n",
    "# -----Load ERA data\n",
    "era_fn = os.path.join(scm_path, 'compiled_data', 'all_era_data.csv')\n",
    "era = pd.read_csv(era_fn)\n",
    "# format dates as datetimes\n",
    "era['Date'] = pd.to_datetime(era['Date'])\n",
    "print('All ERA data loaded from file.')\n",
    "\n",
    "# -----Load and compile snowlines\n",
    "snowlines_fn = os.path.join(scm_path, 'compiled_data', 'all_snowlines.csv')\n",
    "snowlines = pd.read_csv(snowlines_fn)\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "print('All snowlines loaded from file.')\n",
    "# snowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f42c5-2e36-49a0-952b-b7f7aa86b7dc",
   "metadata": {},
   "source": [
    "## Filter snowlines to before September, merge snowlines and ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f48cc1-9a25-4f44-8d99-214b431bb3e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Add Month column to snowlines\n",
    "snowlines['Month'] = pd.DatetimeIndex(snowlines['datetime']).month.values\n",
    "# Remove observations after August\n",
    "snowlines_filt = snowlines.loc[snowlines['Month'] <= 8]\n",
    "# Grab ERA data on dates\n",
    "snowlines_filt[['Cumulative_Snowfall_mwe', 'Cumulative_Positive_Degree_Days']] = '', ''\n",
    "for site_name in tqdm(snowlines_filt['site_name'].drop_duplicates().values):\n",
    "    snowlines_site = snowlines_filt.loc[snowlines_filt['site_name']==site_name]\n",
    "    snowlines_site.sort_values(by='datetime', inplace=True)\n",
    "    era_site = era.loc[era['site_name']==site_name]\n",
    "    merged = pd.merge_asof(snowlines_site, era_site, left_on='datetime', right_on='Date')\n",
    "    snowlines_filt.loc[snowlines_filt['site_name']==site_name, 'Cumulative_Snowfall_mwe'] = merged['Cumulative_Snowfall_mwe_y']\n",
    "    snowlines_filt.loc[snowlines_filt['site_name']==site_name, 'Cumulative_Positive_Degree_Days'] = merged['Cumulative_Positive_Degree_Days_y']\n",
    "snowlines_filt = snowlines_filt.loc[snowlines_filt['Cumulative_Snowfall_mwe']!='']\n",
    "snowlines_filt.reset_index(drop=True, inplace=True)\n",
    "snowlines_filt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29b699b-6118-4fed-bc5c-061ad9205cf0",
   "metadata": {},
   "source": [
    "## Fit linear and non-parametric models to PDDs and Snowfall vs. AARs for each subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d5652a-7b2b-43d2-b68d-df2e421d85c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define non-parametric fit function\n",
    "def svr_fit(X, y):\n",
    "    model = SVR().fit(X, y)\n",
    "    score = model.score(X, y)\n",
    "    return model, score\n",
    "\n",
    "# Define linear fit function\n",
    "def linear_fit(X, y):\n",
    "    model = LinearRegression().fit(X,y)\n",
    "    score = model.score(X, y)\n",
    "    return model, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacacc71-11df-44b0-9dfb-966d06e04a25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize dataframe for storing results\n",
    "fit_df = pd.DataFrame()\n",
    "\n",
    "# Iterate over subregions \n",
    "for o1, o2 in tqdm(aois[['O1Region', 'O2Region']].drop_duplicates().values):\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1, o2)\n",
    "    # Subset snowlines to subregion\n",
    "    site_names = aois.loc[(aois['O1Region']==o1) & (aois['O2Region']==o2), 'RGIId'].drop_duplicates().values\n",
    "    snowlines_subregion = snowlines_filt[snowlines_filt['site_name'].isin(site_names)]\n",
    "    # Fit linear and SVR models to data\n",
    "    X = snowlines_subregion_filt[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].values\n",
    "    y = snowlines_subregion_filt['AAR'].values\n",
    "    if (np.ravel(X)=='').all():\n",
    "        model_linear, score_linear = np.nan, np.nan\n",
    "        model_svr, score_svr = np.nan, np.nan\n",
    "    else:\n",
    "        model_linear, score_linear = linear_fit(X, y)\n",
    "        model_svr, score_svr = svr_fit(X, y)\n",
    "        # plot\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(X[:,0], y, '.')\n",
    "        plt.plot(X[:,0], model_linear.predict(X), '.b', label='Linear')\n",
    "        plt.plot(X[:,0], model_svr.predict(X), '.m', label='SVR')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.xlabel('$\\Sigma$PDDs')\n",
    "        plt.ylabel('AAR')\n",
    "        plt.ylim(0,1)\n",
    "        plt.title(subregion_name \n",
    "                  + '\\nLinear score = ' + str(np.round(score_linear, 4)) \n",
    "                  + '\\nSVR score = ' + str(np.round(score_svr, 4)))\n",
    "        plt.show()\n",
    "    # Save in dataframe\n",
    "    df = pd.DataFrame({'Subregion': [subregion_name],\n",
    "                       'coef_linear': [model_linear.coef_[0]],\n",
    "                       'score_linear': [score_linear],\n",
    "                       'score_svr': [score_svr],\n",
    "                       'N': [len(y)]})\n",
    "    # Concatenate to full dataframe\n",
    "    fit_df = pd.concat([fit_df, df])\n",
    "    \n",
    "fit_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffc96d4-d3ff-4dc0-870f-b47763b71e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "snowlines_subregion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b419368-a289-4bac-adc5-9c9e43603fe5",
   "metadata": {},
   "source": [
    "## Fit linear and non-parametric models to PDDs and Snowfall vs. AARs for each climate cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b2eb6d-7447-4cc4-8cd1-cfadc466184d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f0aafad-a4fd-460a-93c1-95a28f82eb8a",
   "metadata": {},
   "source": [
    "## Fit a linear trend to PDDs and Snowfall vs. AARs for each site separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83c1180-f410-479d-b03e-21c7612987b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppress warnings to prevent kernel crashing (future warning from pandas)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "run_MC = False # whether to fit linear trendlines using Monte Carlo bootstrapping\n",
    "\n",
    "# Load model training data file names\n",
    "training_data_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'training_data_*.csv')))\n",
    "training_data_fns = [x for x in training_data_fns if '_scaled' not in x]\n",
    "\n",
    "# Initialize results dataframe\n",
    "aar_pdd_linear_df = pd.DataFrame()\n",
    "\n",
    "# Define linear fit function\n",
    "def linear_fit(X, y, plot=False):\n",
    "    # fit model to data\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    # save stats\n",
    "    coef = model.coef_\n",
    "    intercept = model.intercept_\n",
    "    score = model.score(X, y)\n",
    "    # plot\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(subplot_kw={'projection': '3d'})\n",
    "        ax.plot(X[:,0], X[:,1], y, '.k', markersize=1)\n",
    "        ax.plot(X[:,0], X[:,1], model.predict(X), '-k')\n",
    "        ax.set_xlabel('$\\Sigma$PDDs')\n",
    "        ax.set_ylabel('$\\Sigma$Snowfall [m.w.e.]')\n",
    "        ax.set_zlabel('AAR')\n",
    "        ax.grid()\n",
    "        ax.set_title(score)\n",
    "        plt.show()\n",
    "    \n",
    "    return coef, intercept, score\n",
    "\n",
    "# Define Monte Carlo simulation function\n",
    "def monte_carlo_linear_fit(X, y, n=100, ptest=0.8):\n",
    "    # initialize metrics\n",
    "    coefs = []  # linear fit coefficients\n",
    "    intercepts = []  # linear fit coefficients\n",
    "    scores = []  # scores (R^2)\n",
    "    # iterate over MC simulations\n",
    "    for i in range(0,n):\n",
    "        # split into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ptest)\n",
    "        # fit model to data\n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "        # grab parameters from model\n",
    "        coefs.append(model.coef_)\n",
    "        intercepts.append(model.intercept_)\n",
    "        scores.append(model.score(X, y))\n",
    "    # save stats for each\n",
    "    coef_median, coef_iqr = np.nanmedian(coefs), iqr(coefs)\n",
    "    intercept_median, intercept_iqr = np.nanmedian(intercepts), iqr(intercepts)\n",
    "    score_median, score_iqr = np.nanmedian(scores), iqr(scores)\n",
    "    \n",
    "    return coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr\n",
    "\n",
    "# Iterate over subregion file names\n",
    "for training_data_fn in tqdm(training_data_fns):\n",
    "    # Load training data\n",
    "    training_data = pd.read_csv(training_data_fn)\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name = os.path.basename(training_data_fn).split('training_data_')[1].split('.csv')[0]\n",
    "    # Iterate over sites\n",
    "    for site_name in training_data['site_name'].drop_duplicates().values:\n",
    "        # Subset training data\n",
    "        training_data_site = training_data.loc[training_data['site_name']==site_name]\n",
    "        # Fit linear trendline to AAR and Cumulative PDDs using Monte Carlo simulations\n",
    "        X = training_data_site[['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']].values.reshape(-1, 2)\n",
    "        y = training_data_site['AAR']\n",
    "        if run_MC:\n",
    "            if (np.ravel(X)==0).all():\n",
    "                coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "            else:\n",
    "                coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr = monte_carlo_linear_fit(X, y)\n",
    "            # Save in dataframe\n",
    "            df = pd.DataFrame({'site_name': [site_name],\n",
    "                               'Subregion': [subregion_name],\n",
    "                               'coef_median': [coef_median],\n",
    "                               'coef_iqr': [coef_iqr],\n",
    "                               'intercept_median': [intercept_median],\n",
    "                               'intercept_iqr': [intercept_iqr],\n",
    "                               'score_median': [score_median],\n",
    "                               'score_iqr': [score_iqr],\n",
    "                               'N': [len(training_data_site)]})\n",
    "        else:\n",
    "            if (np.ravel(X)==0).all():\n",
    "                coef, intercept, score = np.nan, np.nan, np.nan\n",
    "            else:\n",
    "                coef, intercept, score = linear_fit(X, y)\n",
    "            # Save in dataframe\n",
    "            df = pd.DataFrame({'site_name': [site_name],\n",
    "                               'Subregion': [subregion_name],\n",
    "                               'coef_PDD': [coef[0]],\n",
    "                               'coef_snowfall': [coef[1]],\n",
    "                               'intercept': [intercept],\n",
    "                               'score': [score],\n",
    "                               'N': [len(training_data_site)]})\n",
    "        # Concatenate to full dataframe\n",
    "        aar_pdd_linear_df = pd.concat([aar_pdd_linear_df, df])\n",
    "\n",
    "# Save results\n",
    "if run_MC:\n",
    "    aar_pdd_linear_fn = os.path.join(scm_path, 'results', 'aar_pdd_snowfall_linear_fit_MC.csv')\n",
    "else:\n",
    "    aar_pdd_linear_fn = os.path.join(scm_path, 'results', 'aar_pdd_snowfall_linear_fit.csv')\n",
    "aar_pdd_linear_df.to_csv(aar_pdd_linear_fn, index=False)\n",
    "print('Data table saved to file:', aar_pdd_linear_fn)\n",
    "aar_pdd_linear_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b9d64ed-ca1a-4a27-896f-cfef7ada318a",
   "metadata": {},
   "source": [
    "## Fit a linear trend to max. PDDs and max. Snowfall vs. min. AARs for each site separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb8900b7-5774-4887-acd8-d4efc54aca8e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Suppress warnings to prevent kernel crashing (future warning from pandas)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "run_MC = False # whether to fit linear trendlines using Monte Carlo bootstrapping\n",
    "\n",
    "# Load model training data file names\n",
    "training_data_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'training_data_*.csv')))\n",
    "training_data_fns = [x for x in training_data_fns if '_scaled' not in x]\n",
    "\n",
    "# Initialize results dataframe\n",
    "aar_pdd_linear_df = pd.DataFrame()\n",
    "\n",
    "# Define linear fit function\n",
    "def linear_fit(X, y, plot=False):\n",
    "    # fit model to data\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    # save stats\n",
    "    coef = model.coef_\n",
    "    intercept = model.intercept_\n",
    "    score = model.score(X, y)\n",
    "    # plot\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(subplot_kw={'projection': '3d'})\n",
    "        ax.plot(X[:,0], X[:,1], y, '.k', markersize=1)\n",
    "        ax.plot(X[:,0], X[:,1], model.predict(X), '-k')\n",
    "        ax.set_xlabel('$\\Sigma$PDDs')\n",
    "        ax.set_ylabel('$\\Sigma$Snowfall [m.w.e.]')\n",
    "        ax.set_zlabel('AAR')\n",
    "        ax.grid()\n",
    "        ax.set_title(score)\n",
    "        plt.show()\n",
    "    \n",
    "    return coef, intercept, score\n",
    "\n",
    "# Define Monte Carlo simulation function\n",
    "def monte_carlo_linear_fit(X, y, n=100, ptest=0.8):\n",
    "    # initialize metrics\n",
    "    coefs = []  # linear fit coefficients\n",
    "    intercepts = []  # linear fit coefficients\n",
    "    scores = []  # scores (R^2)\n",
    "    # iterate over MC simulations\n",
    "    for i in range(0,n):\n",
    "        # split into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ptest)\n",
    "        # fit model to data\n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "        # grab parameters from model\n",
    "        coefs.append(model.coef_)\n",
    "        intercepts.append(model.intercept_)\n",
    "        scores.append(model.score(X, y))\n",
    "    # save stats for each\n",
    "    coef_median, coef_iqr = np.nanmedian(coefs), iqr(coefs)\n",
    "    intercept_median, intercept_iqr = np.nanmedian(intercepts), iqr(intercepts)\n",
    "    score_median, score_iqr = np.nanmedian(scores), iqr(scores)\n",
    "    \n",
    "    return coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr\n",
    "\n",
    "# Iterate over subregion file names\n",
    "for training_data_fn in tqdm(training_data_fns):\n",
    "    # Load training data\n",
    "    training_data = pd.read_csv(training_data_fn)\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name = os.path.basename(training_data_fn).split('training_data_')[1].split('.csv')[0]\n",
    "    # Iterate over sites\n",
    "    for site_name in training_data['site_name'].drop_duplicates().values:\n",
    "        # Subset training data\n",
    "        training_data_site = training_data.loc[training_data['site_name']==site_name]\n",
    "        # Add year and WOY columns\n",
    "        training_data_site['Date'] = pd.to_datetime(training_data_site['Date'])\n",
    "        training_data_site['Year'] = training_data_site['Date'].dt.isocalendar().year\n",
    "        training_data_site['WOY'] = training_data_site['Date'].dt.isocalendar().week\n",
    "        # Fit linear trendline to min. AAR, max. PDDs, and min. snowfall using Monte Carlo simulations\n",
    "        X = training_data_site.groupby('Year')[['Cumulative_Positive_Degree_Days', \n",
    "                                        'Cumulative_Snowfall_mwe']].max().values\n",
    "        y = training_data_site.groupby('Year')['AAR'].min().values\n",
    "        if run_MC:\n",
    "            if (np.ravel(X)==0).all():\n",
    "                coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "            else:\n",
    "                coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr = monte_carlo_linear_fit(X, y)\n",
    "            # Save in dataframe\n",
    "            df = pd.DataFrame({'site_name': [site_name],\n",
    "                               'Subregion': [subregion_name],\n",
    "                               'coef_median': [coef_median],\n",
    "                               'coef_iqr': [coef_iqr],\n",
    "                               'intercept_median': [intercept_median],\n",
    "                               'intercept_iqr': [intercept_iqr],\n",
    "                               'score_median': [score_median],\n",
    "                               'score_iqr': [score_iqr],\n",
    "                               'N': [len(y)]})\n",
    "        else:\n",
    "            if (np.ravel(X)==0).all():\n",
    "                coef, intercept, score = np.nan, np.nan, np.nan\n",
    "            else:\n",
    "                coef, intercept, score = linear_fit(X, y)\n",
    "            # Save in dataframe\n",
    "            df = pd.DataFrame({'site_name': [site_name],\n",
    "                               'Subregion': [subregion_name],\n",
    "                               'coef_PDD': [coef[0]],\n",
    "                               'coef_snowfall': [coef[1]],\n",
    "                               'intercept': [intercept],\n",
    "                               'score': [score],\n",
    "                               'N': [len(training_data_site)]})\n",
    "        # Concatenate to full dataframe\n",
    "        aar_pdd_linear_df = pd.concat([aar_pdd_linear_df, df])\n",
    "\n",
    "# # Save results\n",
    "# if run_MC:\n",
    "#     aar_pdd_linear_fn = os.path.join(scm_path, 'results', 'aar_pdd_snowfall_linear_fit_MC.csv')\n",
    "# else:\n",
    "#     aar_pdd_linear_fn = os.path.join(scm_path, 'results', 'aar_pdd_snowfall_linear_fit.csv')\n",
    "# aar_pdd_linear_df.to_csv(aar_pdd_linear_fn, index=False)\n",
    "# print('Data table saved to file:', aar_pdd_linear_fn)\n",
    "# aar_pdd_linear_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70788d2-19df-4a16-8bc2-7fc76fcf7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "aar_pdd_linear_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4969c18-8088-4c22-8da8-ac078095a522",
   "metadata": {},
   "source": [
    "## Fit a linear trend to max. PDDs and max. Snowfall vs. min. AARs for each subregion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05688685-1e8f-40cf-a9e4-d4cb680dd30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings to prevent kernel crashing (future warning from pandas)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "run_MC = False # whether to fit linear trendlines using Monte Carlo bootstrapping\n",
    "\n",
    "# Load model training data file names\n",
    "training_data_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'training_data_*.csv')))\n",
    "training_data_fns = [x for x in training_data_fns if '_scaled' not in x]\n",
    "\n",
    "# Initialize results dataframe\n",
    "aar_pdd_linear_df = pd.DataFrame()\n",
    "\n",
    "# Define linear fit function\n",
    "def linear_fit(X, y, plot=False):\n",
    "    # fit model to data\n",
    "    model = LinearRegression().fit(X, y)\n",
    "    # save stats\n",
    "    coef = model.coef_\n",
    "    intercept = model.intercept_\n",
    "    score = model.score(X, y)\n",
    "    # plot\n",
    "    if plot:\n",
    "        fig, ax = plt.subplots(subplot_kw={'projection': '3d'})\n",
    "        ax.plot(X[:,0], X[:,1], y, '.k', markersize=1)\n",
    "        ax.plot(X[:,0], X[:,1], model.predict(X), '-k')\n",
    "        ax.set_xlabel('$\\Sigma$PDDs')\n",
    "        ax.set_ylabel('$\\Sigma$Snowfall [m.w.e.]')\n",
    "        ax.set_zlabel('AAR')\n",
    "        ax.grid()\n",
    "        ax.set_title(score)\n",
    "        plt.show()\n",
    "    \n",
    "    return coef, intercept, score\n",
    "\n",
    "# Define Monte Carlo simulation function\n",
    "def monte_carlo_linear_fit(X, y, n=100, ptest=0.8):\n",
    "    # initialize metrics\n",
    "    coefs = []  # linear fit coefficients\n",
    "    intercepts = []  # linear fit coefficients\n",
    "    scores = []  # scores (R^2)\n",
    "    # iterate over MC simulations\n",
    "    for i in range(0,n):\n",
    "        # split into training and testing sets\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=ptest)\n",
    "        # fit model to data\n",
    "        model = LinearRegression().fit(X_train, y_train)\n",
    "        # grab parameters from model\n",
    "        coefs.append(model.coef_)\n",
    "        intercepts.append(model.intercept_)\n",
    "        scores.append(model.score(X, y))\n",
    "    # save stats for each\n",
    "    coef_median, coef_iqr = np.nanmedian(coefs), iqr(coefs)\n",
    "    intercept_median, intercept_iqr = np.nanmedian(intercepts), iqr(intercepts)\n",
    "    score_median, score_iqr = np.nanmedian(scores), iqr(scores)\n",
    "    \n",
    "    return coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr\n",
    "\n",
    "# Iterate over subregion file names\n",
    "for training_data_fn in tqdm(training_data_fns):\n",
    "    # Load training data\n",
    "    training_data = pd.read_csv(training_data_fn)\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name = os.path.basename(training_data_fn).split('training_data_')[1].split('.csv')[0]\n",
    "        \n",
    "    # Add year and WOY columns\n",
    "    training_data['Date'] = pd.to_datetime(training_data['Date'])\n",
    "    training_data['Year'] = training_data['Date'].dt.isocalendar().year\n",
    "    training_data['WOY'] = training_data['Date'].dt.isocalendar().week\n",
    "    # Fit linear trendline to min. AAR, max. PDDs, and min. snowfall using Monte Carlo simulations\n",
    "    X = training_data.groupby(['Year', 'WOY'])[['Cumulative_Positive_Degree_Days', \n",
    "                                                'Cumulative_Snowfall_mwe']].median().reset_index().groupby('Year')[['Cumulative_Positive_Degree_Days', \n",
    "                                                                                                                    'Cumulative_Snowfall_mwe']].max().values\n",
    "    y = training_data.groupby(['Year', 'WOY'])['AAR'].median().reset_index().groupby('Year')['AAR'].min().values\n",
    "    if run_MC:\n",
    "        if (np.ravel(X)==0).all():\n",
    "            coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr = np.nan, np.nan, np.nan, np.nan, np.nan, np.nan\n",
    "        else:\n",
    "            coef_median, coef_iqr, intercept_median, intercept_iqr, score_median, score_iqr = monte_carlo_linear_fit(X, y)\n",
    "        # Save in dataframe\n",
    "        df = pd.DataFrame({'Subregion': [subregion_name],\n",
    "                           'coef_median': [coef_median],\n",
    "                           'coef_iqr': [coef_iqr],\n",
    "                           'intercept_median': [intercept_median],\n",
    "                           'intercept_iqr': [intercept_iqr],\n",
    "                           'score_median': [score_median],\n",
    "                           'score_iqr': [score_iqr],\n",
    "                           'N': [len(y)]})\n",
    "    else:\n",
    "        if (np.ravel(X)==0).all():\n",
    "            coef, intercept, score = np.nan, np.nan, np.nan\n",
    "        else:\n",
    "            coef, intercept, score = linear_fit(X, y)\n",
    "        # Save in dataframe\n",
    "        df = pd.DataFrame({'Subregion': [subregion_name],\n",
    "                           'coef_PDD': [coef[0]],\n",
    "                           'coef_snowfall': [coef[1]],\n",
    "                           'intercept': [intercept],\n",
    "                           'score': [score],\n",
    "                           'N': [len(y)]})\n",
    "    # Concatenate to full dataframe\n",
    "    aar_pdd_linear_df = pd.concat([aar_pdd_linear_df, df])\n",
    "aar_pdd_linear_df\n",
    "# # Save results\n",
    "# if run_MC:\n",
    "#     aar_pdd_linear_fn = os.path.join(scm_path, 'results', 'aar_pdd_snowfall_linear_fit_MC.csv')\n",
    "# else:\n",
    "#     aar_pdd_linear_fn = os.path.join(scm_path, 'results', 'aar_pdd_snowfall_linear_fit.csv')\n",
    "# aar_pdd_linear_df.to_csv(aar_pdd_linear_fn, index=False)\n",
    "# print('Data table saved to file:', aar_pdd_linear_fn)\n",
    "# aar_pdd_linear_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30acf071-65a9-4ecf-a740-241ee879b2ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d8696f2-2fa5-4dfc-8da6-d0e79ce86ea4",
   "metadata": {},
   "source": [
    "## For each site, identify annual AARs, fit linear trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f533432-1fd1-48ed-b794-ef9b056050d7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # -----Check if already exists in directory\n",
    "# min_aars_fn = os.path.join(scm_path, 'results', 'minimum_AARs_linear_fit.csv') \n",
    "# if not os.path.exists(min_aars_fn):\n",
    "    \n",
    "#     # -----For each site, identify minimum AARs, fit linear trendlines\n",
    "#     # add year column\n",
    "#     snowlines['Year'] = snowlines['datetime'].dt.isocalendar().year\n",
    "#     # subset to dates after 2015\n",
    "#     snowlines_subset = snowlines.loc[snowlines['Year'] > 2016]\n",
    "#     # initialize dataframe for storing minimum AARs and linear fits\n",
    "#     min_aars_df = pd.DataFrame()\n",
    "#     # iterate over site names\n",
    "#     for site_name in tqdm(snowlines['site_name'].drop_duplicates().values):\n",
    "#         # subset snowlines to site\n",
    "#         snowlines_site = snowlines.loc[snowlines['site_name']==site_name]\n",
    "#         # count number of AAR estimates for each year\n",
    "#         count_df = snowlines_site.groupby(['Year'])['AAR'].count()\n",
    "#         # extract minimum AARs and dates\n",
    "#         min_aars = snowlines_site.groupby(['Year'])['AAR'].min()\n",
    "#         min_aars = min_aars.to_frame()\n",
    "#         min_aars['count'] = count_df.values\n",
    "#         # remove years with less than 20 observations from fit estimates\n",
    "#         min_aars = min_aars.loc[min_aars['count'] >= 20]\n",
    "#         # check if more than two observations after filtering\n",
    "#         if len(min_aars) < 2:\n",
    "#             print('Less than two years of AARs, skipping...')\n",
    "#             continue\n",
    "#         min_dts = []\n",
    "#         # iterate over years to extract dates\n",
    "#         for year, min_aar in zip(np.array(min_aars.index), min_aars['AAR'].values):\n",
    "#             min_dt = snowlines_site.loc[(snowlines_site['Year']==year) & (snowlines_site['AAR']==min_aar)]['datetime'].values[0]\n",
    "#             min_dts.append(min_dt)\n",
    "#         # fit a linear model to dates and AARs\n",
    "#         model = LinearRegression()\n",
    "#         model_fit = model.fit(np.array(min_aars.index).reshape(-1, 1), min_aars['AAR'])\n",
    "    \n",
    "#         # save in dataframe\n",
    "#         min_aar_df = pd.DataFrame({'site_name': [site_name],\n",
    "#                                    'minimum_AARs': [list(min_aars['AAR'].values)],\n",
    "#                                    'minimum_AARs_dts': [min_dts],\n",
    "#                                    'linear_fit_coef': [model_fit.coef_[0]],\n",
    "#                                    'linear_fit_intercept': [model_fit.intercept_]\n",
    "#                                   })\n",
    "#         # concatenate to full dataframe\n",
    "#         min_aars_df = pd.concat([min_aars_df, min_aar_df])\n",
    "\n",
    "#         # plot minimum AARs \n",
    "#         # fig, ax = plt.subplots()\n",
    "#         # ax.plot(snowlines_site['datetime'], snowlines_site['AAR'], '.')\n",
    "#         # ax.plot(min_dts, min_aars['AAR'].values, '.-b')\n",
    "#         # ax.grid()\n",
    "#         # ax.set_title(site_name + '\\nm = ' + str(model_fit.coef_))\n",
    "#         # plt.show()\n",
    "    \n",
    "#     # save to file\n",
    "#     min_aars_df.to_csv(min_aars_fn, index=False)\n",
    "#     print('Data table saved to file: ', min_aars_fn)\n",
    "#     min_aars_df.reset_index(drop=True, inplace=True)\n",
    "#     min_aars_df\n",
    "\n",
    "# else:\n",
    "\n",
    "#     min_aars_df = pd.read_csv(min_aars_fn)\n",
    "\n",
    "# min_aars_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f93ac8-a075-470f-876e-687bdf0d9c81",
   "metadata": {},
   "source": [
    "## Fit linear trends to different subregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8277b411-acf6-4d76-a88c-c072ae73b094",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # -----Grab training data file names\n",
    "# training_data_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'training_data*.csv')))\n",
    "# training_data_fns = [x for x in training_data_fns if '_scaled' not in x]\n",
    "\n",
    "# # -----Set up simple plot\n",
    "# fig, ax = plt.subplots(10, 2, figsize=(10, 24))\n",
    "# ax = ax.flatten()\n",
    "# fig2, ax2 = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "# # -----Iterate over subregions\n",
    "# # Initialize dataframe for storing results\n",
    "# aar_linear_fit_df = pd.DataFrame()\n",
    "# # Grab subregion names from file names\n",
    "# subregion_names = [os.path.basename(x).split('training_data_')[1].split('.csv')[0] \n",
    "#                    for x in training_data_fns]\n",
    "# for i, subregion_name in enumerate(subregion_names):\n",
    "#     # load training data\n",
    "#     training_data_subregion_fn = [x for x in training_data_fns if subregion_name in x][0]\n",
    "#     training_data_subregion = pd.read_csv(training_data_subregion_fn)\n",
    "\n",
    "#     # grab color for plotting\n",
    "#     o1, o2 = training_data_subregion[['O1Region', 'O2Region']].values[0]\n",
    "#     _, color = f.determine_subregion_name_color(o1, o2)\n",
    "\n",
    "#     # Fit linear regression models\n",
    "#     model = LinearRegression()\n",
    "#     model_fit_pdd = model.fit(training_data_subregion['Cumulative_Positive_Degree_Days'].values.reshape(-1, 1), \n",
    "#                               training_data_subregion['AAR'].values)\n",
    "#     model = LinearRegression()\n",
    "#     model_fit_snowfall = model.fit(training_data_subregion['Cumulative_Snowfall_mwe'].values.reshape(-1, 1), \n",
    "#                                    training_data_subregion['AAR'].values)\n",
    "#     # Save results in dataframe\n",
    "#     df = pd.DataFrame({'Subregion': [subregion_name],\n",
    "#                        'Color': [color],\n",
    "#                        'Linear_Fit_Coefficient_PDD': model_fit_pdd.coef_[0],\n",
    "#                        'Linear_Fit_Intercept_PDD': model_fit_pdd.intercept_,\n",
    "#                        'Linear_Fit_Coefficient_Snowfall': model_fit_snowfall.coef_[0],\n",
    "#                        'Linear_Fit_Intercept': model_fit_snowfall.intercept_\n",
    "#                       })\n",
    "#     aar_linear_fit_df = pd.concat([aar_linear_fit_df, df])\n",
    "#     # Plot\n",
    "#     # PDDs\n",
    "#     ax[i*2].plot(training_data_subregion['Cumulative_Positive_Degree_Days'], \n",
    "#                  training_data_subregion['AAR'], '.', markersize=0.5, color=color, alpha=0.8)\n",
    "#     ax[i*2].plot(training_data_subregion['Cumulative_Positive_Degree_Days'], \n",
    "#                  model_fit_pdd.predict(training_data_subregion['Cumulative_Positive_Degree_Days'].values.reshape(-1, 1)),\n",
    "#                  '-', linewidth=3, color=color)\n",
    "#     ax[i*2].grid()\n",
    "#     r2 = np.round(model_fit_pdd.score(training_data_subregion['Cumulative_Positive_Degree_Days'].values.reshape(-1, 1), \n",
    "#                              training_data_subregion['AAR'].values), 3)\n",
    "#     ax[i*2].set_title(f'R$^2$ = {r2}')\n",
    "#     ax2[0].plot(training_data_subregion['Cumulative_Positive_Degree_Days'], \n",
    "#                  model_fit_pdd.predict(training_data_subregion['Cumulative_Positive_Degree_Days'].values.reshape(-1, 1)),\n",
    "#                  '-', linewidth=2, color=color, label=subregion_name)\n",
    "#     # Snowfall\n",
    "#     ax[(i*2)+1].plot(training_data_subregion['Cumulative_Snowfall_mwe'], \n",
    "#                      training_data_subregion['AAR'], '.', markersize=0.5, color=color, alpha=0.8)\n",
    "#     ax[(i*2)+1].plot(training_data_subregion['Cumulative_Snowfall_mwe'], \n",
    "#                      model_fit_snowfall.predict(training_data_subregion['Cumulative_Snowfall_mwe'].values.reshape(-1, 1)),\n",
    "#                      '-', linewidth=3, color=color)\n",
    "#     ax[(i*2)+1].grid()    \n",
    "#     r2 = np.round(model_fit_snowfall.score(training_data_subregion['Cumulative_Snowfall_mwe'].values.reshape(-1, 1), \n",
    "#                                   training_data_subregion['AAR'].values), 3)\n",
    "#     ax[(i*2)+1].set_title(f'R$^2$ = {r2}')\n",
    "#     ax[i*2].set_ylabel('AAR')\n",
    "#     ax[(i*2)+1].set_ylabel('AAR')\n",
    "#     ax2[1].plot(training_data_subregion['Cumulative_Snowfall_mwe'], \n",
    "#                 model_fit_snowfall.predict(training_data_subregion['Cumulative_Snowfall_mwe'].values.reshape(-1, 1)),\n",
    "#                 '-', linewidth=2, color=color, label=subregion_name)\n",
    "\n",
    "#     if i==len(subregion_names):\n",
    "#         ax[i*2].set_xlabel('$\\Sigma$PDDs')\n",
    "#         ax[(i*2)+1].set_xlabel('$\\Sigma$Snowfall [m.w.e.]')\n",
    "\n",
    "# ax2[0].set_ylabel('AAR')\n",
    "# ax2[0].set_xlabel('$\\Sigma$PDDs')\n",
    "# ax2[0].grid()\n",
    "# ax2[1].set_xlabel('$\\Sigma$Snowfall [m.w.e.]')\n",
    "# ax2[1].grid()\n",
    "# ax2[1].legend(loc='center right', bbox_to_anchor=[1.4, 0.4, 0.2, 0.2])\n",
    "# fig.subplots_adjust(hspace=0.5)\n",
    "# plt.show()\n",
    "\n",
    "# # -----Save figures\n",
    "# fig_fn = os.path.join(figures_out_path, 'aar_climate_linear_fit_subregions.png')\n",
    "# fig.savefig(fig_fn, dpi=250, bbox_inches='tight')\n",
    "# fig2_fn = os.path.join(figures_out_path, 'aar_climate_linear_fit_subregions_single_axes.png')\n",
    "# fig2.savefig(fig2_fn, dpi=250, bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
