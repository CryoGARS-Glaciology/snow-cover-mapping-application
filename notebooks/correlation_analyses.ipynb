{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c58c8a4-4406-4119-a14b-6cffb0df6b50",
   "metadata": {},
   "source": [
    "# Correlation analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b89f21-5838-48f3-a4ee-f28fe0c9ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import seaborn as sns\n",
    "from scipy.stats import iqr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7c8f1d-d0b7-4ae7-87e1-b7a34a8552f9",
   "metadata": {},
   "source": [
    "## Define paths in directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0828d6-7c8b-48de-a275-7ec709d2a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "study_sites_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/'\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d75508-ea76-420a-849a-90a7a46c353b",
   "metadata": {},
   "source": [
    "## Load snowlines, glacier boundaries, and ERA data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07f2d1-ecc3-45a7-96cb-7ee2c738ce91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load snowlines\n",
    "snowlines_path = os.path.join(scm_path, 'all_snowlines')\n",
    "snowlines_fn = 'all_snowlines.csv'\n",
    "snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "snowlines.index = snowlines['datetime']\n",
    "print('All snowlines loaded from file.')\n",
    "\n",
    "# -----Load glacier boundaries\n",
    "aois_path = os.path.join(scm_path, 'all_AOIs')\n",
    "aois_fn = 'all_aois.shp'\n",
    "aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "print('All glacier boundaries loaded from file.')\n",
    "\n",
    "# -----Load ERA data \n",
    "eras_path = os.path.join(scm_path, 'all_ERA_data')\n",
    "eras_fn = 'all_era_data.csv'\n",
    "eras = pd.read_csv(os.path.join(eras_path, eras_fn))    \n",
    "eras['Date'] = pd.to_datetime(eras['Date'], format='mixed')\n",
    "print('All ERA data loaded from file.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6d8e5-185b-411b-972d-412a833df77c",
   "metadata": {},
   "source": [
    "## Calculate coefficients for AAR time series within subregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7a4f3-5944-482d-b131-272fc5157a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':12, 'font.sans-serif': 'Arial'})\n",
    "\n",
    "# iterate over subregions\n",
    "for o1region, o2region in aois[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    \n",
    "    # identify subregion name and color for plotting\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    print(subregion_name)\n",
    "\n",
    "    # check if subregion correlations file already exists in file\n",
    "    correlations_subregion_fn = os.path.join(scm_path, 'results', \n",
    "                                             'correlation_coefficients_' \n",
    "                                             + subregion_name.replace('.','').replace(' ','') + '.csv')\n",
    "    if os.path.exists(correlations_subregion_fn):\n",
    "        print('AAR correlations for subregion already exist in file, skipping...')\n",
    "        continue\n",
    "\n",
    "    # initialize dataframe for subregion correlation coefficients\n",
    "    correlations_subregion = pd.DataFrame()\n",
    "\n",
    "    # subset AOIs to subregion\n",
    "    aois_subregion = aois.loc[(aois['O1Region']==o1region) & (aois['O2Region']==o2region)]\n",
    "\n",
    "    # identify number of sites\n",
    "    unique_site_names = aois_subregion['RGIId'].drop_duplicates().values\n",
    "\n",
    "    # iterate over sites in subregion\n",
    "    for i in tqdm(range(0, len(unique_site_names))):\n",
    "\n",
    "        # grab site 1 time series\n",
    "        site1_df = snowlines.loc[snowlines['site_name']==unique_site_names[i]]\n",
    "        if len(site1_df) < 1:\n",
    "            continue\n",
    "        # remove duplicate dates, sort by date\n",
    "        site1_df = site1_df[~site1_df.index.duplicated(keep='first')].sort_index()  \n",
    "\n",
    "        # iterate over all other sites in subregion\n",
    "        for j in range(i+1, len(unique_site_names)):\n",
    "            \n",
    "            # grab site 2 time series\n",
    "            site2_df = snowlines.loc[snowlines['site_name']==unique_site_names[j]]\n",
    "            if len(site2_df) < 1:\n",
    "                continue\n",
    "            # remove duplicate dates, sort by date\n",
    "            site2_df = site2_df[~site2_df.index.duplicated(keep='first')].sort_index()  \n",
    "\n",
    "            # resample both dataframes at a daily time interval\n",
    "            site1_df = site1_df.resample('1D').bfill()\n",
    "            site2_df = site2_df.resample('1D').bfill()\n",
    "            min_date = np.min([site1_df.iloc[0]['datetime'], site2_df.iloc[0]['datetime']])\n",
    "            max_date = np.max([site1_df.iloc[-1]['datetime'], site2_df.iloc[-1]['datetime']])\n",
    "            site1_df = site1_df.loc[(site1_df['datetime'] >= min_date) & (site1_df['datetime'] <= max_date)]\n",
    "            site2_df = site2_df.loc[(site2_df['datetime'] >= min_date) & (site2_df['datetime'] <= max_date)]\n",
    "\n",
    "            # calculate correlation coefficient\n",
    "            aar_correlation = pd.DataFrame({'Site1': site1_df['AAR'],\n",
    "                                            'Site2': site2_df['AAR']}).corr().iloc[0,1]\n",
    "            correlation_sites = pd.DataFrame({'Site1': [unique_site_names[i]],\n",
    "                                              'Site2': [unique_site_names[j]],\n",
    "                                              'AAR Corr. Coeff.': [aar_correlation]})\n",
    "            # append to dataframe\n",
    "            correlations_subregion = pd.concat([correlations_subregion, correlation_sites])\n",
    "\n",
    "    # save CSV\n",
    "    correlations_subregion_pivot = correlations_subregion.pivot_table(index='Site1', \n",
    "                                                                      columns='Site2', \n",
    "                                                                      values='AAR Corr. Coeff.')\n",
    "    correlations_subregion_pivot.to_csv(correlations_subregion_fn, index=False)\n",
    "    print('Correlation coefficients saved to file: ', correlations_subregion_fn)\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 8), gridspec_kw={'width_ratios': [4,1]})\n",
    "    # heatmap\n",
    "    sns.heatmap(correlations_subregion_pivot, ax=ax[0], cmap='coolwarm', vmin=-1, vmax=1, cbar=False)\n",
    "    ax[0].set_title(subregion_name)\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_ylabel('')\n",
    "    ax[0].set_yticks([])\n",
    "    # boxplot\n",
    "    ax[1].boxplot(correlations_subregion['AAR Corr. Coeff.'].values)\n",
    "    ax[1].set_ylim(-1,1)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa683b8-7b88-4877-ad35-8efea0d53f93",
   "metadata": {},
   "source": [
    "## Calculate correlation coefficients between AAR and cumulative PDD time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0b90f-d2f6-4c09-84ab-f4b7edef3543",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over subregions\n",
    "for i, (o1region, o2region) in enumerate(aois[['O1Region', 'O2Region']].drop_duplicates().values):\n",
    "    \n",
    "    # identify subregion name and color for plotting\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    print(subregion_name)\n",
    "\n",
    "    # check if subregion AAR-PDD correlations already exist in file\n",
    "    correlations_subregion_fn = os.path.join(scm_path, 'results',\n",
    "                                             'correlation_coefficients_aar-pdd_' \n",
    "                                             + subregion_name + '.csv')\n",
    "    if os.path.exists(correlations_subregion_fn):\n",
    "        print('AAR-PDD correlations already exist in file, skipping...')\n",
    "        continue\n",
    "    \n",
    "    # initialize dataframe for subregion correlation coefficients\n",
    "    correlations_subregion = pd.DataFrame()\n",
    "\n",
    "    # subset AOIs to subregion\n",
    "    aois_subregion = aois.loc[(aois['O1Region']==o1region) & (aois['O2Region']==o2region)]\n",
    "\n",
    "    # iterate over sites in subregion\n",
    "    for site_name in tqdm(sorted(aois_subregion['RGIId'].drop_duplicates().values)):\n",
    "\n",
    "        # grab site snowlines time series\n",
    "        snowlines_site = snowlines.loc[snowlines['site_name']==site_name]\n",
    "        if len(snowlines_site) < 1:\n",
    "            print('No snowlines for', site_name)\n",
    "            continue\n",
    "        # remove duplicate dates, sort by date\n",
    "        snowlines_site = snowlines_site[~snowlines_site.index.duplicated(keep='first')].sort_index()  \n",
    "\n",
    "        # grab ERA data for site\n",
    "        era_site = eras.loc[eras['site_name']==site_name]\n",
    "        if len(era_site) < 1:\n",
    "            print('No ERA data for', site_name)\n",
    "            continue\n",
    "        era_site.index = era_site['Date']\n",
    "        \n",
    "        # resample both dataframes at a daily time interval\n",
    "        snowlines_site = snowlines_site.resample('1D').bfill()\n",
    "        era_site = era_site.resample('1D').bfill()\n",
    "        min_date = np.min([snowlines_site.iloc[0]['datetime'], era_site.iloc[0]['Date']])\n",
    "        max_date = np.max([snowlines_site.iloc[-1]['datetime'], era_site.iloc[-1]['Date']])\n",
    "        snowlines_site = snowlines_site.loc[(snowlines_site['datetime'] >= min_date) & (snowlines_site['datetime'] <= max_date)]\n",
    "        era_site = era_site.loc[(era_site['Date'] >= min_date) & (era_site['Date'] <= max_date)]\n",
    "\n",
    "        # calculate correlation coefficient\n",
    "        aar_pdd_correlation = pd.DataFrame({'site_name': snowlines_site['AAR'],\n",
    "                                            'PDD_cumsum': era_site['Cumulative_Positive_Degree_Days']}).corr().iloc[0,1]\n",
    "        correlation_site = pd.DataFrame({'Site': [site_name],\n",
    "                                          'AAR-PDD Corr. Coeff.': [aar_pdd_correlation]})\n",
    "        # append to dataframe\n",
    "        correlations_subregion = pd.concat([correlations_subregion, correlation_site])\n",
    "\n",
    "    # save CSV\n",
    "    correlations_subregion.to_csv(correlations_subregion_fn,index=False)\n",
    "    print('Correlation coefficients saved to file: ', correlations_subregion_fn)\n",
    "\n",
    "    # print stats\n",
    "    print('Correlation coefficients:')\n",
    "    print('\\t Mean:', np.nanmean(correlations_subregion['AAR-PDD Corr. Coeff.']))\n",
    "    print('\\t Std.:', np.nanstd(correlations_subregion['AAR-PDD Corr. Coeff.']))\n",
    "    print('\\t Median:', np.nanmedian(correlations_subregion['AAR-PDD Corr. Coeff.']))\n",
    "    print('\\t IQR:', iqr(correlations_subregion['AAR-PDD Corr. Coeff.']))\n",
    "    print(' ')\n",
    "    \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718c3387-9349-49bd-ac53-01a3faf9cf79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
