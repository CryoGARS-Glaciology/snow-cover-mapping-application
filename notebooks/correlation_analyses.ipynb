{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c58c8a4-4406-4119-a14b-6cffb0df6b50",
   "metadata": {},
   "source": [
    "# Correlation analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b89f21-5838-48f3-a4ee-f28fe0c9ab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely import wkt\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0828d6-7c8b-48de-a275-7ec709d2a699",
   "metadata": {},
   "outputs": [],
   "source": [
    "scm_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "sys.path.append(os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6d75508-ea76-420a-849a-90a7a46c353b",
   "metadata": {},
   "source": [
    "## Load and compile snowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c07f2d1-ecc3-45a7-96cb-7ee2c738ce91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load and compile snowlines\n",
    "snowlines_path = os.path.join(scm_path, 'all_snowlines')\n",
    "snowlines_fn = 'all_snowlines.csv'\n",
    "# check if snowlines path exists\n",
    "if not os.path.exists(snowlines_path):\n",
    "    os.mkdir(snowlines_path)\n",
    "# check if all snowlines CSV exists\n",
    "if not os.path.exists(os.path.join(snowlines_path, snowlines_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    snowlines = pd.DataFrame()\n",
    "    for site_name in tqdm(site_names):\n",
    "        snowline_path = os.path.join(study_sites_path, site_name)\n",
    "        snowline_fns = glob.glob(os.path.join(snowline_path, '*_snowlines.csv'))\n",
    "        if len(snowline_fns) > 0:\n",
    "            snowline_fn = snowline_fns[0]\n",
    "            snowline = pd.read_csv(snowline_fn)\n",
    "            snowlines = pd.concat([snowlines, snowline])\n",
    "    snowlines.reset_index(drop=True, inplace=True)\n",
    "    snowlines.to_csv(os.path.join(snowlines_path, snowlines_fn), index=False)\n",
    "    print('All snowlines saved to file: ', os.path.join(snowlines_path, snowlines_fn))\n",
    "\n",
    "else:\n",
    "    # load from file if it already exists\n",
    "    snowlines = pd.read_csv(os.path.join(snowlines_path, snowlines_fn))\n",
    "    snowlines['datetime'] = pd.to_datetime(snowlines['datetime'], format='mixed')\n",
    "    snowlines.index = snowlines['datetime']\n",
    "    print('All snowlines loaded from file.')\n",
    "\n",
    "snowlines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af119046-cec3-419f-8db6-28ca5981921d",
   "metadata": {},
   "source": [
    "## Load and compile glacier boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c34bbb1-f1ad-47fb-932e-1c9649e54b5f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load and compile AOIs\n",
    "aois_path = os.path.join(scm_path, 'all_AOIs')\n",
    "aois_fn = 'all_aois.shp'\n",
    "# check if aois path exists\n",
    "if not os.path.exists(aois_path):\n",
    "    os.mkdir(aois_path)\n",
    "# check if all aois shapefile exists\n",
    "if not os.path.exists(os.path.join(aois_path, aois_fn)):\n",
    "    # compile all RGI glacier boundaries\n",
    "    aois = gpd.GeoDataFrame()\n",
    "    for site_name in tqdm(site_names):\n",
    "        aoi_path = os.path.join(study_sites_path, site_name, 'AOIs')\n",
    "        aoi_fns = glob.glob(os.path.join(aoi_path, '*RGI*.shp'))\n",
    "        if len(aoi_fns) > 0:\n",
    "            aoi_fn = aoi_fns[0]\n",
    "            aoi = gpd.read_file(aoi_fn)\n",
    "            aoi = aoi.to_crs('EPSG:4326')\n",
    "            aois = pd.concat([aois, aoi])\n",
    "    aois.reset_index(drop=True, inplace=True)\n",
    "    aois.to_file(os.path.join(aois_path, aois_fn), index=False)\n",
    "    print('All glacier boundaries saved to file: ', os.path.join(aois_path, aois_fn))\n",
    "\n",
    "else:\n",
    "    # load from file if it already exists\n",
    "    aois = gpd.read_file(os.path.join(aois_path, aois_fn))\n",
    "    print('All glacier boundaries loaded from file.')\n",
    "aois[['O1Region', 'O2Region']] = aois[['O1Region', 'O2Region']].astype(int)\n",
    "aois"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f6d8e5-185b-411b-972d-412a833df77c",
   "metadata": {},
   "source": [
    "## Calculate coefficients for AAR time series within subregions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be7a4f3-5944-482d-b131-272fc5157a81",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':12, 'font.sans-serif': 'Arial'})\n",
    "\n",
    "# iterate over subregions\n",
    "for o1region, o2region in aois[['O1Region', 'O2Region']].drop_duplicates().values:\n",
    "    \n",
    "    # identify subregion name and color for plotting\n",
    "    subregion_name, color = f.determine_subregion_name_color(o1region, o2region)\n",
    "    print(subregion_name)\n",
    "\n",
    "    # initialize dataframe for subregion correlation coefficients\n",
    "    correlations_subregion = pd.DataFrame()\n",
    "\n",
    "    # subset AOIs to subregion\n",
    "    aois_subregion = aois.loc[(aois['O1Region']==o1region) & (aois['O2Region']==o2region)]\n",
    "\n",
    "    # identify number of sites\n",
    "    unique_site_names = aois_subregion['RGIId'].drop_duplicates().values\n",
    "\n",
    "    # iterate over sites in subregion\n",
    "    for i in tqdm(range(0, len(unique_site_names))):\n",
    "\n",
    "        # grab site 1 time series\n",
    "        site1_df = snowlines.loc[snowlines['site_name']==unique_site_names[i]]\n",
    "        if len(site1_df) < 1:\n",
    "            continue\n",
    "        # remove duplicate dates, sort by date\n",
    "        site1_df = site1_df[~site1_df.index.duplicated(keep='first')].sort_index()  \n",
    "\n",
    "        # iterate over all other sites in subregion\n",
    "        for j in range(i+1, len(unique_site_names)):\n",
    "            \n",
    "            # grab site 2 time series\n",
    "            site2_df = snowlines.loc[snowlines['site_name']==unique_site_names[j]]\n",
    "            if len(site2_df) < 1:\n",
    "                continue\n",
    "            # remove duplicate dates, sort by date\n",
    "            site2_df = site2_df[~site2_df.index.duplicated(keep='first')].sort_index()  \n",
    "\n",
    "            # resample both dataframes at a daily time interval\n",
    "            site1_df = site1_df.resample('1D').bfill()\n",
    "            site2_df = site2_df.resample('1D').bfill()\n",
    "            min_date = np.min([site1_df.iloc[0]['datetime'], site2_df.iloc[0]['datetime']])\n",
    "            max_date = np.max([site1_df.iloc[-1]['datetime'], site2_df.iloc[-1]['datetime']])\n",
    "            site1_df = site1_df.loc[(site1_df['datetime'] >= min_date) & (site1_df['datetime'] <= max_date)]\n",
    "            site2_df = site2_df.loc[(site2_df['datetime'] >= min_date) & (site2_df['datetime'] <= max_date)]\n",
    "\n",
    "            # calculate correlation coefficient\n",
    "            aar_correlation = pd.DataFrame({'Site1': site1_df['AAR'],\n",
    "                                            'Site2': site2_df['AAR']}).corr().iloc[0,1]\n",
    "            correlation_sites = pd.DataFrame({'Site1': [unique_site_names[i]],\n",
    "                                              'Site2': [unique_site_names[j]],\n",
    "                                              'AAR Corr. Coeff.': [aar_correlation]})\n",
    "            # append to dataframe\n",
    "            correlations_subregion = pd.concat([correlations_subregion, correlation_sites])\n",
    "\n",
    "    # save CSV\n",
    "    correlations_subregion_pivot = correlations_subregion.pivot_table(index='Site1', \n",
    "                                                                      columns='Site2', \n",
    "                                                                      values='AAR Corr. Coeff.')\n",
    "    correlations_subregion_fn = 'correlation_coefficients_' + subregion_name.replace('.','').replace(' ','') + '.csv'\n",
    "    correlations_subregion_pivot.to_csv(os.path.join(snowlines_path, correlations_subregion_fn),index=False)\n",
    "    print('Correlation coefficients saved to file: ', os.path.join(snowlines_path, correlations_subregion_fn))\n",
    "    \n",
    "    # plot\n",
    "    fig, ax = plt.subplots(1, 2, figsize=(10, 8), gridspec_kw={'width_ratios': [4,1]})\n",
    "    # heatmap\n",
    "    sns.heatmap(correlations_subregion_pivot, ax=ax[0], cmap='coolwarm', vmin=-1, vmax=1, cbar=False)\n",
    "    ax[0].set_title(subregion_name)\n",
    "    ax[0].set_xlabel('')\n",
    "    ax[0].set_xticks([])\n",
    "    ax[0].set_ylabel('')\n",
    "    ax[0].set_yticks([])\n",
    "    # boxplot\n",
    "    ax[1].boxplot(correlations_subregion['AAR Corr. Coeff.'].values)\n",
    "    ax[1].set_ylim(-1,1)\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa683b8-7b88-4877-ad35-8efea0d53f93",
   "metadata": {},
   "source": [
    "## Calculate correlation coefficients between AAR and PDD time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e0b90f-d2f6-4c09-84ab-f4b7edef3543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
