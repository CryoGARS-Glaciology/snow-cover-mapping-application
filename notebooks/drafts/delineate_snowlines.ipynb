{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94da50fa-70dc-4972-b330-f0e3bc481dcb",
   "metadata": {
    "id": "94da50fa-70dc-4972-b330-f0e3bc481dcb"
   },
   "source": [
    "# Delineate snowlines\n",
    "\n",
    "## Must already have classified images available in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96tN8CQ7J6JP",
   "metadata": {
    "id": "96tN8CQ7J6JP"
   },
   "outputs": [],
   "source": [
    "!pip install rioxarray geedim wxee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26770060-9961-406c-aa12-58526768f341",
   "metadata": {
    "id": "26770060-9961-406c-aa12-58526768f341"
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import ee\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from shapely.geometry import LineString, MultiLineString, Point, Polygon, MultiPolygon\n",
    "import json\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.ndimage import binary_fill_holes, binary_dilation\n",
    "from skimage.measure import find_contours\n",
    "from scipy.interpolate import interp1d\n",
    "import matplotlib\n",
    "import geedim as gd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oPcmwiPyJzxG",
   "metadata": {
    "id": "oPcmwiPyJzxG"
   },
   "outputs": [],
   "source": [
    "# If using Google Colab, mount Google Drive so you can access your Drive folders\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9867ea8-7e4e-4f2a-ab60-57ae9d0265b6",
   "metadata": {
    "id": "a9867ea8-7e4e-4f2a-ab60-57ae9d0265b6"
   },
   "outputs": [],
   "source": [
    "# -----Define paths in directory\n",
    "site_name = 'LemonCreek'\n",
    "# path to snow-cover-mapping/\n",
    "base_path = ('drive/MyDrive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/snow-cover-mapping/')\n",
    "# path to folder containing AOI files\n",
    "AOI_path = ('drive/MyDrive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/' + site_name + '/AOIs/')\n",
    "# AOI file name\n",
    "AOI_fn = glob.glob(AOI_path + site_name + '*USGS*outline*.shp' )[0]\n",
    "# path to classified images\n",
    "im_classified_path = AOI_path + '../imagery/classified/'\n",
    "# path for output snowlines\n",
    "snowlines_path = AOI_path + '../imagery/snowlines/'\n",
    "# path to PlanetScope image mosaics\n",
    "# Note: set PS_im_path=None if not using PlanetScope\n",
    "PS_im_path = AOI_path + '../imagery/PlanetScope/mosaics/'\n",
    "# path for output figures\n",
    "figures_out_path = AOI_path + '../figures/'\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.append(base_path + 'functions/')\n",
    "import pipeline_utils as f\n",
    "\n",
    "# -----Load dataset dictionary\n",
    "dataset_dict = json.load(open(base_path + 'inputs-outputs/datasets_characteristics.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410c9bfb-779f-47b2-bb8c-d79648fcaae2",
   "metadata": {
    "id": "410c9bfb-779f-47b2-bb8c-d79648fcaae2"
   },
   "outputs": [],
   "source": [
    "# -----Initialize Google Earth Engine\n",
    "try:\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize(opt_url='https://earthengine-highvolume.googleapis.com')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b3403b-fc1d-4a0d-bd6d-5eb686dcabe1",
   "metadata": {
    "id": "b1b3403b-fc1d-4a0d-bd6d-5eb686dcabe1"
   },
   "outputs": [],
   "source": [
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs('EPSG:4326')\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "# grab the optimal UTM zone EPSG code\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "print('Optimal UTM CRS = EPSG:' + str(epsg_UTM))\n",
    "# reproject AOI to the optimal UTM zone\n",
    "AOI_UTM = AOI.to_crs('EPSG:'+epsg_UTM)\n",
    "\n",
    "# -----Plot\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "if type(AOI_UTM.geometry[0])==Polygon:\n",
    "    ax.plot([x/1e3 for x in AOI_UTM.geometry[0].exterior.coords.xy[0]],\n",
    "            [y/1e3 for y in AOI_UTM.geometry[0].exterior.coords.xy[1]], '-k')\n",
    "elif type(AOI_UTM.geometry[0])==MultiPolygon:\n",
    "    [ax.plot([x/1e3 for x in geom.exterior.coords.xy[0]],\n",
    "            [y/1e3 for y in geom.exterior.coords.xy[1]], '-k') for geom in AOI_UTM.geometry[0].geoms]\n",
    "ax.grid()\n",
    "ax.set_xlabel('Easting [km]')\n",
    "ax.set_ylabel('Northing [km]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa29761e-8a55-4424-a58b-6fb888d3c468",
   "metadata": {
    "id": "fa29761e-8a55-4424-a58b-6fb888d3c468"
   },
   "outputs": [],
   "source": [
    "# Define necessary functions\n",
    "\n",
    "def delineate_snowline(im_classified, site_name, aoi, dataset_dict, dataset, im_date, snowline_fn,\n",
    "                       out_path, figures_out_path, plot_results, verbose=False):\n",
    "\n",
    "    # -----Make directory for snowlines (if it does not already exist)\n",
    "    if not os.path.exists(out_path):\n",
    "        os.mkdir(out_path)\n",
    "        print('Made directory for snowlines: ' + out_path)\n",
    "\n",
    "    # -----Make directory for figures (if it does not already exist)\n",
    "    if (not os.path.exists(figures_out_path)) & plot_results:\n",
    "        os.mkdir(figures_out_path)\n",
    "        print('Made directory for output figures: ' + figures_out_path)\n",
    "\n",
    "    # -----Subset dataset_dict to dataset\n",
    "    ds_dict = dataset_dict[dataset]\n",
    "\n",
    "    # -----Remove time dimension\n",
    "    im_classified = im_classified.isel(time=0)\n",
    "\n",
    "    # -----Create no data mask\n",
    "    no_data_mask = xr.where(np.isnan(im_classified), 1, 0).classified.data\n",
    "    # dilate by ~30 m\n",
    "    iterations = int(np.round(ds_dict['resolution_m'] / 30))  # number of pixels equal to 30 m\n",
    "    dilated_mask = binary_dilation(no_data_mask, iterations=iterations)\n",
    "    no_data_mask = np.logical_not(dilated_mask)\n",
    "    # add no_data_mask variable classified image\n",
    "    im_classified = im_classified.assign(no_data_mask=([\"y\", \"x\"], no_data_mask))\n",
    "\n",
    "    # -----Determine snow covered elevations\n",
    "    all_elev = np.ravel(im_classified.elevation.data)\n",
    "    all_elev = all_elev[~np.isnan(all_elev)]  # remove NaNs\n",
    "    snow_est_elev = np.ravel(im_classified.where((im_classified.classified <= 2))\n",
    "                             .where(im_classified.classified != -9999).elevation.data)\n",
    "    snow_est_elev = snow_est_elev[~np.isnan(snow_est_elev)]  # remove NaNs\n",
    "\n",
    "    # -----Create elevation histograms\n",
    "    # determine bins to use in histograms\n",
    "    elev_min = np.fix(np.nanmin(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "    elev_max = np.round(np.nanmax(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "    bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max - elev_min) / 10 + 1))\n",
    "    bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "    # calculate elevation histograms\n",
    "    hist_elev = np.histogram(all_elev, bins=bin_edges)[0]\n",
    "    hist_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "    hist_snow_est_elev_norm = hist_snow_est_elev / hist_elev\n",
    "\n",
    "    # -----Make all pixels at elevation bins with >75% snow coverage = snow\n",
    "    # determine elevation with > 75% snow coverage\n",
    "    if np.any(hist_snow_est_elev_norm > 0.75):\n",
    "        elev_75_snow = bin_centers[np.argmax(hist_snow_est_elev_norm > 0.75)]\n",
    "        # make a copy of im_classified for adjusting\n",
    "        im_classified_adj = im_classified.copy()\n",
    "        # Fill gaps in elevation using linear interpolation along the spatial dimensions\n",
    "        im_classified_adj['elevation'] = im_classified['elevation'].interpolate_na(dim='x', method='linear')\n",
    "        # set all pixels above the elev_75_snow to snow (1)\n",
    "        im_classified_adj['classified'] = xr.where(im_classified_adj['elevation'] > elev_75_snow, 1,\n",
    "                                                   im_classified_adj['classified'])\n",
    "        # create a binary mask for everything above the first instance of the snow-covered percentage threshold\n",
    "        sca_perc_threshold = 0.25\n",
    "        if np.any(hist_snow_est_elev_norm > sca_perc_threshold):\n",
    "            elev_thresh_snow = bin_centers[np.argmax(hist_snow_est_elev_norm > sca_perc_threshold)]\n",
    "            elevation_threshold_mask = xr.where(im_classified.elevation > elev_thresh_snow, 1, 0)\n",
    "        else:\n",
    "            elevation_threshold_mask = xr.where(~np.isnan(im_classified.elevation), 0, 0)\n",
    "\n",
    "    else:\n",
    "        im_classified_adj = im_classified\n",
    "        elevation_threshold_mask = None\n",
    "\n",
    "    # -----Delineate snow lines\n",
    "    # create binary snow matrix\n",
    "    im_binary = xr.where(im_classified_adj > 2, 1, 0).classified.data\n",
    "    # fill holes in binary image (0s within 1s = 1)\n",
    "    im_binary_no_holes = binary_fill_holes(im_binary)\n",
    "    # find contours at a constant value of 0.5 (between 0 and 1)\n",
    "    contours = find_contours(im_binary_no_holes, 0.5)\n",
    "    # convert contour points to image coordinates\n",
    "    contours_coords = []\n",
    "    for contour in contours:\n",
    "        # convert image pixel coordinates to real coordinates\n",
    "        fx = interp1d(range(0, len(im_classified_adj.x.data)), im_classified_adj.x.data)\n",
    "        fy = interp1d(range(0, len(im_classified_adj.y.data)), im_classified_adj.y.data)\n",
    "        coords = (fx(contour[:, 1]), fy(contour[:, 0]))\n",
    "        # zip points together\n",
    "        xy = list(zip([x for x in coords[0]],\n",
    "                      [y for y in coords[1]]))\n",
    "        contours_coords.append(xy)\n",
    "\n",
    "    # convert list of coordinates to list of LineStrings\n",
    "    # do not include points in the no data mask or points above the elevation threshold\n",
    "    contour_lines = []\n",
    "    for contour_coords in contours_coords:\n",
    "        # use elevation_threshold_mask to filter points if it exists\n",
    "        if elevation_threshold_mask is not None:\n",
    "            points_real = [Point(x, y) for x, y in contour_coords\n",
    "                           if im_classified.sel(x=x, y=y, method='nearest').no_data_mask.data.item()\n",
    "                           and ~np.isnan(im_classified.sel(x=x, y=y, method='nearest').elevation.data.item())\n",
    "                           and (elevation_threshold_mask.sel(x=x, y=y, method='nearest').data.item() == 1)\n",
    "                           ]\n",
    "        else:\n",
    "            points_real = [Point(x, y) for x, y in contour_coords\n",
    "                           if im_classified.sel(x=x, y=y, method='nearest').no_data_mask.data.item()\n",
    "                           and ~np.isnan(im_classified.sel(x=x, y=y, method='nearest').elevation.data.item())\n",
    "                           ]\n",
    "\n",
    "        if len(points_real) > 2:  # need at least 2 points for a LineString\n",
    "            contour_line = LineString([[point.x, point.y] for point in points_real])\n",
    "            contour_lines.append(contour_line)\n",
    "\n",
    "    # proceed if lines were found after filtering\n",
    "    if len(contour_lines) > 0:\n",
    "\n",
    "        # -----Use the longest line as the snowline\n",
    "        lengths = [line.length for line in contour_lines]\n",
    "        max_length_index = max(range(len(contour_lines)), key=lambda i: lengths[i])\n",
    "        snowline = contour_lines[max_length_index]\n",
    "\n",
    "        # -----Interpolate elevations at snow line coordinates\n",
    "        # compile all line coordinates into arrays of x- and y-coordinates\n",
    "        xpts = np.ravel([x for x in snowline.coords.xy[0]])\n",
    "        ypts = np.ravel([y for y in snowline.coords.xy[1]])\n",
    "        # interpolate elevation at snow line points\n",
    "        snowline_elevs = [im_classified.sel(x=x, y=y, method='nearest').elevation.data.item()\n",
    "                          for x, y in list(zip(xpts, ypts))]\n",
    "\n",
    "    else:\n",
    "\n",
    "        snowline = []\n",
    "        snowline_elevs = np.nan\n",
    "\n",
    "    # -----If AOI is ~covered in snow, set snowline elevation to the minimum elevation in the AOI\n",
    "    if np.all(np.isnan(snowline_elevs)) and (np.nanmedian(hist_snow_est_elev_norm) > 0.5):\n",
    "        snowline_elevs = np.nanmin(np.ravel(im_classified.elevation.data))\n",
    "\n",
    "    # -----Calculate snow-covered area (SCA) and accumulation area ratio (AAR)\n",
    "    # pixel resolution\n",
    "    dx = im_classified.x.data[1] - im_classified.x.data[0]\n",
    "    # snow-covered area\n",
    "    sca = len(np.ravel(im_classified.classified.data[im_classified.classified.data <= 2])) * (\n",
    "            dx ** 2)  # number of snow-covered pixels * pixel resolution [m^2]\n",
    "    # accumulation area ratio\n",
    "    total_area = len(np.ravel(im_classified.classified.data[~np.isnan(im_classified.classified.data)])) * (\n",
    "            dx ** 2)  # number of pixels * pixel resolution [m^2]\n",
    "    aar = sca / total_area\n",
    "\n",
    "    # -----Compile results in dataframe\n",
    "    # calculate median snow line elevation\n",
    "    median_snowline_elev = np.nanmedian(snowline_elevs)\n",
    "    # compile results in df\n",
    "    if type(snowline) == LineString:\n",
    "        snowlines_coords_x = [list(snowline.coords.xy[0])]\n",
    "        snowlines_coords_y = [list(snowline.coords.xy[1])]\n",
    "    else:\n",
    "        snowlines_coords_x = [[]]\n",
    "        snowlines_coords_y = [[]]\n",
    "    snowline_df = pd.DataFrame({'site_name': [site_name],\n",
    "                                'datetime': [im_date],\n",
    "                                'snowlines_coords_X': snowlines_coords_x,\n",
    "                                'snowlines_coords_Y': snowlines_coords_y,\n",
    "                                'CRS': ['EPSG:' + str(aoi.crs.to_epsg())],\n",
    "                                'snowline_elevs_m': [snowline_elevs],\n",
    "                                'snowline_elevs_median_m': [median_snowline_elev],\n",
    "                                'SCA_m2': [sca],\n",
    "                                'AAR': [aar],\n",
    "                                'dataset': [dataset],\n",
    "                                'geometry': [snowline]\n",
    "                                })\n",
    "\n",
    "    # -----Save snowline df to file\n",
    "    # reduce memory storage of dataframe\n",
    "    snowline_df = f.reduce_memory_usage(snowline_df, verbose=False)\n",
    "    # save using user-specified file extension\n",
    "    if 'pkl' in snowline_fn:\n",
    "        snowline_df.to_pickle(out_path + snowline_fn)\n",
    "        if verbose:\n",
    "            print('Snowline saved to file: ' + out_path + snowline_fn)\n",
    "    elif 'csv' in snowline_fn:\n",
    "        snowline_df.to_csv(out_path + snowline_fn, index=False)\n",
    "        if verbose:\n",
    "            print('Snowline saved to file: ' + out_path + snowline_fn)\n",
    "    else:\n",
    "        print('Please specify snowline_fn with extension .pkl or .csv. Exiting...')\n",
    "        return 'N/A'\n",
    "\n",
    "    # -----Plot results\n",
    "    fig, ax = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    # define x and y limits\n",
    "    xmin, xmax = aoi.geometry[0].buffer(100).bounds[0] / 1e3, aoi.geometry[0].buffer(100).bounds[2] / 1e3\n",
    "    ymin, ymax = aoi.geometry[0].buffer(100).bounds[1] / 1e3, aoi.geometry[0].buffer(100).bounds[3] / 1e3\n",
    "    # define colors for plotting\n",
    "    colors = list(dataset_dict['classified_image']['class_colors'].values())\n",
    "    cmp = matplotlib.colors.ListedColormap(colors)\n",
    "    # classified image\n",
    "    ax[1].imshow(im_classified['classified'].data, cmap=cmp, clim=(1, 5),\n",
    "                 extent=(np.min(im_classified.x.data) / 1e3, np.max(im_classified.x.data) / 1e3,\n",
    "                         np.min(im_classified.y.data) / 1e3, np.max(im_classified.y.data) / 1e3))\n",
    "    # snowline coordinates\n",
    "    if type(snowline) == LineString:\n",
    "        ax[0].plot(np.divide(snowline.coords.xy[0], 1e3), np.divide(snowline.coords.xy[1], 1e3),\n",
    "                   '.', color='#f768a1', markersize=2)\n",
    "        ax[1].plot(np.divide(snowline.coords.xy[0], 1e3), np.divide(snowline.coords.xy[1], 1e3),\n",
    "                   '.', color='#f768a1', markersize=2)\n",
    "    # plot dummy points for legend\n",
    "    ax[1].scatter(0, 0, color=colors[0], s=50, label='Snow')\n",
    "    ax[1].scatter(0, 0, color=colors[1], s=50, label='Shadowed snow')\n",
    "    ax[1].scatter(0, 0, color=colors[2], s=50, label='Ice')\n",
    "    ax[1].scatter(0, 0, color=colors[3], s=50, label='Rock')\n",
    "    ax[1].scatter(0, 0, color=colors[4], s=50, label='Water')\n",
    "    if type(snowline) == LineString:\n",
    "        ax[0].scatter(0, 0, color='#f768a1', s=25, label='Snowline estimate')\n",
    "        ax[1].scatter(0, 0, color='#f768a1', s=25, label='Snowline estimate')\n",
    "    ax[0].set_ylabel('Northing [km]')\n",
    "    ax[0].set_xlabel('Easting [km]')\n",
    "    ax[1].set_xlabel('Easting [km]')\n",
    "    # AOI\n",
    "    label = 'AOI'\n",
    "    if type(aoi.geometry[0].boundary) == MultiLineString:\n",
    "        for ii, geom in enumerate(aoi.geometry[0].boundary.geoms):\n",
    "            if ii > 0:\n",
    "                label = '_nolegend_'\n",
    "            ax[0].plot(np.divide(geom.coords.xy[0], 1e3),\n",
    "                       np.divide(geom.coords.xy[1], 1e3), '-k', linewidth=1, label=label)\n",
    "            ax[1].plot(np.divide(geom.coords.xy[0], 1e3),\n",
    "                       np.divide(geom.coords.xy[1], 1e3), '-k', linewidth=1, label=label)\n",
    "    elif type(aoi.geometry[0].boundary) == LineString:\n",
    "        ax[0].plot(np.divide(aoi.geometry[0].boundary.coords.xy[0], 1e3),\n",
    "                   np.divide(aoi.geometry[0].boundary.coords.xy[1], 1e3), '-k', linewidth=1, label=label)\n",
    "        ax[1].plot(np.divide(aoi.geometry[0].boundary.coords.xy[0], 1e3),\n",
    "                   np.divide(aoi.geometry[0].boundary.coords.xy[1], 1e3), '-k', linewidth=1, label=label)\n",
    "\n",
    "    # reset x and y limits\n",
    "    ax[0].set_xlim(xmin, xmax)\n",
    "    ax[0].set_ylim(ymin, ymax)\n",
    "    ax[1].set_xlim(xmin, xmax)\n",
    "    ax[1].set_ylim(ymin, ymax)\n",
    "    # normalized snow elevations histogram\n",
    "    ax[2].bar(bin_centers, hist_snow_est_elev_norm, width=(bin_centers[1] - bin_centers[0]), color=colors[0],\n",
    "              align='center')\n",
    "    ax[2].plot([median_snowline_elev, median_snowline_elev], [0, 1], '-', color='#f768a1',\n",
    "               linewidth=3, label='Median snowline elevation')\n",
    "    ax[2].set_xlabel(\"Elevation [m]\")\n",
    "    ax[2].set_ylabel(\"Fraction snow-covered\")\n",
    "    ax[2].grid()\n",
    "    ax[2].set_xlim(elev_min - 10, elev_max + 10)\n",
    "    ax[2].set_ylim(0, 1)\n",
    "    # determine figure title and file name\n",
    "    title = im_date.replace('-', '').replace(':', '') + '_' + site_name + '_' + dataset + '_snow-cover'\n",
    "    # add legends\n",
    "    ax[0].legend(loc='best')\n",
    "    ax[1].legend(loc='best')\n",
    "    ax[2].legend(loc='lower right')\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    return snowline_df, fig, ax, title\n",
    "\n",
    "def query_gee_for_image_thumbnail(dataset, dt, aoi_utm):\n",
    "\n",
    "    # -----Grab datetime from snowline df\n",
    "    date_start = str(dt - np.timedelta64(1, 'D'))\n",
    "    date_end = str(dt + np.timedelta64(1, 'D'))\n",
    "\n",
    "    # -----Buffer AOI by 1km\n",
    "    aoi_utm_buffer = aoi_utm.buffer(1e3)\n",
    "    # determine bounds for image plotting\n",
    "    bounds = aoi_utm_buffer.geometry[0].bounds\n",
    "\n",
    "    # -----Reformat AOI for image filtering\n",
    "    # reproject CRS from AOI to WGS\n",
    "    aoi_wgs = aoi_utm.to_crs('EPSG:4326')\n",
    "    aoi_buffer_wgs = aoi_utm_buffer.to_crs('EPSG:4326')\n",
    "    # prepare AOI for querying geedim (AOI bounding box)\n",
    "    region = {'type': 'Polygon',\n",
    "              'coordinates': [[[aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.miny[0]],\n",
    "                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.maxy[0]],\n",
    "                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]]\n",
    "                               ]]}\n",
    "    region_buffer_ee = ee.Geometry.Polygon([[[aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.miny[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.maxx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.maxy[0]],\n",
    "                                              [aoi_buffer_wgs.geometry.bounds.minx[0], aoi_buffer_wgs.geometry.bounds.miny[0]]\n",
    "                                            ]])\n",
    "\n",
    "    # -----Query GEE for imagery\n",
    "    if dataset == 'Landsat':\n",
    "        # Landsat 8\n",
    "        im_col_gd_8 = gd.MaskedCollection.from_name('LANDSAT/LC08/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                                     end_date=date_end,\n",
    "                                                                                     mask=True,\n",
    "                                                                                     region=region,\n",
    "                                                                                     fill_portion=50)\n",
    "        # Landsat 9\n",
    "        im_col_gd_9 = gd.MaskedCollection.from_name('LANDSAT/LC09/C02/T1_L2').search(start_date=date_start,\n",
    "                                                                                     end_date=date_end,\n",
    "                                                                                     mask=True,\n",
    "                                                                                     region=region,\n",
    "                                                                                     fill_portion=50)\n",
    "        im_col_ee = im_col_gd_8.ee_collection.merge(im_col_gd_9.ee_collection)\n",
    "\n",
    "        # apply scaling factors\n",
    "        def apply_scale_factors(image):\n",
    "            opticalBands = image.select('SR_B.').multiply(0.0000275).add(-0.2)\n",
    "            thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0)\n",
    "            return image.addBands(opticalBands, None, True).addBands(thermalBands, None, True)\n",
    "\n",
    "        im_col_ee = im_col_ee.map(apply_scale_factors)\n",
    "        # define how to display image\n",
    "        visualization = {'bands': ['SR_B4', 'SR_B3', 'SR_B2'], 'min': 0.0, 'max': 1.0, 'dimensions': 768,\n",
    "                         'region': region_buffer_ee}\n",
    "    elif dataset == 'Sentinel-2_TOA':\n",
    "        im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_HARMONIZED').search(start_date=date_start,\n",
    "                                                                                     end_date=date_end,\n",
    "                                                                                     mask=True,\n",
    "                                                                                     region=region,\n",
    "                                                                                     fill_portion=50)\n",
    "        im_col_ee = im_col_gd.ee_collection\n",
    "        # define how to display image\n",
    "        visualization = {'bands': ['B4', 'B3', 'B2'], 'min': 0.0, 'max': 1e4, 'dimensions': 768,\n",
    "                         'region': region_buffer_ee}\n",
    "    elif dataset == 'Sentinel-2_SR':\n",
    "        im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_SR_HARMONIZED').search(start_date=date_start,\n",
    "                                                                                        end_date=date_end,\n",
    "                                                                                        mask=True,\n",
    "                                                                                        region=region,\n",
    "                                                                                        fill_portion=50)\n",
    "        im_col_ee = im_col_gd.ee_collection\n",
    "        # define how to display image\n",
    "        visualization = {'bands': ['B4', 'B3', 'B2'], 'min': 0.0, 'max': 1e4, 'dimensions': 768,\n",
    "                         'region': region_buffer_ee}\n",
    "    else:\n",
    "        print(\n",
    "            \"'dataset' variable not recognized. Please set to 'Landsat', 'Sentinel-2_TOA', or 'Sentinel-2_SR'. Exiting...\")\n",
    "        return 'N/A'\n",
    "\n",
    "    # -----Display image, snowline, and AOI on geemap.Map()\n",
    "    # Reproject the Earth Engine image to UTM projection\n",
    "    utm_epsg = aoi_utm.crs.to_epsg()  # Get UTM EPSG code from AOI's CRS\n",
    "    im_col_ee_utm = im_col_ee.map(lambda img: img.reproject(crs=f'EPSG:{utm_epsg}', scale=30))\n",
    "    # Fetch the image URL from Google Earth Engine\n",
    "    image_url = im_col_ee_utm.first().clip(region_buffer_ee).getThumbURL(visualization)\n",
    "    # Fetch the image and convert it to a PIL Image object\n",
    "    response = requests.get(image_url)\n",
    "    image_bytes = io.BytesIO(response.content)\n",
    "    image = Image.open(image_bytes)\n",
    "\n",
    "    return image, bounds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb43f46-cb90-4448-8286-4fe3cea6f7bc",
   "metadata": {
    "id": "4cb43f46-cb90-4448-8286-4fe3cea6f7bc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Load classified image file names\n",
    "im_classified_fns = sorted([os.path.basename(x) for x in glob.glob(im_classified_path +'*.nc')])\n",
    "im_classified_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wu92drxHomQS",
   "metadata": {
    "id": "Wu92drxHomQS"
   },
   "outputs": [],
   "source": [
    "fns = glob.glob(snowlines_path + '*.csv')\n",
    "fns\n",
    "# for fn in fns:\n",
    "#   os.remove(fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60e7f0b-9f16-4f65-be79-c6efddc9eb15",
   "metadata": {
    "id": "e60e7f0b-9f16-4f65-be79-c6efddc9eb15",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Iterate over classified image file names\n",
    "for im_classified_fn in tqdm(im_classified_fns[475:]):\n",
    "\n",
    "    # load classified image\n",
    "    im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "    # remove no data values\n",
    "    im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "\n",
    "    # determine image date and dataset\n",
    "    im_date = im_classified_fn[0:15]\n",
    "    im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "    dataset = im_classified_fn.split(site_name + '_')[1].split('_classified')[0]\n",
    "    print(str(im_dt) + ' ' + dataset)\n",
    "\n",
    "    # check whether snowline exists in file\n",
    "    snowline_fn = snowline_fn = im_date + '_' + site_name + '_' + dataset + '_snowline.csv'\n",
    "    if os.path.exists(snowlines_path + snowline_fn):\n",
    "        print('Snowline already exists in file, loading...')\n",
    "        print(' ')\n",
    "        continue # no need to load snowline if it already exists\n",
    "    else:\n",
    "\n",
    "      # delineate snowline and set up figure\n",
    "      snowline_df, fig, ax, title = delineate_snowline(im_classified, site_name, AOI_UTM, dataset_dict, dataset, im_date, snowline_fn,\n",
    "                                                        snowlines_path, figures_out_path, plot_results=True, verbose=True)\n",
    "      print('Accumulation Area Ratio =  ' + str(snowline_df['AAR'][0]))\n",
    "\n",
    "      # load image from file\n",
    "      if dataset == 'PlanetScope':\n",
    "          try:\n",
    "              im_fn = glob.glob(PS_im_path + '*' + str(im_date)[0:8] + '*.tif')[0]\n",
    "              im = rxr.open_rasterio(im_fn)\n",
    "              im = xr.where(im != -9999, im / 1e4, np.nan)\n",
    "              ax[0].imshow(np.dstack([im.data[2], im.data[1], im.data[0]]),\n",
    "                            extent=(np.min(im.x.data) / 1e3, np.max(im.x.data) / 1e3,\n",
    "                                    np.min(im.y.data) / 1e3, np.max(im.y.data) / 1e3))\n",
    "              # save figure\n",
    "              fig_fn = figures_out_path + title + '.png'\n",
    "              fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none')\n",
    "              print('Figure saved to file: ' + fig_fn)\n",
    "              plt.close()\n",
    "              print(' ')\n",
    "          except Exception as e:\n",
    "              print(e)\n",
    "              print(' ')\n",
    "              continue\n",
    "      # otherwise, load image thumbnail from GEE\n",
    "      else:\n",
    "          try:\n",
    "              # get PIL image object\n",
    "              im, bounds = query_gee_for_image_thumbnail(dataset, im_dt, AOI_UTM)\n",
    "              # plot RGB image\n",
    "              ax[0].imshow(im, extent=(bounds[0] / 1e3, bounds[2] / 1e3, bounds[1] / 1e3, bounds[3] / 1e3))\n",
    "              # save figure\n",
    "              fig_fn = figures_out_path + title + '.png'\n",
    "              fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none')\n",
    "              print('Figure saved to file: ' + fig_fn)\n",
    "              plt.close()\n",
    "              print(' ')\n",
    "          except Exception as e:\n",
    "              print(e)\n",
    "              print(' ')\n",
    "              plt.close()\n",
    "              continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p5ZDpNRHV5dN",
   "metadata": {
    "id": "p5ZDpNRHV5dN"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
