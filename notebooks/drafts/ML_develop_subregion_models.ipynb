{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86ad5990-b034-4b1e-979f-96296dcfd52b",
   "metadata": {},
   "source": [
    "# Develop machine learning models for each subregion to predict the AAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31d28e2-ddba-4473-8251-b8d1d8b2e012",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "import os\n",
    "from tqdm.auto import tqdm\n",
    "import sys\n",
    "from joblib import dump, load\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e997ea8-3bfb-4f08-a1b7-529259dfcb00",
   "metadata": {},
   "source": [
    "## Define paths in directory, import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c986bbef-11f9-4e75-b3ed-c490b5ea9f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to study-sites/\n",
    "scm_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/'\n",
    "# define path to snow-cover-mapping-application/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping-application/'\n",
    "# path to save output figures\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "\n",
    "# Load necessary functions\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import model_analyze_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14840695-be10-4034-93c2-94f0dad6193a",
   "metadata": {},
   "source": [
    "## Define the feature columns and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08627601-44d6-456e-a2c9-3a796a762636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns to use for prediction in training data\n",
    "feature_columns = ['Cumulative_Positive_Degree_Days', \n",
    "                   'Cumulative_Snowfall_mwe', \n",
    "                   'Hypsometric_Index',\n",
    "                   'Area', \n",
    "                   'Zmed', \n",
    "                   'Slope', \n",
    "                   'Aspect'\n",
    "                  ]\n",
    "# how to display each feature column in plots, etc.\n",
    "feature_columns_display = ['$\\Sigma$PDDs', \n",
    "                           '$\\Sigma$Snowfall [m.w.e.]',\n",
    "                           'Hypsometric Index',\n",
    "                           'Area', \n",
    "                           'Z$_{med}$', \n",
    "                           'Slope', \n",
    "                           'Aspect'\n",
    "                          ]\n",
    "# variable to predict\n",
    "labels = ['AAR']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec80f97-bb7f-43a8-994b-ede2a877657a",
   "metadata": {},
   "source": [
    "## Plot pairplots of (un-scaled) training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a7fef3-69b4-4c92-9d95-9a89df43a0a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size':14, 'font.sans-serif':'Arial'})\n",
    "\n",
    "# Grab training data file names\n",
    "training_data_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'training_data_*.csv')))\n",
    "training_data_fns = [x for x in training_data_fns if '_scaled' not in x]\n",
    "\n",
    "for training_data_fn in training_data_fns:\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name = training_data_fn.split('training_data_')[1].split('.csv')[0]\n",
    "\n",
    "    # Load training data\n",
    "    training_data = pd.read_csv(training_data_fn)\n",
    "    \n",
    "    # Rename columns for plotting\n",
    "    d = {}\n",
    "    for (x, x_display) in zip(feature_columns, feature_columns_display):\n",
    "        d[x] = x_display\n",
    "    training_data.rename(columns=d, inplace=True)\n",
    "    \n",
    "    # Plot\n",
    "    plot = sns.pairplot(training_data, vars=feature_columns_display, corner=True, diag_kind='kde', hue='AAR')\n",
    "    plot.fig.suptitle(subregion_name)\n",
    "    plt.show()\n",
    "\n",
    "    # Save figure\n",
    "    fig_fn = os.path.join(figures_out_path, 'training_data_pairplot_' + subregion_name + '.png')\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5892ebf-61b2-474c-ac98-19c755f93cc6",
   "metadata": {},
   "source": [
    "## Define supervised regression models to test\n",
    "\n",
    "See the [SciKitLearn Supervised Learning page](https://scikit-learn.org/stable/supervised_learning.html) for more models, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e593da-00f4-4a48-ab9d-d45ee30381db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models\n",
    "models = [\n",
    "    LinearRegression(),\n",
    "    RandomForestRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    SVR(),\n",
    "    GradientBoostingRegressor(),\n",
    "    Ridge()\n",
    "]\n",
    "\n",
    "# Model names (used for plotting, etc.)\n",
    "model_names = [\n",
    "    \"Linear Regression\",\n",
    "    \"Random Forest Regression\",\n",
    "    \"Decision Tree Regression\",\n",
    "    \"Support Vector Regression\",\n",
    "    \"Gradient Boosting Regression\",\n",
    "    \"Ridge Regression\"\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae7e4ed-ec5c-496d-8f50-d1d0409151d5",
   "metadata": {},
   "source": [
    "## Iterate over subregions, apply the machine learning workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e81720-ef4f-4428-9551-ceaea37d30a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Grab scaled training data file names\n",
    "training_data_scaled_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'training_data_*_scaled.csv')))\n",
    "\n",
    "# Iterate over scaled training data file names\n",
    "for training_data_scaled_fn in training_data_scaled_fns:\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name = training_data_scaled_fn.split('training_data_')[1].split('_scaled.csv')[0]\n",
    "    print('\\n' + subregion_name)\n",
    "    \n",
    "    # Load scaled training data\n",
    "    training_data_scaled = pd.read_csv(training_data_scaled_fn)\n",
    "    training_data_scaled['Date'] = pd.to_datetime(training_data_scaled['Date'])\n",
    "\n",
    "    # Split training data into X and y\n",
    "    X = training_data_scaled[feature_columns]\n",
    "    y = training_data_scaled[labels]\n",
    "\n",
    "    # Determine best model using K-folds cross-validation and save to file\n",
    "    out_path = os.path.join(scm_path, 'machine_learning')\n",
    "    best_model_fn = 'best_model_' + subregion_name + '.joblib'\n",
    "    best_model = f.determine_best_model(training_data_scaled, models, model_names, \n",
    "                                        feature_columns, labels, out_path, best_model_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce98a45-df0a-484d-b873-6a8763d4045c",
   "metadata": {},
   "source": [
    "## Conduct weather sensitivity tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11dc1fc9-db02-41d6-aa14-bd4ff6911e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up figure\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20,6), sharey=True)\n",
    "\n",
    "# Grab model file names\n",
    "model_fns = sorted(glob.glob(os.path.join(scm_path, 'machine_learning', 'best_model*')))\n",
    "\n",
    "# Iterate over model file names\n",
    "for model_fn in model_fns:\n",
    "    # Grab subregion name from file name\n",
    "    subregion_name = model_fn.split('best_model_')[1].split('.joblib')[0]\n",
    "    print('\\n' + subregion_name)\n",
    "    \n",
    "    # Load trained model\n",
    "    model = load(model_fn)\n",
    "    \n",
    "    # Load training data\n",
    "    training_data_fn = os.path.join(scm_path, 'machine_learning', 'training_data_' + subregion_name + '.csv')\n",
    "    training_data = pd.read_csv(training_data_fn)\n",
    "    \n",
    "    # Load fit scaler for training data\n",
    "    scaler_fit_fn = os.path.join(scm_path, 'machine_learning', 'scaler_fit_' + subregion_name + '.gz')\n",
    "    scaler_fit = load(scaler_fit_fn)\n",
    "\n",
    "    # Create dataframe of mean conditions\n",
    "    training_data_mean = pd.DataFrame(training_data[scaler_fit_columns].mean()).transpose()\n",
    "    training_data_mean['scenario'] = 'mean'\n",
    "    \n",
    "    # Grab names of columns that need to be scaled\n",
    "    scaler_fit_columns = scaler_fit.get_feature_names_out()\n",
    "    \n",
    "    # Iterate over cumsum(PDDs) and cumsum(snowfall)\n",
    "    for i, column in enumerate(['Cumulative_Positive_Degree_Days', 'Cumulative_Snowfall_mwe']):\n",
    "        # Define range of values\n",
    "        values_range = np.linspace(training_data[column].min(), training_data[column].max(), num=100)\n",
    "        df = training_data_mean.copy(deep=True)\n",
    "        df = pd.concat([df]*(len(values_range)+1),ignore_index=True)\n",
    "        df.loc[1:, column] = values_range\n",
    "        df.loc[1:, 'scenario'] = np.arange(1, len(values_range)+1, step=1)\n",
    "        \n",
    "        # Scale values\n",
    "        df_scaled = df.copy(deep=True)\n",
    "        df_scaled.loc[:, scaler_fit_columns] = scaler_fit.transform(df_scaled[scaler_fit_columns])\n",
    "    \n",
    "        # Predict AAR\n",
    "        df['AAR'] = model.predict(df_scaled[feature_columns])\n",
    "\n",
    "        # Save results\n",
    "        df_fn = os.path.join(scm_path, 'machine_learning', \n",
    "                             'aar_sensitivity_tests_' + subregion_name + '_' + column + '.csv')\n",
    "        df.to_csv(df_fn, index=False)\n",
    "        print(column + ' sensitivity tests saved to file:', df_fn)\n",
    "\n",
    "        # Plot results\n",
    "        plot = ax[i].plot(df.loc[1:, column], df.loc[1:, 'AAR'], '-', label=subregion_name)\n",
    "\n",
    "ax[0].set_ylim(0,1)\n",
    "ax[0].set_xlabel('$\\Sigma$PDDs')\n",
    "ax[0].set_ylabel('AAR')\n",
    "ax[0].grid()\n",
    "ax[1].set_xlabel('$\\Sigma$Snowfall [m.w.e.]')\n",
    "ax[1].grid()\n",
    "ax[1].legend(bbox_to_anchor=[1.2, 0.5, 0.2, 0.2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c61cb6e-e8c5-477e-8ec5-07023539e7f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
