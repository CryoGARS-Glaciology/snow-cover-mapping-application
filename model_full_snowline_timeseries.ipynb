{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "773296c3",
   "metadata": {},
   "source": [
    "# Analyze controls on snowlines/ELAs using machine learning and ERA-derived PDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f345891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import KFold\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67fd8fd",
   "metadata": {},
   "source": [
    "### Determine settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f81e49fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define path to study-sites/\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "\n",
    "# define sites to use\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe57b202",
   "metadata": {},
   "source": [
    "### Set up training data: snowline/AAR time series and ERA-derived PDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7ee3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Check if training data already exist in directory\n",
    "training_data_fn = 'full_snowline_timeseries_training_data.csv'\n",
    "if training_data_fn in os.listdir():\n",
    "    print('Training dataset already exists in directory, loading...')\n",
    "    training_full_df = pd.read_csv(training_data_fn)\n",
    "    \n",
    "else:\n",
    "\n",
    "    print('Constructing training dataset...')\n",
    "    \n",
    "    # -----Initialize dataframe for full training dataset\n",
    "    training_full_df = pd.DataFrame()\n",
    "\n",
    "    # -----Iterate over site names\n",
    "    for site_name in site_names:\n",
    "\n",
    "        print(site_name)\n",
    "\n",
    "        # Load snowlines\n",
    "        snowlines_df = pd.DataFrame()\n",
    "        snowline_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*.csv')\n",
    "        for snowline_fn in snowline_fns:\n",
    "            try:\n",
    "                snowline = pd.read_csv(snowline_fn)\n",
    "                snowlines_df = pd.concat([snowlines_df, snowline])\n",
    "            except:\n",
    "                continue\n",
    "        snowlines_df.reset_index(drop=True, inplace=True)\n",
    "        snowlines_df['datetime'] = pd.to_datetime(snowlines_df['datetime'], format='mixed')\n",
    "        snowlines_df['Date'] = snowlines_df['datetime'].values.astype('datetime64[D]')\n",
    "\n",
    "        # Load AOI\n",
    "        AOI_fn = glob.glob(study_sites_path + site_name + '/AOIs/*RGI*shp')[0]\n",
    "        AOI = gpd.read_file(AOI_fn)\n",
    "        # add terrain parameters to training df\n",
    "        AOI_columns = ['Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect']\n",
    "        for column in AOI_columns:\n",
    "            snowlines_df[column] = AOI[column].values[0]\n",
    "\n",
    "        # Load ERA data\n",
    "        era_fn = glob.glob(study_sites_path + site_name + '/ERA/*.csv')[0]\n",
    "        era = pd.read_csv(era_fn)\n",
    "        era.reset_index(drop=True, inplace=True)\n",
    "        era['Date'] = pd.to_datetime(era['Date'])\n",
    "        # merge era and snowline dates\n",
    "        snowlines_df = pd.merge(snowlines_df, era)\n",
    "\n",
    "        # Compile all info into single dataframe\n",
    "        columns = ['snowline_elevs_median_m', 'AAR', 'Cumulative_Positive_Degree_Days'] + AOI_columns\n",
    "        training_df = snowlines_df[columns]\n",
    "\n",
    "        # Compile and concatenate to training_df\n",
    "        training_full_df = pd.concat([training_full_df, training_df])\n",
    "\n",
    "    # -----Save training data to file\n",
    "    training_full_df.reset_index(drop=True, inplace=True)\n",
    "    training_data.to_csv(training_full_df, index=False)\n",
    "    print('Training data saved to file: ' + training_data_fn)\n",
    "\n",
    "training_full_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb796f3",
   "metadata": {},
   "source": [
    "## Split training dataset into X (_features_)and y (_labels_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17acf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['Cumulative_Positive_Degree_Days', 'Area', 'Zmin', 'Zmax', 'Zmed', 'Slope', 'Aspect']\n",
    "labels = ['snowline_elevs_median_m']\n",
    "\n",
    "# Remove NaNs from training dataset\n",
    "training_full_df.dropna(inplace=True)\n",
    "training_full_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = training_full_df[feature_columns]\n",
    "y = training_full_df[labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d686b03",
   "metadata": {},
   "source": [
    "## Define supervised machine learning models to test\n",
    "\n",
    "\n",
    "See the [SciKitLearn Classifier comparison page](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html) for more models, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c7604",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifier names\n",
    "names = [\n",
    "    \"Linear Regression\",\n",
    "    \"Random Forest Regression\",\n",
    "    \"Decision Tree Regression\",\n",
    "    \"Support Vector Regression\",\n",
    "    \"Gradient Boosting Regression\",\n",
    "    \"Ridge Regression\"\n",
    "]\n",
    "\n",
    "# Classifiers\n",
    "classifiers = [\n",
    "    LinearRegression(),\n",
    "    RandomForestRegressor(),\n",
    "    DecisionTreeRegressor(),\n",
    "    SVR(),\n",
    "    GradientBoostingRegressor(),\n",
    "    Ridge()\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3398d23b",
   "metadata": {},
   "source": [
    "## Train and test machine learning models using K-folds cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6236d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Initialize performance metrics\n",
    "abs_err = np.zeros(len(names)) # absolute error [m]\n",
    "\n",
    "# Iterate over classifiers\n",
    "for i, (name, clf) in enumerate(zip(names, classifiers)):\n",
    "    \n",
    "    print(name)\n",
    "\n",
    "    # Conduct K-Fold cross-validation\n",
    "    num_folds = 10\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "    abs_err_folds = np.zeros(num_folds) # absolute error for all folds\n",
    "    j = 0 # fold counter\n",
    "\n",
    "    # loop through fold indices\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "        # split data into training and testing using kfold indices\n",
    "        X_train, X_test = X.iloc[train_ix], X.iloc[test_ix]\n",
    "        y_train, y_test = np.ravel(y.iloc[train_ix].values), np.ravel(y.iloc[test_ix].values)\n",
    "\n",
    "        # fit model to X_train and y_train\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # predict outputs for X_test values\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # calculate performance metrics\n",
    "        abs_err_folds[j] = np.nanmean(np.abs(y_test - y_pred))\n",
    "        j += 1\n",
    "\n",
    "    # take average performance metrics for all folds\n",
    "    abs_err[i] = np.nanmean(abs_err_folds)\n",
    "    \n",
    "    # grab feature importances from random forest and plot\n",
    "    if name=='Random Forest Regression':\n",
    "        # train model with full dataset\n",
    "        clf.fit(X, y)\n",
    "        \n",
    "        # grab feature importances\n",
    "        importances = clf.feature_importances_\n",
    "        std = np.std([tree.feature_importances_ for tree in clf.estimators_], axis=0)\n",
    "        forest_importances = pd.Series(importances, index=feature_columns)\n",
    "        \n",
    "        # plot\n",
    "        fig, ax = plt.subplots()\n",
    "        forest_importances.plot.bar(yerr=std, ax=ax)\n",
    "        ax.set_xticks(ax.get_xticks())\n",
    "        ax.set_xticklabels(ax.get_xticklabels(), rotation=30, ha='right')\n",
    "        ax.set_title(\"Feature importances using MDI\")\n",
    "        ax.set_ylabel(\"Mean decrease in impurity\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # display performance results\n",
    "    print('    Mean absolute error = ' + str(np.round(abs_err[i])) + ' m')\n",
    "print(' ')\n",
    "\n",
    "# -----Determine best model\n",
    "ibest = np.argwhere(abs_err==np.min(abs_err))[0][0]\n",
    "best_clf = classifiers[ibest]\n",
    "best_clf_name = names[ibest]\n",
    "print('Most accurate classifier: ' + best_clf_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe66fb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
