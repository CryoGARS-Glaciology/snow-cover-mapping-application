{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPKd+aGEVVfD3eNDSYmQX9o"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Download images from GEE to modify snowline traces in QGIS"],"metadata":{"id":"IGngJeDeaXsD"}},{"cell_type":"code","source":["# Install necessary packages\n","!pip install geedim"],"metadata":{"id":"U8SD7MXVbY5i"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u0ZPPC2FaKeP"},"outputs":[],"source":["# Import packages\n","import geopandas as gpd\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import ee\n","import geedim as gd\n","import os\n","import glob\n","import json\n","import math"]},{"cell_type":"code","source":["# Authenticate your Google Earth Engine account\n","try:\n","    ee.Initialize()\n","except:\n","    ee.Authenticate()\n","    ee.Initialize()"],"metadata":{"id":"sobqgwLTboF5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# If using Google Colab, mount Google Drive so you can access the files in this folder\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"9tyDRGKTcBhX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# @title\n","# -----Define function for solving for optimal UTM zone\n","def convert_wgs_to_utm(lon: float, lat: float):\n","    \"\"\"\n","    Return best UTM epsg-code based on WGS84 lat and lon coordinate pair\n","\n","    Parameters\n","    ----------\n","    lon: float\n","        longitude coordinate\n","    lat: float\n","        latitude coordinate\n","\n","    Returns\n","    ----------\n","    epsg_code: str\n","        optimal UTM zone, e.g. \"32606\"\n","    \"\"\"\n","    utm_band = str((math.floor((lon + 180) / 6) % 60) + 1)\n","    if len(utm_band) == 1:\n","        utm_band = '0' + utm_band\n","    if lat >= 0:\n","        epsg_code = '326' + utm_band\n","        return epsg_code\n","    epsg_code = '327' + utm_band\n","    return epsg_code\n","\n","\n","# -----Define function for querying imagery\n","def query_gee_for_imagery(dataset_dict, dataset, aoi_utm, date_start, date_end, month_start, month_end,\n","                          cloud_cover_max, mask_clouds, out_path=None, im_download=False):\n","    \"\"\"\n","    Query Google Earth Engine for Landsat 8 and 9 surface reflectance (SR), Sentinel-2 top of atmosphere (TOA) or SR imagery.\n","    Images captured within the hour will be mosaicked.\n","\n","    Parameters\n","    __________\n","    dataset_dict: dict\n","        dictionary of parameters for each dataset\n","    dataset: str\n","        name of dataset ('Landsat', 'Sentinel-2_SR', 'Sentinel-2_TOA', 'PlanetScope')\n","    aoi_utm: geopandas.geodataframe.GeoDataFrame\n","        area of interest used for searching and clipping images\n","    date_start: str\n","        start date for image search ('YYYY-MM-DD')\n","    date_end: str\n","        end date for image search ('YYYY-MM-DD')\n","    month_start: int\n","        starting month for calendar range filtering\n","    month_end: int\n","        ending month for calendar range filtering\n","    cloud_cover_max: int\n","        maximum image cloud cover percentage (0-100)\n","    mask_clouds: bool\n","        whether to mask clouds using geedim masking tools\n","    out_path: str\n","        path where images will be downloaded\n","    im_download: bool\n","        whether to download images. Folder for downloads (out_path) must be specified.\n","\n","    Returns\n","    __________\n","    im_xr_list: list\n","        list of xarray.Datasets over the AOI\n","    \"\"\"\n","\n","    # -----Grab optimal UTM zone from AOI CRS\n","    epsg_utm = str(aoi_utm.crs.to_epsg())\n","\n","    # -----Reformat AOI for image filtering\n","    # reproject CRS from AOI to WGS\n","    aoi_wgs = aoi_utm.to_crs('EPSG:4326')\n","    # prepare AOI for querying geedim (AOI bounding box)\n","    region = {'type': 'Polygon',\n","              'coordinates': [[[aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]],\n","                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.miny[0]],\n","                               [aoi_wgs.geometry.bounds.maxx[0], aoi_wgs.geometry.bounds.maxy[0]],\n","                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.maxy[0]],\n","                               [aoi_wgs.geometry.bounds.minx[0], aoi_wgs.geometry.bounds.miny[0]]\n","                               ]]}\n","\n","    # -----Query GEE for imagery\n","    print('Querying GEE for ' + dataset + ' imagery...')\n","    if dataset == 'Landsat':\n","        # Landsat 8\n","        im_col_gd_8 = gd.MaskedCollection.from_name('LANDSAT/LC08/C02/T1_L2').search(start_date=date_start,\n","                                                                                     end_date=date_end,\n","                                                                                     region=region,\n","                                                                                     cloudless_portion=100 - cloud_cover_max,\n","                                                                                     mask=mask_clouds,\n","                                                                                     fill_portion=70)\n","        # Landsat 9\n","        im_col_gd_9 = gd.MaskedCollection.from_name('LANDSAT/LC09/C02/T1_L2').search(start_date=date_start,\n","                                                                                     end_date=date_end,\n","                                                                                     region=region,\n","                                                                                     cloudless_portion=100 - cloud_cover_max,\n","                                                                                     mask=mask_clouds,\n","                                                                                     fill_portion=70)\n","    elif dataset == 'Sentinel-2_TOA':\n","        im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_HARMONIZED').search(start_date=date_start,\n","                                                                                     end_date=date_end,\n","                                                                                     region=region,\n","                                                                                     cloudless_portion=100 - cloud_cover_max,\n","                                                                                     mask=mask_clouds,\n","                                                                                     fill_portion=70)\n","    elif dataset == 'Sentinel-2_SR':\n","        im_col_gd = gd.MaskedCollection.from_name('COPERNICUS/S2_SR_HARMONIZED').search(start_date=date_start,\n","                                                                                        end_date=date_end,\n","                                                                                        region=region,\n","                                                                                        cloudless_portion=100 - cloud_cover_max,\n","                                                                                        mask=mask_clouds,\n","                                                                                        fill_portion=70)\n","    else:\n","        print(\"'dataset' variable not recognized. Please set to 'Landsat', 'Sentinel-2_TOA', or 'Sentinel-2_SR'. \"\n","              \"Exiting...\")\n","        return 'N/A'\n","\n","    # -----Create list of image IDs to download, include those that will be composited\n","    # define function to create list of IDs to be mosaicked\n","    def image_mosaic_ids(im_col_gd):\n","        # create lists of image properties\n","        try:\n","            properties = im_col_gd.properties\n","        except Exception as e:\n","            exc_id = str(e).split('ID=')[1].split(')')[0]\n","            print('Error accessing image ID: ' + exc_id + '. Exiting...')\n","            return 'N/A', 'N/A'\n","        ims = dict(properties).keys()\n","        im_ids = [properties[im]['system:id'] for im in ims]\n","        # return if no images found\n","        if len(im_ids) < 1:\n","            return 'N/A', 'N/A'\n","        im_dts = np.array(\n","            [datetime.datetime.utcfromtimestamp(properties[im]['system:time_start'] / 1000) for im in ims])\n","        # remove image datetimes and IDs outside the specified month range\n","        i = [int(ii) for ii in np.arange(0, len(im_dts)) if\n","             (im_dts[ii].month >= month_start) and (im_dts[ii].month <= month_end)]  # indices of images to keep\n","        im_dts, im_ids = [im_dts[ii] for ii in i], [im_ids[ii] for ii in i]  # subset of image datetimes and IDs\n","        # return if no images remain after filtering by month range\n","        if len(im_dts) < 1:\n","            return 'N/A', 'N/A'\n","        # grab all unique hours in image datetimes\n","        hours = np.array(im_dts, dtype='datetime64[h]')\n","        unique_hours = sorted(set(hours))\n","        # create list of IDs for each unique hour\n","        im_ids_list, im_dts_list = [], []\n","        for unique_hour in unique_hours:\n","            i = list(np.ravel(np.argwhere(hours == unique_hour)))\n","            im_ids_list_hour = [im_ids[ii] for ii in i]\n","            im_ids_list.append(im_ids_list_hour)\n","            im_dts_list_hour = [im_dts[ii] for ii in i]\n","            im_dts_list.append(im_dts_list_hour)\n","\n","        return im_ids_list, im_dts_list\n","\n","    # extract list of IDs to be mosaicked\n","    if dataset == 'Landsat':  # must run for Landsat 8 and 9 separately\n","        im_ids_list_8, im_dts_list_8 = image_mosaic_ids(im_col_gd_8)\n","        im_ids_list_9, im_dts_list_9 = image_mosaic_ids(im_col_gd_9)\n","        # check which collections found images\n","        if (type(im_ids_list_8) is str) and (type(im_ids_list_9) is str):\n","            im_ids_list, im_dts_list = 'N/A', 'N/A'\n","        elif type(im_ids_list_9) is str:\n","            im_ids_list, im_dts_list = im_ids_list_8, im_dts_list_8\n","        elif type(im_ids_list_8) is str:\n","            im_ids_list, im_dts_list = im_ids_list_9, im_dts_list_9\n","        else:\n","            im_ids_list = im_ids_list_8 + im_ids_list_9\n","            im_dts_list = im_dts_list_8 + im_dts_list_9\n","    else:\n","        im_ids_list, im_dts_list = image_mosaic_ids(im_col_gd)\n","    # check if any images were found after filtering by month and determine mosaic IDs\n","    if type(im_ids_list) is str:\n","        print('No images found or error in one or more image IDs, exiting...')\n","        return 'N/A'\n","\n","    # -----Determine whether images must be downloaded (if image sizes exceed GEE limit)\n","    # Calculate width and height of AOI bounding box [m]\n","    aoi_utm_bb_width = aoi_utm.geometry[0].bounds[2] - aoi_utm.geometry[0].bounds[0]\n","    aoi_utm_bb_height = aoi_utm.geometry[0].bounds[3] - aoi_utm.geometry[0].bounds[1]\n","    # Check if number of pixels in each image exceeds GEE limit\n","    res = dataset_dict[dataset]['resolution_m']\n","    num_bands = len(dataset_dict[dataset]['refl_bands'])\n","    if ((aoi_utm_bb_width / res * num_bands) * (aoi_utm_bb_height / res * num_bands)) > 1e8:\n","        im_download = True\n","        print(dataset + ' images must be downloaded for full spatial resolution')\n","    else:\n","        print('No image downloads necessary, ' + dataset + ' images over the AOI are within the GEE limit.')\n","    if (out_path is None) & im_download:\n","        print('Variable out_path must be specified to download images. Exiting...')\n","        return 'N/A'\n","\n","    # -----Create list of xarray.Datasets from list of image IDs\n","    im_xr_list = []  # initialize list of xarray.Datasets\n","    # loop through image IDs\n","    for i in tqdm(range(0, len(im_ids_list))):\n","\n","        # subset image IDs and image datetimes\n","        im_ids, im_dts = im_ids_list[i], im_dts_list[i]\n","\n","        # if images must be downloaded, use geedim\n","        if im_download:\n","\n","            # make directory for outputs (out_path) if it doesn't exist\n","            if not os.path.exists(out_path):\n","                os.mkdir(out_path)\n","                print('Made directory for image downloads: ' + out_path)\n","            # define filename\n","            if len(im_ids) > 1:\n","                im_fn = dataset + '_' + str(im_dts[0]).replace('-', '')[0:8] + '_MOSAIC.tif'\n","            else:\n","                im_fn = dataset + '_' + str(im_dts[0]).replace('-', '')[0:8] + '.tif'\n","            # check file does not already exist in directory, download\n","            if not os.path.exists(out_path + im_fn):\n","                # create list of MaskedImages from IDs\n","                im_gd_list = [gd.MaskedImage.from_id(im_id) for im_id in im_ids]\n","                # combine into new MaskedCollection\n","                im_collection = gd.MaskedCollection.from_list(im_gd_list)\n","                # create image composite\n","                im_composite = im_collection.composite(method=gd.CompositeMethod.q_mosaic,\n","                                                       mask=mask_clouds, region=region)\n","                # download to file\n","                im_composite.download(out_path + im_fn, region=region, scale=dataset_dict[dataset]['resolution_m'],\n","                                      crs='EPSG:' + epsg_utm, bands=im_composite.refl_bands)\n","            # load image from file\n","            im_da = rxr.open_rasterio(out_path + im_fn)\n","            # convert to xarray.DataSet\n","            im_ds = im_da.to_dataset('band')\n","            band_names = list(dataset_dict[dataset]['refl_bands'].keys())\n","            im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n","            # account for image scalar and no data values\n","            im_ds = xr.where(im_ds != dataset_dict[dataset]['no_data_value'],\n","                             im_ds / dataset_dict[dataset]['image_scalar'], np.nan)\n","            # add time dimension\n","            im_dt = np.datetime64(datetime.datetime.fromtimestamp(im_da.attrs['system-time_start'] / 1000))\n","            im_ds = im_ds.expand_dims({'time': [im_dt]})\n","            # set CRS\n","            im_ds.rio.write_crs('EPSG:' + str(im_da.rio.crs.to_epsg()), inplace=True)\n","            # add xarray.Dataset to list\n","            im_xr_list.append(im_ds)\n","\n","        else:  # if no image downloads necessary, use wxee\n","\n","            # if more than one ID, composite images\n","            if len(im_dts) > 1:\n","                # create list of MaskedImages from IDs\n","                ims_gd = [gd.MaskedImage.from_id(im_id, mask=mask_clouds, region=region) for im_id in im_ids]\n","                # convert to list of ee.Images\n","                ims_ee = [ee.Image(im_gd.ee_image).select(im_gd.refl_bands) for im_gd in ims_gd]\n","                # convert to xarray.Datasets\n","                ims_xr = [im_ee.wx.to_xarray(scale=res, region=region, crs='EPSG:' + epsg_utm) for im_ee in ims_ee]\n","                # composite images\n","                ims_xr_composite = xr.merge(ims_xr, compat='override')\n","                # account for image scalar\n","                ims_xr_composite = xr.where(ims_xr_composite != dataset_dict[dataset]['no_data_value'],\n","                                            ims_xr_composite / dataset_dict[dataset]['image_scalar'], np.nan)\n","                # set CRS\n","                ims_xr_composite.rio.write_crs('EPSG:' + epsg_utm, inplace=True)\n","                # append to list of xarray.Datasets\n","                im_xr_list.append(ims_xr_composite)\n","            else:\n","                # create MaskedImage from ID\n","                im_gd = gd.MaskedImage.from_id(im_ids[0], mask=mask_clouds, region=region)\n","                # convert to ee.Image\n","                im_ee = ee.Image(im_gd.ee_image).select(im_gd.refl_bands)\n","                # convert to xarray.Datasets\n","                im_xr = im_ee.wx.to_xarray(scale=res, region=region, crs='EPSG:' + epsg_utm)\n","                # account for image scalar\n","                im_xr = xr.where(im_xr != dataset_dict[dataset]['no_data_value'],\n","                                 im_xr / dataset_dict[dataset]['image_scalar'], np.nan)\n","                # set CRS\n","                im_xr.rio.write_crs('EPSG:' + epsg_utm, inplace=True)\n","                # append to list of xarray.Datasets\n","                im_xr_list.append(im_xr)\n","\n","    return im_xr_list"],"metadata":{"cellView":"form","id":"5ZMR32-8ahKV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----Define settings\n","# Rainey's path to study-sites/\n","study_sites_path = 'drive/MyDrive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n","\n","# Rainey's path to snow-cover-mapping-application/\n","base_path = 'drive/MyDrive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/snow-cover-mapping-application/'\n","\n","# Define sites to load imagery for\n","# If you want to load them all at once, use:\n","# site_names = sorted(os.listdir(study_sites_path)) # grab all folders in study-sites/\n","# site_names = [x for x in site_names if not x.startswith('.')] # remove hidden folders from list\n","site_names = ['Blue', 'Boulder']\n","\n","# Load dataset dictionary\n","dataset_dict = json.load(open(base_path + 'datasets_characteristics.json'))"],"metadata":{"id":"srZal4dzblU_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Locate files/paths in directory if needed\n","# os.listdir(study_sites_path)"],"metadata":{"id":"-CMCV5elc0hz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# -----Iterate over site names\n","for site_name in site_names:\n","\n","    print(site_name)\n","\n","    # load AOI shapefile\n","    AOI_path = study_sites_path + site_name + '/AOIs/'\n","    AOI_fn = glob.glob(AOI_path + '*RGI*.shp')[0]\n","    AOI = gpd.read_file(AOI_fn)\n","    # reproject to UTM\n","    AOI_centroid_lon = AOI.geometry[0].centroid.coords.xy[0][0]\n","    AOI_centroid_lat = AOI.geometry[0].centroid.coords.xy[1][0]\n","    epsg_UTM = convert_wgs_to_utm(AOI_centroid_lon, AOI_centroid_lat)\n","    AOI_UTM = AOI.to_crs('EPSG:' + str(epsg_UTM))\n","\n","    # define path for output images\n","    out_path = study_sites_path + site_name + '/imagery_for_ELAs/'\n","    # make directory if it doesn't exist\n","    if not os.path.exists(out_path):\n","        os.mkdir(out_path)\n","        print('Made directory for output images: ' + out_path)\n","\n","    # load ELA files\n","    ELAs_path = study_sites_path + site_name + '/ELAs/' # define path to ELA files\n","    ELA_fns = sorted(glob.glob(ELAs_path + site_name + '*.csv')) # grab all CSV file names in ELAs_path\n","    ELA_fns = [os.path.basename(x) for x in ELA_fns] # grab just the file names (not the full path to file)\n","\n","    # iterate over ELA file names\n","    for ELA_fn in ELA_fns:\n","\n","        # load file\n","        ELA = pd.read_csv(ELAs_path + ELA_fn)\n","\n","        # grab dataset and image datetime from file name\n","        dataset = ELA['dataset'].values\n","        datetime = np.datetime64(ELA['datetime'].values[0]).astype('datetime64[D]')\n","\n","        # define a date range based on the datetime\n","        date_start = datetime\n","        date_end = datetime + np.timedelta64(1,'D')\n","        print(date_start, date_end)\n","\n","        #### EDIT HERE #####\n","        # query GEE for imagery\n","        # im_xr_list = query_gee_for_imagery(dataset_dict, dataset, aoi_utm, date_start, date_end, month_start, month_end,\n","        #                                    cloud_cover_max, mask_clouds, out_path=None, im_download=False)\n","\n","\n","    print(' ')"],"metadata":{"id":"VVKN-mHadT38"},"execution_count":null,"outputs":[]}]}